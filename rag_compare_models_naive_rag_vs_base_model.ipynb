{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Steps to run this notebook:\n",
        "1. Go to sslvpn.asu.edu and install the Cisco AnyConnect Client VPN\n",
        "• Faculty, staff, and students will also need Duo Two-Factor\n",
        "Authentication\n",
        "2. In the connect window, enter “sslvpn.asu.edu/2fa”\n",
        "3. Sign in with ASURITE, ASU password, and DUO two-factor\n",
        "authentication method\n",
        "4. Login to sol using: https://sol.asu.edu/\n",
        "5. Select Jupyter Notebook server and launch the server selecting the required cores, hour of operation and hackathon sbatch options code."
      ],
      "metadata": {
        "id": "1DVpgzsS4Ik8"
      },
      "id": "1DVpgzsS4Ik8"
    },
    {
      "cell_type": "markdown",
      "id": "935cf2fe-088f-4a5f-95a1-3294a66a11a5",
      "metadata": {
        "id": "935cf2fe-088f-4a5f-95a1-3294a66a11a5"
      },
      "source": [
        "# Base Model Interface"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9b57d9b3-d93b-49a8-a49b-1bd8b880edcc",
      "metadata": {
        "id": "9b57d9b3-d93b-49a8-a49b-1bd8b880edcc"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import gradio as gr\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "690aab4d-5a69-4b3f-8ed9-6f3ad1585c0e",
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "3c5070e407db42cea99e0b9baf2c0784"
          ]
        },
        "id": "690aab4d-5a69-4b3f-8ed9-6f3ad1585c0e",
        "outputId": "277e1189-eb0b-4aab-9a98-8336350e3b8c"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3c5070e407db42cea99e0b9baf2c0784",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "model_name = '/data/datasets/community/huggingface/Llama3-8b-instruct/'\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name,torch_dtype = torch.float16, device_map = \"auto\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7e5f6c3f-fc5b-46e0-8159-50b479d3387b",
      "metadata": {
        "id": "7e5f6c3f-fc5b-46e0-8159-50b479d3387b"
      },
      "outputs": [],
      "source": [
        "class LLMChat:\n",
        "    def __init__(self, tokenizer, model):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.model = model\n",
        "        self.history = []\n",
        "\n",
        "        # THE CRITICAL FIX: Both Llama 3 stop tokens\n",
        "        self.terminators = [\n",
        "            tokenizer.eos_token_id,  # <|end_of_text|> (128001)\n",
        "            tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")  # <|eot_id|> (128009)\n",
        "        ]\n",
        "\n",
        "    def send(self, user_text, temperature=0.6, top_p=0.9):\n",
        "        # Add user message to history\n",
        "        self.history.append({\"role\": \"user\", \"content\": user_text.strip()})\n",
        "\n",
        "        # Build messages with system prompt\n",
        "        messages = [{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}]\n",
        "        messages.extend(self.history)\n",
        "\n",
        "        # Use Llama 3's official chat template\n",
        "        input_ids = self.tokenizer.apply_chat_template(\n",
        "            messages,\n",
        "            add_generation_prompt=True,  # Critical for proper response\n",
        "            return_tensors=\"pt\"\n",
        "        ).to(self.model.device)\n",
        "\n",
        "        # Generate with BOTH stop tokens - this prevents self-conversation\n",
        "        outputs = self.model.generate(\n",
        "            input_ids,\n",
        "            max_new_tokens=2000,\n",
        "            eos_token_id=self.terminators,  # THE KEY FIX\n",
        "            pad_token_id=self.tokenizer.eos_token_id,\n",
        "            do_sample=True,\n",
        "            temperature=temperature,\n",
        "            top_p=top_p,\n",
        "            repetition_penalty=1.1\n",
        "        )\n",
        "\n",
        "        # Extract only the new response tokens\n",
        "        response_tokens = outputs[0][input_ids.shape[-1]:]\n",
        "        response = self.tokenizer.decode(response_tokens, skip_special_tokens=True).strip()\n",
        "\n",
        "        # Add assistant response to history\n",
        "        if response:\n",
        "            self.history.append({\"role\": \"assistant\", \"content\": response})\n",
        "\n",
        "        return response\n",
        "\n",
        "    def clear_history(self):\n",
        "        self.history = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ec064f1a-1804-4fc0-a6da-3716537a3aa0",
      "metadata": {
        "id": "ec064f1a-1804-4fc0-a6da-3716537a3aa0"
      },
      "outputs": [],
      "source": [
        "# Initialize your chat instance\n",
        "chat = LLMChat(tokenizer, model)\n",
        "\n",
        "def chat_fn(message, history, temperature, top_p):\n",
        "    \"\"\"Process chat message and return updated history\"\"\"\n",
        "    # Get response from LLMChat\n",
        "    response = chat.send(message)\n",
        "\n",
        "    # Update gradio history format\n",
        "    history.append([message, response])\n",
        "    return history, \"\"\n",
        "\n",
        "def clear_chat():\n",
        "    \"\"\"Clear chat history\"\"\"\n",
        "    chat.clear_history()\n",
        "    return []"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1f625337",
      "metadata": {
        "id": "1f625337"
      },
      "source": [
        "# Agentic RAG with Local Ollama Model\n",
        "This notebook demonstrates how to build a Retrieval-Augmented Generation (RAG) agent using LangGraph, LangChain, and a local  model run via Ollama.\n",
        "\n",
        "Adapted from: https://langchain-ai.github.io/langgraph/tutorials/rag/langgraph_agentic_rag/\n",
        "\n",
        "## Materials\n",
        "This notebook and all materials referenced here can be found on Sol `/data/sse/ai-accelerated-spark`."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9dbc8378-b454-48d8-a95c-563521334562",
      "metadata": {
        "id": "9dbc8378-b454-48d8-a95c-563521334562"
      },
      "source": [
        "## 1. Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "13cfaace",
      "metadata": {
        "id": "13cfaace"
      },
      "outputs": [],
      "source": [
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_core.messages import HumanMessage, AIMessage, BaseMessage\n",
        "from langchain_core.tools import Tool\n",
        "from langgraph.graph import Graph\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from transformers import pipeline\n",
        "import torch\n",
        "from typing import List\n",
        "from pydantic import BaseModel, Field\n",
        "import os\n",
        "\n",
        "os.environ[\"USER_AGENT\"] = \"Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:109.0) Gecko/20100101 Firefox/109.0\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "59114acf",
      "metadata": {
        "id": "59114acf"
      },
      "source": [
        "## 2. Preprocess documents\n",
        "### 2.1. Fetch documents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "24bf7599",
      "metadata": {
        "id": "24bf7599"
      },
      "outputs": [],
      "source": [
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "\n",
        "urls = [\n",
        "    \"https://medium.com/rapids-ai\",\n",
        "    \"https://medium.com/cupy-team/announcing-cupy-v13-66979ee7fab0\",\n",
        "    \"https://www.unum.cloud/blog/2022-01-26-cupy\",\n",
        "    \"https://docs.nvidia.com/cuda/\"\n",
        "]\n",
        "\n",
        "docs = [WebBaseLoader(url).load() for url in urls]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4d6bc96c-c2c6-4ca8-af17-1a0dc06d46cd",
      "metadata": {
        "id": "4d6bc96c-c2c6-4ca8-af17-1a0dc06d46cd",
        "outputId": "bd4cbead-8f02-402b-9542-e26b9cf460a7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'RAPIDS AI - MediumHomepageOpen in appSign inGet startedRAPIDS AIRAPIDS is a suite of software libraries for executing end-to-end data science & analytics pipelines entirely on GPUs.RAPIDS ReleasesDataFramesMachine LearningData VisualizationGraph AnalyticsNLPLearn MoreFollowRAPIDS 24.08: Better scalability, performance, and CPU/GPU interoperabilityRAPIDS 24.08: Better scalability, performance, and CPU/GPU interoperabilityRAPIDS 24.08 is now available with significant updates geared towards processing larger workloads and seamless CPU/GPU interoperability.Manas SinghAug 27, 2024RAPIDS 24.04 ReleaseRAPIDS 24.04\\xa0ReleaseExpanded accelerated vector search and zero-code change data science, Dask query planning, and support for Python 3.11 and pandas\\xa02Nick BeckerApr 24, 2024RAPIDS 24.02 releaseRAPIDS 24.02\\xa0releaseZero code-change NetworkX, RAPIDS on Databricks with Dask, and\\xa0more!Alex RavenelFeb 22, 2024RAPIDS 23.10 ReleaseRAPIDS 23.10\\xa0ReleaseRAPIDS Goes CPU/GPU (zero code change!), XGBoost 2.'"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "docs[0][0].page_content.strip()[:1000]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b6dcb754-10f3-4a16-b522-c1ec4cbd812b",
      "metadata": {
        "id": "b6dcb754-10f3-4a16-b522-c1ec4cbd812b"
      },
      "source": [
        "### 2.2. Split the fetched documents into smaller chunks for indexing into the vectorstore"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "97649da8-f2bc-49fd-be94-009b59a1a5e5",
      "metadata": {
        "id": "97649da8-f2bc-49fd-be94-009b59a1a5e5"
      },
      "outputs": [],
      "source": [
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "\n",
        "docs_list = [item for sublist in docs for item in sublist]\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
        "    chunk_size=100, chunk_overlap=50\n",
        ")\n",
        "doc_splits = text_splitter.split_documents(docs_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "15cad859-5123-4811-b53a-a58ee9be9232",
      "metadata": {
        "id": "15cad859-5123-4811-b53a-a58ee9be9232",
        "outputId": "7f133165-89d8-49f4-daca-8372cb51bdd1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'RAPIDS AI - MediumHomepageOpen in appSign inGet startedRAPIDS AIRAPIDS is a suite of software libraries for executing end-to-end data science & analytics pipelines entirely on GPUs.RAPIDS ReleasesDataFramesMachine LearningData VisualizationGraph AnalyticsNLPLearn MoreFollowRAPIDS 24.08: Better scalability, performance, and CPU/GPU interoperabilityRAPIDS 24.08: Better scalability, performance, and CPU/GPU'"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "doc_splits[0].page_content"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5dbf0613",
      "metadata": {
        "id": "5dbf0613"
      },
      "source": [
        "## 3.Create a retriever tool\n",
        "### 3.1. Use an in-memory vector store and all-MiniLM-L6-V2 embeddings model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d72e509c-47b7-4cc8-912d-e62cafc09f90",
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "3d4a297468f94b8990f83b275e625a0f"
          ]
        },
        "id": "d72e509c-47b7-4cc8-912d-e62cafc09f90",
        "outputId": "ca68bae5-4ad2-45aa-80fe-678ace368d6e"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3d4a297468f94b8990f83b275e625a0f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "import numpy as np\n",
        "\n",
        "class LlamaEmbeddings:\n",
        "    def __init__(self, model_path):\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "        self.model = AutoModel.from_pretrained(\n",
        "            model_path,\n",
        "            torch_dtype=torch.float16,  # Use half precision to save memory\n",
        "            device_map=\"auto\"\n",
        "        )\n",
        "\n",
        "        # Add padding token if it doesn't exist\n",
        "        if self.tokenizer.pad_token is None:\n",
        "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
        "\n",
        "    def embed_documents(self, texts):\n",
        "        embeddings = []\n",
        "        for text in texts:\n",
        "            # Tokenize\n",
        "            inputs = self.tokenizer(\n",
        "                text,\n",
        "                return_tensors=\"pt\",\n",
        "                truncation=True,\n",
        "                padding=True,\n",
        "                max_length=512\n",
        "            )\n",
        "\n",
        "            # Get hidden states\n",
        "            with torch.no_grad():\n",
        "                outputs = self.model(**inputs)\n",
        "                # Use mean pooling of last hidden state\n",
        "                hidden_states = outputs.last_hidden_state\n",
        "                attention_mask = inputs['attention_mask']\n",
        "\n",
        "                # Mean pooling\n",
        "                embeddings_tensor = (hidden_states * attention_mask.unsqueeze(-1)).sum(1) / attention_mask.sum(-1).unsqueeze(-1)\n",
        "                embeddings.append(embeddings_tensor.cpu().numpy())\n",
        "\n",
        "        return np.vstack(embeddings)\n",
        "\n",
        "    def embed_query(self, text):\n",
        "        return self.embed_documents([text])[0]\n",
        "\n",
        "# Usage with LangChain\n",
        "from langchain_core.embeddings import Embeddings\n",
        "\n",
        "class LangChainLlamaEmbeddings(Embeddings):\n",
        "    def __init__(self, model_path):\n",
        "        self.llama_embedder = LlamaEmbeddings(model_path)\n",
        "\n",
        "    def embed_documents(self, texts):\n",
        "        return self.llama_embedder.embed_documents(texts).tolist()\n",
        "\n",
        "    def embed_query(self, text):\n",
        "        return self.llama_embedder.embed_query(text).tolist()\n",
        "\n",
        "# Use it\n",
        "embedding_model = LangChainLlamaEmbeddings(\"/data/datasets/community/huggingface/Llama3-8b-instruct/\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f985362e-b14d-484a-a3f7-cfdafcbbd5f5",
      "metadata": {
        "id": "f985362e-b14d-484a-a3f7-cfdafcbbd5f5"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "import torch\n",
        "\n",
        "# This approach tries to adapt the model\n",
        "class LlamaSentenceTransformer:\n",
        "    def __init__(self, model_path):\n",
        "        # Load as base transformer model\n",
        "        from transformers import AutoModel, AutoTokenizer\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "        self.model = AutoModel.from_pretrained(\n",
        "            model_path,\n",
        "            torch_dtype=torch.float16,\n",
        "            device_map=\"auto\"\n",
        "        )\n",
        "\n",
        "        if self.tokenizer.pad_token is None:\n",
        "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
        "\n",
        "    def encode(self, sentences, convert_to_tensor=False):\n",
        "        if isinstance(sentences, str):\n",
        "            sentences = [sentences]\n",
        "\n",
        "        embeddings = []\n",
        "        for sentence in sentences:\n",
        "            inputs = self.tokenizer(\n",
        "                sentence,\n",
        "                return_tensors=\"pt\",\n",
        "                truncation=True,\n",
        "                padding=True,\n",
        "                max_length=512\n",
        "            )\n",
        "\n",
        "            with torch.no_grad():\n",
        "                outputs = self.model(**inputs)\n",
        "                # Mean pooling\n",
        "                attention_mask = inputs['attention_mask']\n",
        "                token_embeddings = outputs.last_hidden_state\n",
        "                input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
        "                embeddings.append(torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9))\n",
        "\n",
        "        result = torch.cat(embeddings, dim=0)\n",
        "        if convert_to_tensor:\n",
        "            return result\n",
        "        return result.cpu().numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "64d01351-ab57-442a-9b57-2b1a3ac796b8",
      "metadata": {
        "id": "64d01351-ab57-442a-9b57-2b1a3ac796b8"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModel, BitsAndBytesConfig\n",
        "import torch\n",
        "\n",
        "# Use 4-bit quantization to reduce memory usage\n",
        "quantization_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_compute_dtype=torch.float16\n",
        ")\n",
        "\n",
        "class QuantizedLlamaEmbeddings:\n",
        "    def __init__(self, model_path):\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "        self.model = AutoModel.from_pretrained(\n",
        "            model_path,\n",
        "            quantization_config=quantization_config,\n",
        "            device_map=\"auto\"\n",
        "        )\n",
        "\n",
        "        if self.tokenizer.pad_token is None:\n",
        "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
        "\n",
        "    def get_embeddings(self, texts, batch_size=1):\n",
        "        all_embeddings = []\n",
        "\n",
        "        for i in range(0, len(texts), batch_size):\n",
        "            batch_texts = texts[i:i+batch_size]\n",
        "\n",
        "            inputs = self.tokenizer(\n",
        "                batch_texts,\n",
        "                return_tensors=\"pt\",\n",
        "                truncation=True,\n",
        "                padding=True,\n",
        "                max_length=512\n",
        "            )\n",
        "\n",
        "            with torch.no_grad():\n",
        "                outputs = self.model(**inputs)\n",
        "                # Mean pooling\n",
        "                embeddings = outputs.last_hidden_state.mean(dim=1)\n",
        "                all_embeddings.append(embeddings.cpu())\n",
        "\n",
        "        return torch.cat(all_embeddings, dim=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "812a77a3-ada9-4eaa-a4cf-ca56baf35460",
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "64a844ebf5d941c39bcd5c6a373557fb"
          ]
        },
        "id": "812a77a3-ada9-4eaa-a4cf-ca56baf35460",
        "outputId": "be329cbd-1f82-4e56-dd82-802fc80a40a8"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "64a844ebf5d941c39bcd5c6a373557fb",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from langchain_core.vectorstores import InMemoryVectorStore\n",
        "\n",
        "# Create your custom embedding class\n",
        "embedding_model = LangChainLlamaEmbeddings(\"/data/datasets/community/huggingface/Llama3-8b-instruct/\")\n",
        "\n",
        "# Now use it normally\n",
        "vectorstore = InMemoryVectorStore.from_documents(\n",
        "    documents=doc_splits,\n",
        "    embedding=embedding_model\n",
        ")\n",
        "retriever = vectorstore.as_retriever()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "abcb57e5-dcb5-4387-8327-5756c2df4962",
      "metadata": {
        "id": "abcb57e5-dcb5-4387-8327-5756c2df4962"
      },
      "outputs": [],
      "source": [
        "# TODO: Use ChromaDB for persistent vectorstore\n",
        "# https://python.langchain.com/docs/integrations/vectorstores/"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4f517c44-2e14-4614-9290-8d75e7faef4f",
      "metadata": {
        "id": "4f517c44-2e14-4614-9290-8d75e7faef4f"
      },
      "source": [
        "### 3.2. Create a retriever tool using LangChain's prebuild `create_retriever_tool`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e5278231-c5eb-40ca-94f5-8a74d99a1b88",
      "metadata": {
        "id": "e5278231-c5eb-40ca-94f5-8a74d99a1b88"
      },
      "outputs": [],
      "source": [
        "from langchain.tools.retriever import create_retriever_tool\n",
        "\n",
        "retriever_tool = create_retriever_tool(\n",
        "    retriever,\n",
        "    \"retrieve_python_gpu_acceleration\",\n",
        "    \"Search and return information about accelerating Python code using the GPU with RAPIDS and CuPy.\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e7fcbee6-e9d6-466f-9c7a-303ca012b469",
      "metadata": {
        "id": "e7fcbee6-e9d6-466f-9c7a-303ca012b469"
      },
      "source": [
        "### 3.3. Test the tool"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2ba4b915-9cfc-4e71-9ea2-6060cde12d50",
      "metadata": {
        "id": "2ba4b915-9cfc-4e71-9ea2-6060cde12d50",
        "outputId": "3bbe2e94-38c4-423b-8473-c754deb93120"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'None), slice(None, None))],}# The array is allocated in devices 0 and 1multi_gpu_array = distributed_array(cpu_array, mapping)The work has been done by @shino16 during the Preferred Networks 2023 summer internship.Upgrading from CuPy v12 or earlierAs always, we have prepared the Upgrade Guide, which lists the changes that may break your code when migrating between major releases. Especially note that the minimum requirement for CuPy is now\\n\\nis a library developed by the NVIDIA RAPIDS project that provides GPU-accelerated implementation of signal processing algorithms using CuPy as its backend. cuSignal includes scipy.signal compatible APIs, so we shared the same goals. After a discussion with the cuSignal team, we agreed to merge cuSignal into CuPy to provide users a better experience with a unified library for SciPy routines on GPU. We would like to acknowledge and thank Adam Thompson and everyone involved in\\n\\nAfter a discussion with the cuSignal team, we agreed to merge cuSignal into CuPy to provide users a better experience with a unified library for SciPy routines on GPU. We would like to acknowledge and thank Adam Thompson and everyone involved in the development for creating this great library and agreeing to this transition.Import CuPy without CUDAEssentially, CuPy is tightly coupled with underlying GPU toolkits such as NVIDIA CUDA and AMD ROCm. For a long time,\\n\\ntime, CuPy’s installation assumed the toolkit to be installed locally — if not, import cupy has yielded an ImportError. This has been a problem in some scenarios, such as NumPy-based projects with optional GPU support via CuPy. In CuPy v13, all CUDA Toolkit libraries are lazily-loaded, making import cupy to success even when CUDA is not installed.More NumPy & SciPy-compatible APIsIn addition to the signal processing'"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "retriever_tool.invoke({\"query\": \"How can I create a CuPy-backed Dask array for random data?\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "29127ddb-5909-4809-8525-3204ef2d08cf",
      "metadata": {
        "id": "29127ddb-5909-4809-8525-3204ef2d08cf"
      },
      "source": [
        "## 4. Generate query\n",
        "### 4.1. Load local LLM\n",
        "\n",
        "Start ollama using the terminal:\n",
        "```bash\n",
        "module load ollama/0.9.0\n",
        "export OLLAMA_MODELS=/data/datasets/community/ollama\n",
        "ollama-start\n",
        "```\n",
        "\n",
        "Check the available list of models using `ollama list`. Let me know via Slack if you would like to use and test other models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e4504022-deee-4190-b681-6dfa9ba5e758",
      "metadata": {
        "id": "e4504022-deee-4190-b681-6dfa9ba5e758"
      },
      "outputs": [],
      "source": [
        "from langchain_ollama import ChatOllama\n",
        "import socket\n",
        "from langchain_ollama.llms import OllamaLLM\n",
        "from langchain.chat_models import init_chat_model\n",
        "\n",
        "host_node = socket.gethostname()\n",
        "llm_model = init_chat_model(\"ollama:qwen3:14b\", temperature=0.6, base_url=f\"http://jgarc111@{host_node}:11434/\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "972da38e-7439-4d43-8fc6-6aafcdf00069",
      "metadata": {
        "id": "972da38e-7439-4d43-8fc6-6aafcdf00069"
      },
      "source": [
        "### 4.2. Build a `generate_query_or_respond` node"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "65ecc05d-868d-4228-bc67-bd8ed7b65a44",
      "metadata": {
        "id": "65ecc05d-868d-4228-bc67-bd8ed7b65a44"
      },
      "outputs": [],
      "source": [
        "from langgraph.graph import MessagesState\n",
        "import re\n",
        "\n",
        "def generate_query_or_respond(state: MessagesState):\n",
        "    \"\"\"Call the model to generate a response based on the current state. Given\n",
        "    the question, it will decide to retrieve using the retriever tool, or simply respond to the user.\n",
        "    \"\"\"\n",
        "    response = (\n",
        "        llm_model\n",
        "        .bind_tools([retriever_tool]).invoke(state[\"messages\"])\n",
        "    )\n",
        "    # remove thinking text\n",
        "    content = re.sub(r\"<think>.*</think>\", \"\", response.content, flags=re.DOTALL).strip()\n",
        "    response.content = content\n",
        "    return {\"messages\": [response]}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e0091934-a78a-4a46-bdd6-24d62eae69ed",
      "metadata": {
        "id": "e0091934-a78a-4a46-bdd6-24d62eae69ed"
      },
      "source": [
        "### 4.3. Try a random input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6fbc5702-ff03-4934-b664-3d6a8edca463",
      "metadata": {
        "id": "6fbc5702-ff03-4934-b664-3d6a8edca463",
        "outputId": "386debea-ec1c-48c6-936f-6000e95daeb5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "The color of the sky is typically blue during the day due to the scattering of sunlight by the Earth's atmosphere, a phenomenon known as Rayleigh scattering. However, the sky can appear different colors at various times of day (e.g., red/orange during sunrise/sunset) or under different weather conditions. \n",
            "\n",
            "Since your question is unrelated to GPU acceleration or Python libraries like RAPIDS/CuPy, I cannot use the provided tools to address it. Let me know if you'd like help with Python/GPU-related topics!\n"
          ]
        }
      ],
      "source": [
        "input = {\"messages\": [{\"role\": \"user\", \"content\": \"Hello! What is the color of the sky?\"}]}\n",
        "generate_query_or_respond(input)[\"messages\"][-1].pretty_print()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e50e1018-8b5c-405e-ab80-2bfc79e9a7ab",
      "metadata": {
        "id": "e50e1018-8b5c-405e-ab80-2bfc79e9a7ab"
      },
      "source": [
        "### 4.4. Try semantic search question"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "934dd1bc-fa2c-414c-80f9-cb94b93ffde3",
      "metadata": {
        "id": "934dd1bc-fa2c-414c-80f9-cb94b93ffde3",
        "outputId": "c5b49787-1586-43f1-b54d-ee20872b8976"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "Tool Calls:\n",
            "  retrieve_python_gpu_acceleration (2ef686ce-26c7-471f-ae92-487415519d6e)\n",
            " Call ID: 2ef686ce-26c7-471f-ae92-487415519d6e\n",
            "  Args:\n",
            "    query: Creating CuPy-backed Dask arrays for random data\n"
          ]
        }
      ],
      "source": [
        "input = {\n",
        "    \"messages\": [\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": \"How can I create a CuPy-backed Dask array for random data?\",\n",
        "        }\n",
        "    ]\n",
        "}\n",
        "generate_query_or_respond(input)[\"messages\"][-1].pretty_print()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eddaf2a9-6dac-4e24-a77d-2ee588ac6eed",
      "metadata": {
        "id": "eddaf2a9-6dac-4e24-a77d-2ee588ac6eed"
      },
      "source": [
        "## 5. Grade documents\n",
        "### 5.1. Add conditional edge `grade_documents` to determine the relevance of retrieved documents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a6294ac9-5927-4f26-ac89-046829bcdfba",
      "metadata": {
        "id": "a6294ac9-5927-4f26-ac89-046829bcdfba"
      },
      "outputs": [],
      "source": [
        "from pydantic import BaseModel, Field\n",
        "from typing import Literal\n",
        "\n",
        "GRADE_PROMPT = (\n",
        "    \"You are a grader assessing relevance of a retrieved document to a user question. \\n \"\n",
        "    \"Here is the retrieved document: \\n\\n {context} \\n\\n\"\n",
        "    \"Here is the user question: {question} \\n\"\n",
        "    \"If the document contains keyword(s) or semantic meaning related to the user question, grade it as relevant. \\n\"\n",
        "    \"Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question.\"\n",
        ")\n",
        "\n",
        "class GradeDocuments(BaseModel):\n",
        "    \"\"\"Grade documents using a binary score for relevance check.\"\"\"\n",
        "\n",
        "    binary_score: str = Field(\n",
        "        description=\"Relevance score: 'yes' if relevant, or 'no' if not relevant\"\n",
        "    )\n",
        "\n",
        "\n",
        "def grade_documents(\n",
        "    state: MessagesState,\n",
        ") -> Literal[\"generate_answer\", \"rewrite_question\"]:\n",
        "    \"\"\"Determine whether the retrieved documents are relevant to the question.\"\"\"\n",
        "    question = state[\"messages\"][0].content\n",
        "    context = state[\"messages\"][-1].content\n",
        "\n",
        "\n",
        "    prompt = GRADE_PROMPT.format(question=question, context=context)\n",
        "    response = (\n",
        "        llm_model\n",
        "        .with_structured_output(GradeDocuments).invoke(\n",
        "            [{\"role\": \"user\", \"content\": prompt}]\n",
        "        )\n",
        "    )\n",
        "    score = response.binary_score\n",
        "\n",
        "    if score == \"yes\":\n",
        "        return \"generate_answer\"\n",
        "    else:\n",
        "        return \"rewrite_question\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5a918ef9-e933-4f0c-83ce-75d02585a241",
      "metadata": {
        "id": "5a918ef9-e933-4f0c-83ce-75d02585a241"
      },
      "source": [
        "### 5.2. Try with irrelevant documents in the tool response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "00be481b-9484-40db-98c5-f227129cafd9",
      "metadata": {
        "id": "00be481b-9484-40db-98c5-f227129cafd9",
        "outputId": "6c71740e-bd74-45d3-a266-a7bcecea40a6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'rewrite_question'"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain_core.messages import convert_to_messages\n",
        "\n",
        "input = {\n",
        "    \"messages\": convert_to_messages(\n",
        "        [\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": \"How can I create a CuPy-backed Dask array for random data?\",\n",
        "            },\n",
        "            {\n",
        "                \"role\": \"assistant\",\n",
        "                \"content\": \"\",\n",
        "                \"tool_calls\": [\n",
        "                    {\n",
        "                        \"id\": \"1\",\n",
        "                        \"name\": \"retrieve_python_gpu_acceleration\",\n",
        "                        \"args\": {\"query\": \"creating CuPy-backed Dask arrays for random data\"},\n",
        "                    }\n",
        "                ],\n",
        "            },\n",
        "            {\"role\": \"tool\", \"content\": \"meow\", \"tool_call_id\": \"1\"},\n",
        "        ]\n",
        "    )\n",
        "}\n",
        "grade_documents(input)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fdf4e35d-63e6-4658-9ade-c6e835c6e39e",
      "metadata": {
        "id": "fdf4e35d-63e6-4658-9ade-c6e835c6e39e"
      },
      "source": [
        "### 5.3. Try with relevant documents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0d7119fa-55d5-4179-8bcc-661a68e100b7",
      "metadata": {
        "id": "0d7119fa-55d5-4179-8bcc-661a68e100b7",
        "outputId": "528cecb4-5a6c-4cf2-b02f-5bbb801e4d0b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'generate_answer'"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "input = {\n",
        "    \"messages\": convert_to_messages(\n",
        "        [\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": \"How can I create a CuPy-backed Dask array for random data?\",\n",
        "            },\n",
        "            {\n",
        "                \"role\": \"assistant\",\n",
        "                \"content\": \"\",\n",
        "                \"tool_calls\": [\n",
        "                    {\n",
        "                        \"id\": \"1\",\n",
        "                        \"name\": \"retrieve_python_gpu_acceleration\",\n",
        "                        \"args\": {\"query\": \"creating CuPy-backed Dask arrays for random data\"},\n",
        "                    }\n",
        "                ],\n",
        "            },\n",
        "            {\n",
        "                \"role\": \"tool\",\n",
        "                \"content\": 'Now, we can leverage the array.backend configuration to create a CuPy-backed Dask array for random data:>>> with dask.config.set({“array.backend”: “cupy”}):…    darr = da.random.randint(0, 3, size=(10, 20), chunks=(2, 5)) #\\n\\n= rs.randint(0, 3, size=(10, 20), chunks=(2, 5))>>> darrdask.array<randint, shape=(10, 20), dtype=int64, chunksize=(2, 5), \\\\chunktype=cupy.ndarray>Now, we can leverage the array.backend configuration to create a CuPy-backed Dask array for random data:>>> with\\n\\nfor random array creation.',\n",
        "                \"tool_call_id\": \"1\",\n",
        "            },\n",
        "        ]\n",
        "    )\n",
        "}\n",
        "grade_documents(input)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d6520541-4db0-4db2-839d-66d74494644b",
      "metadata": {
        "id": "d6520541-4db0-4db2-839d-66d74494644b"
      },
      "source": [
        "## 6.\n",
        "### 6.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "199cb600-4cad-4e9a-80fc-93746877b3b7",
      "metadata": {
        "id": "199cb600-4cad-4e9a-80fc-93746877b3b7"
      },
      "outputs": [],
      "source": [
        "REWRITE_PROMPT = (\n",
        "    \"Look at the input and try to reason about the underlying semantic intent / meaning.\\n\"\n",
        "    \"Here is the initial question:\"\n",
        "    \"\\n ------- \\n\"\n",
        "    \"{question}\"\n",
        "    \"\\n ------- \\n\"\n",
        "    \"Formulate an improved question:\"\n",
        ")\n",
        "\n",
        "\n",
        "def rewrite_question(state: MessagesState):\n",
        "    \"\"\"Rewrite the original user question.\"\"\"\n",
        "    messages = state[\"messages\"]\n",
        "    question = messages[0].content\n",
        "    prompt = REWRITE_PROMPT.format(question=question)\n",
        "    response = llm_model.invoke([{\"role\": \"user\", \"content\": prompt}])\n",
        "    # remove thinking text\n",
        "    content = re.sub(r\"<think>.*</think>\", \"\", response.content, flags=re.DOTALL).strip()\n",
        "    response.content = content\n",
        "    return {\"messages\": [{\"role\": \"user\", \"content\": response.content}]}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f025fedf-4e70-443f-acae-c384b7163d8f",
      "metadata": {
        "id": "f025fedf-4e70-443f-acae-c384b7163d8f"
      },
      "source": [
        "### 6.2 Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "114f4730-0d74-4172-aaf8-9dc8c55a60a3",
      "metadata": {
        "id": "114f4730-0d74-4172-aaf8-9dc8c55a60a3",
        "outputId": "99992419-9f57-4b80-d5ba-0dac8b3c9962"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "**Improved Question:**  \n",
            "How can I generate a Dask array backed by CuPy that contains random data (e.g., uniformly distributed or normally distributed values), and what are the key considerations for ensuring compatibility between Dask and CuPy in this context?  \n",
            "\n",
            "**Reasoning:**  \n",
            "The improved question clarifies the intent to generate random data (specifying potential distributions) and explicitly asks about compatibility considerations, which are critical for ensuring the solution works correctly with both Dask and CuPy. It also avoids ambiguity by framing the request as a technical implementation task rather than a vague \"how-to\" question.\n"
          ]
        }
      ],
      "source": [
        "input = {\n",
        "    \"messages\": convert_to_messages(\n",
        "        [\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": \"How can I create a CuPy-backed Dask array for random data?\",\n",
        "            },\n",
        "            {\n",
        "                \"role\": \"assistant\",\n",
        "                \"content\": \"\",\n",
        "                \"tool_calls\": [\n",
        "                    {\n",
        "                        \"id\": \"1\",\n",
        "                        \"name\": \"retrieve_python_gpu_acceleration\",\n",
        "                        \"args\": {\"query\": \"creating CuPy-backed Dask arrays for random data\"},\n",
        "                    }\n",
        "                ],\n",
        "            },\n",
        "            {\"role\": \"tool\", \"content\": \"meow\", \"tool_call_id\": \"1\"},\n",
        "        ]\n",
        "    )\n",
        "}\n",
        "\n",
        "response = rewrite_question(input)\n",
        "print(response[\"messages\"][-1][\"content\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "20a0771d-200e-4a1c-9da3-e8a4e82fb480",
      "metadata": {
        "id": "20a0771d-200e-4a1c-9da3-e8a4e82fb480"
      },
      "source": [
        "## 7. Generate an answer\n",
        "### 7.1. Build `generate_answer` node"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f3472875-cfd6-4b3f-8b39-7a1139bd7ce7",
      "metadata": {
        "id": "f3472875-cfd6-4b3f-8b39-7a1139bd7ce7"
      },
      "outputs": [],
      "source": [
        "GENERATE_PROMPT = (\n",
        "    \"You are an assistant for question-answering tasks. \"\n",
        "    \"Use the following pieces of retrieved context to answer the question. \"\n",
        "    \"If you don't know the answer, just say that you don't know. \"\n",
        "    \"Use three sentences maximum and keep the answer concise.\\n\"\n",
        "    \"Question: {question} \\n\"\n",
        "    \"Context: {context}\"\n",
        ")\n",
        "\n",
        "\n",
        "def generate_answer(state: MessagesState):\n",
        "    \"\"\"Generate an answer.\"\"\"\n",
        "    question = state[\"messages\"][0].content\n",
        "    context = state[\"messages\"][-1].content\n",
        "    prompt = GENERATE_PROMPT.format(question=question, context=context)\n",
        "    response = llm_model.invoke([{\"role\": \"user\", \"content\": prompt}])\n",
        "    # remove thinking text\n",
        "    content = re.sub(r\"<think>.*</think>\", \"\", response.content, flags=re.DOTALL).strip()\n",
        "    response.content = content\n",
        "    return {\"messages\": [response]}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "38cd1250-820e-40f8-ad58-c85bca8e2c2e",
      "metadata": {
        "id": "38cd1250-820e-40f8-ad58-c85bca8e2c2e"
      },
      "source": [
        "## 7.2 Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "76d74558-2403-4c2c-a50c-944cd63dc501",
      "metadata": {
        "id": "76d74558-2403-4c2c-a50c-944cd63dc501",
        "outputId": "f90b6d23-2d39-49bb-d3c8-a10284dc1b4b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "To create a CuPy-backed Dask array for random data, set the `array.backend` configuration to `\"cupy\"` using `dask.config.set`. Then, use `da.random.randint` (or similar functions) to generate the array, specifying the desired shape, data type, and chunk sizes. This ensures the array is backed by CuPy for GPU acceleration.\n"
          ]
        }
      ],
      "source": [
        "input = {\n",
        "    \"messages\": convert_to_messages(\n",
        "        [\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": \"How can I create a CuPy-backed Dask array for random data?\",\n",
        "            },\n",
        "            {\n",
        "                \"role\": \"assistant\",\n",
        "                \"content\": \"\",\n",
        "                \"tool_calls\": [\n",
        "                    {\n",
        "                        \"id\": \"1\",\n",
        "                        \"name\": \"retrieve_python_gpu_acceleration\",\n",
        "                        \"args\": {\"query\": \"creating CuPy-backed Dask arrays for random data\"},\n",
        "                    }\n",
        "                ],\n",
        "            },\n",
        "            {\n",
        "                \"role\": \"tool\",\n",
        "                \"content\": 'Now, we can leverage the array.backend configuration to create a CuPy-backed Dask array for random data:>>> with dask.config.set({“array.backend”: “cupy”}):…    darr = da.random.randint(0, 3, size=(10, 20), chunks=(2, 5)) #\\n\\n= rs.randint(0, 3, size=(10, 20), chunks=(2, 5))>>> darrdask.array<randint, shape=(10, 20), dtype=int64, chunksize=(2, 5), \\\\chunktype=cupy.ndarray>Now, we can leverage the array.backend configuration to create a CuPy-backed Dask array for random data:>>> with\\n\\nfor random array creation.',\n",
        "                \"tool_call_id\": \"1\",\n",
        "            },\n",
        "        ]\n",
        "    )\n",
        "}\n",
        "\n",
        "response = generate_answer(input)\n",
        "response[\"messages\"][-1].pretty_print()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "beb5fc5f-b09c-42b1-88d9-f2b6784b2eda",
      "metadata": {
        "id": "beb5fc5f-b09c-42b1-88d9-f2b6784b2eda"
      },
      "source": [
        "## 8. Assemble the graph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "49bf2038-5fa5-4e22-a208-4b1167e1cede",
      "metadata": {
        "id": "49bf2038-5fa5-4e22-a208-4b1167e1cede"
      },
      "outputs": [],
      "source": [
        "from langgraph.graph import StateGraph, START, END\n",
        "from langgraph.prebuilt import ToolNode\n",
        "from langgraph.prebuilt import tools_condition\n",
        "\n",
        "workflow = StateGraph(MessagesState)\n",
        "\n",
        "# Define the nodes we will cycle between\n",
        "workflow.add_node(generate_query_or_respond)\n",
        "workflow.add_node(\"retrieve\", ToolNode([retriever_tool]))\n",
        "workflow.add_node(rewrite_question)\n",
        "workflow.add_node(generate_answer)\n",
        "\n",
        "workflow.add_edge(START, \"generate_query_or_respond\")\n",
        "\n",
        "# Decide whether to retrieve\n",
        "workflow.add_conditional_edges(\n",
        "    \"generate_query_or_respond\",\n",
        "    # Assess LLM decision (call `retriever_tool` tool or respond to the user)\n",
        "    tools_condition,\n",
        "    {\n",
        "        # Translate the condition outputs to nodes in our graph\n",
        "        \"tools\": \"retrieve\",\n",
        "        END: END,\n",
        "    },\n",
        ")\n",
        "\n",
        "# Edges taken after the `action` node is called.\n",
        "workflow.add_conditional_edges(\n",
        "    \"retrieve\",\n",
        "    # Assess agent decision\n",
        "    grade_documents,\n",
        ")\n",
        "workflow.add_edge(\"generate_answer\", END)\n",
        "workflow.add_edge(\"rewrite_question\", \"generate_query_or_respond\")\n",
        "\n",
        "# Compile\n",
        "graph = workflow.compile()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d44857e0-4ce3-4a02-ba3b-ade019288936",
      "metadata": {
        "id": "d44857e0-4ce3-4a02-ba3b-ade019288936",
        "outputId": "5791a797-199a-48c6-f2aa-6aeb15257cc0"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from IPython.display import Image, display\n",
        "\n",
        "display(Image(graph.get_graph().draw_mermaid_png()))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "17cc9c8a-4a28-49dc-ae10-ca7b8eb42e43",
      "metadata": {
        "id": "17cc9c8a-4a28-49dc-ae10-ca7b8eb42e43"
      },
      "source": [
        "## 9. Run the agentic RAG"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "90d926a1-4f35-42ed-bd08-1027c975644b",
      "metadata": {
        "id": "90d926a1-4f35-42ed-bd08-1027c975644b",
        "outputId": "64b22063-f481-4830-cfd2-bf5b15c83aec"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Update from node generate_query_or_respond\n",
            "\n",
            "\n",
            "\n",
            "Update from node retrieve\n",
            "\n",
            "\n",
            "\n",
            "Update from node rewrite_question\n",
            "\n",
            "\n",
            "\n",
            "Update from node generate_query_or_respond\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "for chunk in graph.stream(\n",
        "    {\n",
        "        \"messages\": [\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": \"How can I create a CuPy-backed Dask array for random data?\",\n",
        "            }\n",
        "        ]\n",
        "    }\n",
        "):\n",
        "    for node, update in chunk.items():\n",
        "        print(\"Update from node\", node)\n",
        "        update[\"messages\"][-1]\n",
        "        print(\"\\n\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Naive Rag Chat Interface\n"
      ],
      "metadata": {
        "id": "n1e7a34_5zK0"
      },
      "id": "n1e7a34_5zK0"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4d135bb8-88ae-4cc1-9a5a-5ffde1232826",
      "metadata": {
        "id": "4d135bb8-88ae-4cc1-9a5a-5ffde1232826",
        "outputId": "11305c01-4fbe-42a6-822d-b0febe5411e9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "* Running on local URL:  http://127.0.0.1:7860\n",
            "* Running on public URL: https://a7ea9700dce92e4ff9.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"https://a7ea9700dce92e4ff9.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import gradio as gr\n",
        "\n",
        "def ask_graph(user_input, chat_history):\n",
        "    result = graph.invoke({\n",
        "        \"messages\": [\n",
        "            {\"role\": \"user\", \"content\": user_input}\n",
        "        ]\n",
        "    })\n",
        "\n",
        "    response = result[\"messages\"][-1].content\n",
        "\n",
        "    if not chat_history:\n",
        "        response = [{\"role\": \"user\", \"content\": user_input}, {\"role\": \"assistant\", \"content\": response}]\n",
        "    else:\n",
        "        response = chat_history + [{\"role\": \"user\", \"content\": user_input}, {\"role\": \"assistant\", \"content\": response}]\n",
        "\n",
        "    return \"\", response\n",
        "\n",
        "def clear_conversation():\n",
        "    return \"\", \"\"\n",
        "\n",
        "with gr.Blocks(fill_height=True, fill_width=True) as demo:\n",
        "    gr.Markdown(\"### Agentic RAG\")\n",
        "\n",
        "    with gr.Column():\n",
        "\n",
        "        with gr.Row():\n",
        "            chatbot = gr.Chatbot(height=350, type=\"messages\")\n",
        "\n",
        "        with gr.Row():\n",
        "            with gr.Column(scale=4):\n",
        "                query_input = gr.Textbox(\n",
        "                    label=\"Enter text here\", placeholder=\"Ask something...\", lines=1\n",
        "                    )\n",
        "            with gr.Column(scale=1):\n",
        "                with gr.Row():\n",
        "                    submit_btn = gr.Button(\"⬆\")\n",
        "                # 🧹 Clear button\n",
        "                with gr.Row():\n",
        "                    clear_btn = gr.Button(\"🧹 Clear Conversation\")\n",
        "\n",
        "        submit_btn.click(\n",
        "            fn=ask_graph,\n",
        "            inputs=[query_input, chatbot],\n",
        "            outputs=[query_input, chatbot],\n",
        "        )\n",
        "\n",
        "        query_input.submit(\n",
        "            fn=ask_graph,\n",
        "            inputs=[query_input, chatbot],\n",
        "            outputs=[query_input, chatbot],\n",
        "        )\n",
        "\n",
        "        clear_btn.click(\n",
        "            fn=clear_conversation,\n",
        "            outputs=[query_input, chatbot],\n",
        "        )\n",
        "\n",
        "demo.launch(share=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "58b76c3b-d8d9-4a50-8e43-335d0d1097b8",
      "metadata": {
        "id": "58b76c3b-d8d9-4a50-8e43-335d0d1097b8"
      },
      "source": [
        "## Separated Chat Interface (Naive Rag vs Base Model llama )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "989347c6-09ea-4723-8f05-123e9777c1a1",
      "metadata": {
        "id": "989347c6-09ea-4723-8f05-123e9777c1a1",
        "outputId": "b2fcb90c-b857-40fb-b2cd-29eaeb9719f8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "* Running on local URL:  http://0.0.0.0:7864\n",
            "* Running on public URL: https://68c44ebc344c27856f.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"https://68c44ebc344c27856f.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
          ]
        }
      ],
      "source": [
        "import gradio as gr\n",
        "import time\n",
        "import random\n",
        "\n",
        "# Mock chatbot functions - replace these with your actual chatbot APIs\n",
        "def chatbot_a_response(message, history):\n",
        "    \"\"\"Simulate Chatbot A response\"\"\"\n",
        "    if not message.strip():\n",
        "        return \"\", history\n",
        "\n",
        "    time.sleep(1)  # Simulate processing time\n",
        "    response = chat.send(message)\n",
        "\n",
        "    # Update gradio history format using messages format\n",
        "    history.append({\"role\": \"user\", \"content\": message})\n",
        "    history.append({\"role\": \"assistant\", \"content\": response})\n",
        "    return \"\", history\n",
        "\n",
        "def chatbot_b_response(message, chat_history):\n",
        "    \"\"\"Simulate Chatbot B response\"\"\"\n",
        "    if not message.strip():\n",
        "        return \"\", chat_history\n",
        "\n",
        "    time.sleep(1.2)  # Simulate different processing time\n",
        "    result = graph.invoke({\n",
        "        \"messages\": [\n",
        "            {\"role\": \"user\", \"content\": message}\n",
        "        ]\n",
        "    })\n",
        "    response = result[\"messages\"][-1].content\n",
        "\n",
        "    if not chat_history:\n",
        "        updated_history = [{\"role\": \"user\", \"content\": message}, {\"role\": \"assistant\", \"content\": response}]\n",
        "    else:\n",
        "        updated_history = chat_history + [{\"role\": \"user\", \"content\": message}, {\"role\": \"assistant\", \"content\": response}]\n",
        "\n",
        "    return \"\", updated_history\n",
        "\n",
        "def clear_chat_a():\n",
        "    \"\"\"Clear chat A history\"\"\"\n",
        "    chat.clear_history()\n",
        "    return []\n",
        "\n",
        "def clear_chat_b():\n",
        "    \"\"\"Clear chat B history\"\"\"\n",
        "    return []\n",
        "\n",
        "# Create the Gradio interface\n",
        "with gr.Blocks(title=\"Chatbot Comparison\", theme=gr.themes.Soft()) as demo:\n",
        "    gr.Markdown(\"# 🤖 Chatbot Performance Comparison\")\n",
        "    gr.Markdown(\"Compare responses from two different chatbots side by side - each with independent chat\")\n",
        "\n",
        "    with gr.Row():\n",
        "        # Chatbot A Column\n",
        "        with gr.Column(scale=1):\n",
        "            gr.Markdown(\"### 🔵 Chatbot A\")\n",
        "            chatbot_a = gr.Chatbot(\n",
        "                height=300,\n",
        "                show_label=False,\n",
        "                container=True,\n",
        "                type=\"messages\"\n",
        "            )\n",
        "\n",
        "            with gr.Row():\n",
        "                msg_a = gr.Textbox(\n",
        "                    placeholder=\"Message for Chatbot A...\",\n",
        "                    show_label=False,\n",
        "                    scale=4,\n",
        "                    container=False\n",
        "                )\n",
        "                send_btn_a = gr.Button(\"Send\", variant=\"primary\", scale=1)\n",
        "\n",
        "            clear_btn_a = gr.Button(\"Clear Chat A\", variant=\"secondary\", size=\"sm\")\n",
        "\n",
        "        # Chatbot B Column\n",
        "        with gr.Column(scale=1):\n",
        "            gr.Markdown(\"### 🔴 Chatbot B\")\n",
        "            chatbot_b = gr.Chatbot(\n",
        "                height=300,\n",
        "                show_label=False,\n",
        "                container=True,\n",
        "                type=\"messages\"\n",
        "            )\n",
        "\n",
        "            with gr.Row():\n",
        "                msg_b = gr.Textbox(\n",
        "                    placeholder=\"Message for Chatbot B...\",\n",
        "                    show_label=False,\n",
        "                    scale=4,\n",
        "                    container=False\n",
        "                )\n",
        "                send_btn_b = gr.Button(\"Send\", variant=\"primary\", scale=1)\n",
        "\n",
        "            clear_btn_b = gr.Button(\"Clear Chat B\", variant=\"secondary\", size=\"sm\")\n",
        "\n",
        "    # Add some example prompts for both\n",
        "    # with gr.Row():\n",
        "        # with gr.Column(scale=1):\n",
        "        #     gr.Examples(\n",
        "        #         examples=[\n",
        "        #             [\"What is artificial intelligence?\"],\n",
        "        #             [\"Explain quantum computing in simple terms\"],\n",
        "        #             [\"Write a short poem about nature\"],\n",
        "        #             [\"What are the benefits of renewable energy?\"]\n",
        "        #         ],\n",
        "        #         inputs=msg_a,\n",
        "        #         label=\"Example prompts for Chatbot A\"\n",
        "        #     )\n",
        "\n",
        "        # with gr.Column(scale=1):\n",
        "        #     gr.Examples(\n",
        "        #         examples=[\n",
        "        #             [\"What is artificial intelligence?\"],\n",
        "        #             [\"Explain quantum computing in simple terms\"],\n",
        "        #             [\"Write a short poem about nature\"],\n",
        "        #             [\"What are the benefits of renewable energy?\"]\n",
        "        #         ],\n",
        "        #         inputs=msg_b,\n",
        "        #         label=\"Example prompts for Chatbot B\"\n",
        "        #     )\n",
        "\n",
        "    # Event handlers for Chatbot A\n",
        "    send_btn_a.click(\n",
        "        fn=chatbot_a_response,\n",
        "        inputs=[msg_a, chatbot_a],\n",
        "        outputs=[msg_a, chatbot_a]\n",
        "    )\n",
        "\n",
        "    msg_a.submit(\n",
        "        fn=chatbot_a_response,\n",
        "        inputs=[msg_a, chatbot_a],\n",
        "        outputs=[msg_a, chatbot_a]\n",
        "    )\n",
        "\n",
        "    clear_btn_a.click(\n",
        "        fn=clear_chat_a,\n",
        "        outputs=[chatbot_a]\n",
        "    )\n",
        "\n",
        "    # Event handlers for Chatbot B\n",
        "    send_btn_b.click(\n",
        "        fn=chatbot_b_response,\n",
        "        inputs=[msg_b, chatbot_b],\n",
        "        outputs=[msg_b, chatbot_b]\n",
        "    )\n",
        "\n",
        "    msg_b.submit(\n",
        "        fn=chatbot_b_response,\n",
        "        inputs=[msg_b, chatbot_b],\n",
        "        outputs=[msg_b, chatbot_b]\n",
        "    )\n",
        "\n",
        "    clear_btn_b.click(\n",
        "        fn=clear_chat_b,\n",
        "        outputs=[chatbot_b]\n",
        "    )\n",
        "\n",
        "# Launch the interface\n",
        "if __name__ == \"__main__\":\n",
        "    demo.launch(\n",
        "        share=True,  # Set to True to create a public link\n",
        "        server_name=\"0.0.0.0\",  # Allow external connections\n",
        "        # Let Gradio automatically find an available port\n",
        "        # server_port=7860  # Comment out or remove this line\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9131337e-17ea-49e6-a9aa-b0c4798ddfe1",
      "metadata": {
        "id": "9131337e-17ea-49e6-a9aa-b0c4798ddfe1"
      },
      "source": [
        "## 10. Graphic User Interface using Gradio"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cbe4ac34-36bd-4c27-97c3-f1d72407f687",
      "metadata": {
        "id": "cbe4ac34-36bd-4c27-97c3-f1d72407f687"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "574ef629-9c70-410b-a887-973cf4d5e21b",
      "metadata": {
        "id": "574ef629-9c70-410b-a887-973cf4d5e21b",
        "outputId": "48e8114a-dc6a-4a7d-cec1-b225d5688e1a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sat Jun 28 01:32:58 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 565.57.01              Driver Version: 565.57.01      CUDA Version: 12.7     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  NVIDIA A100-SXM4-80GB          On  |   00000000:01:00.0 Off |                    0 |\n",
            "| N/A   33C    P0             76W /  500W |    7567MiB /  81920MiB |      0%      Default |\n",
            "|                                         |                        |             Disabled |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "|   1  NVIDIA A100-SXM4-80GB          On  |   00000000:41:00.0 Off |                    0 |\n",
            "| N/A   30C    P0             73W /  500W |    8051MiB /  81920MiB |      0%      Default |\n",
            "|                                         |                        |             Disabled |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "|   2  NVIDIA A100-SXM4-80GB          On  |   00000000:81:00.0 Off |                    0 |\n",
            "| N/A   32C    P0             74W /  500W |    8051MiB /  81920MiB |      0%      Default |\n",
            "|                                         |                        |             Disabled |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "|   3  NVIDIA A100-SXM4-80GB          On  |   00000000:C1:00.0 Off |                    0 |\n",
            "| N/A   31C    P0             71W /  500W |   18151MiB /  81920MiB |      0%      Default |\n",
            "|                                         |                        |             Disabled |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|    0   N/A  N/A   2364740      C   ...nvs/ollama-python-0.5.1//bin/python       7556MiB |\n",
            "|    1   N/A  N/A   2364740      C   ...nvs/ollama-python-0.5.1//bin/python       8040MiB |\n",
            "|    2   N/A  N/A   2364740      C   ...nvs/ollama-python-0.5.1//bin/python       8040MiB |\n",
            "|    3   N/A  N/A   2364740      C   ...nvs/ollama-python-0.5.1//bin/python       7208MiB |\n",
            "|    3   N/A  N/A   2365625      C   /packages/apps/ollama/0.9.0/bin/ollama      10926MiB |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8bbb1271-8ef8-4d17-b417-d6378e05410d",
      "metadata": {
        "id": "8bbb1271-8ef8-4d17-b417-d6378e05410d"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "ollama-python-0.5.1",
      "language": "python",
      "name": "ollama-python-0.5.1"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.3"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}