{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "713a56a3-525c-4033-abeb-85e1456dde76",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de710bda-3c8d-44f1-aced-678b1a73e007",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/osave/1402SJL-ai-accelerated-spark'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b7ab1cf-fcaf-4344-937c-c9e43bf0a695",
   "metadata": {},
   "source": [
    "## Enhanced Document Loading System\n",
    "\n",
    "This cell implements an enhanced document loading system that combines web content with local files to create a comprehensive knowledge base. The system loads documents from multiple sources and adds appropriate metadata for citation tracking.\n",
    "\n",
    "**Key Features:**\n",
    "- Loads URLs from CSV file with recursive depth\n",
    "- Supports both Jupyter notebooks (.ipynb) and Python files (.py)\n",
    "- Adds source type metadata for proper citation handling\n",
    "- Provides detailed loading progress and statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d4cdd2-630d-4d5e-a150-6e435c523333",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import gc\n",
    "import shutil\n",
    "import glob\n",
    "from langchain.document_loaders import RecursiveUrlLoader\n",
    "from langchain_community.document_loaders import NotebookLoader, TextLoader\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "# =============================================================================\n",
    "# STEP 1: ENHANCED DOCUMENT LOADING (REPLACES YOUR ORIGINAL LOADING)\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"ENHANCED DOCUMENT LOADING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Read URLs from CSV file\n",
    "df = pd.read_csv('gpu_data_science_urls.txt')\n",
    "base_urls = df['url'].dropna().tolist()\n",
    "\n",
    "print(f\"Found {len(base_urls)} URLs to load\")\n",
    "\n",
    "# Step 1: Load all web pages with recursive depth 2\n",
    "documents = []\n",
    "for url in base_urls:\n",
    "    try:\n",
    "        loader = RecursiveUrlLoader(url=url, max_depth=2)\n",
    "        docs = loader.load()\n",
    "        \n",
    "        # Add source type metadata for URL documents\n",
    "        for doc in docs:\n",
    "            doc.metadata['source_type'] = 'url'\n",
    "        \n",
    "        documents.extend(docs)\n",
    "        print(f\"‚úì Loaded: {url}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚úó Failed: {url} - {e}\")\n",
    "\n",
    "print(f\"URL documents loaded: {len(documents)}\")\n",
    "\n",
    "# Step 2: Load notebooks and Python files from folder\n",
    "notebook_folder_path = 'resource/'  # CHANGE THIS TO YOUR FOLDER PATH\n",
    "\n",
    "if os.path.exists(notebook_folder_path):\n",
    "    print(f\"\\nLoading files from: {notebook_folder_path}\")\n",
    "    \n",
    "    # Find all .ipynb and .py files in the folder\n",
    "    notebook_files = glob.glob(os.path.join(notebook_folder_path, \"**/*.ipynb\"), recursive=True)\n",
    "    python_files = glob.glob(os.path.join(notebook_folder_path, \"**/*.py\"), recursive=True)\n",
    "    all_files = notebook_files + python_files\n",
    "    \n",
    "    print(f\"Found {len(notebook_files)} notebook files and {len(python_files)} Python files\")\n",
    "    \n",
    "    for file_path in all_files:\n",
    "        try:\n",
    "            # Choose loader based on file extension\n",
    "            if file_path.endswith('.ipynb'):\n",
    "                loader = NotebookLoader(\n",
    "                    path=file_path,\n",
    "                    include_outputs=True,\n",
    "                    max_output_length=1000,\n",
    "                    remove_newline=True,\n",
    "                )\n",
    "            else:  # .py files\n",
    "                loader = TextLoader(file_path)\n",
    "            \n",
    "            docs = loader.load()\n",
    "            \n",
    "            # Add custom metadata for both notebooks and Python files\n",
    "            for doc in docs:\n",
    "                relative_path = os.path.relpath(file_path, notebook_folder_path)\n",
    "                \n",
    "                if file_path.endswith('.ipynb'):\n",
    "                    source_type = 'notebook'\n",
    "                else:\n",
    "                    source_type = 'python_file'\n",
    "                \n",
    "                doc.metadata.update({\n",
    "                    'source_type': source_type,\n",
    "                    'source': relative_path,  # This will be folder/filename\n",
    "                    'full_path': file_path,\n",
    "                    'filename': os.path.basename(file_path)\n",
    "                })\n",
    "            \n",
    "            documents.extend(docs)\n",
    "            print(f\"‚úì Loaded: {relative_path}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚úó Failed to load {file_path}: {e}\")\n",
    "    \n",
    "    notebook_count = len([d for d in documents if d.metadata.get('source_type') == 'notebook'])\n",
    "    python_count = len([d for d in documents if d.metadata.get('source_type') == 'python_file'])\n",
    "    print(f\"Notebook documents loaded: {notebook_count}\")\n",
    "    print(f\"Python file documents loaded: {python_count}\")\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è  Files folder not found: {notebook_folder_path}\")\n",
    "\n",
    "print(f\"\\nTotal documents loaded: {len(documents)}\")\n",
    "print(\"Step 1 complete - enhanced loading done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46c024ee-b160-4456-83a0-42bbd1ed3896",
   "metadata": {},
   "source": [
    "## Enhanced Document Chunking System\n",
    "\n",
    "This cell implements an intelligent document chunking system that cleans, filters, and optimizes text chunks for better vector store performance. The system includes content quality filtering and adaptive chunk sizing based on document characteristics.\n",
    "\n",
    "**Key Components:**\n",
    "- HTML content cleaning with BeautifulSoup\n",
    "- Quality filtering to remove navigation and boilerplate text\n",
    "- Adaptive chunk size optimization based on document length analysis\n",
    "- Technical content-aware text splitting with code block preservation\n",
    "- Batch processing for memory efficiency\n",
    "- Fallback mechanism for error handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "63b56d61-271a-4dd8-a2f9-4577e40b6926",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "ENHANCED TEXT CHUNKING\n",
      "============================================================\n",
      "Starting enhanced document chunking...\n",
      "Original documents: 1208\n",
      "Step 1: Cleaning document content...\n",
      "  Cleaned 0/1208 documents...\n",
      "  Cleaned 100/1208 documents...\n",
      "  Cleaned 200/1208 documents...\n",
      "  Cleaned 300/1208 documents...\n",
      "  Cleaned 400/1208 documents...\n",
      "  Cleaned 500/1208 documents...\n",
      "  Cleaned 600/1208 documents...\n",
      "  Cleaned 700/1208 documents...\n",
      "  Cleaned 800/1208 documents...\n",
      "  Cleaned 900/1208 documents...\n",
      "  Cleaned 1000/1208 documents...\n",
      "  Cleaned 1100/1208 documents...\n",
      "  Cleaned 1200/1208 documents...\n",
      "  ‚úì Cleaned: 1142 documents\n",
      "  ‚úó Skipped: 66 low-quality documents\n",
      "Step 2: Optimizing chunk parameters...\n",
      "  Using chunk_size: 1500\n",
      "  Using chunk_overlap: 300\n",
      "Step 3: Creating chunks...\n",
      "  Processed 0/1142 documents...\n",
      "  Processed 500/1142 documents...\n",
      "  Processed 1000/1142 documents...\n",
      "Step 4: Final quality filtering...\n",
      "Final Results:\n",
      "  Original documents: N/A\n",
      "  Cleaned documents: 1142\n",
      "  Total chunks created: 22940\n",
      "  Quality chunks: 20962\n",
      "  Average chunk length: 1242 characters\n",
      "‚úì Enhanced document chunking complete!\n",
      "\n",
      "üìÑ Sample chunk preview:\n",
      "Content: GitHub - rapidsai/cuml: cuML - RAPIDS Machine Learning Library Skip to content You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session. You switched accounts on another tab or window. Reload to refresh your ses...\n",
      "Metadata: {'source': 'https://github.com/rapidsai/cuml', 'content_type': 'text/html; charset=utf-8', 'title': 'GitHub - rapidsai/cuml: cuML - RAPIDS Machine Learning Library', 'description': 'cuML - RAPIDS Machine Learning Library. Contribute to rapidsai/cuml development by creating an account on GitHub.', 'language': 'en', 'source_type': 'url'}\n",
      "\n",
      "‚úÖ Text chunking completed successfully!\n",
      "Ready for vector store creation with 20962 high-quality chunks\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from bs4 import BeautifulSoup, XMLParsedAsHTMLWarning\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from typing import List\n",
    "import gc\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=XMLParsedAsHTMLWarning)\n",
    "def clean_content(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Clean document content by removing HTML, excessive whitespace, and noise\n",
    "    \n",
    "    Args:\n",
    "        text (str): Raw document content\n",
    "        \n",
    "    Returns:\n",
    "        str: Cleaned content\n",
    "    \"\"\"\n",
    "    if not text or len(text.strip()) == 0:\n",
    "        return \"\"\n",
    "    \n",
    "    try:\n",
    "        # Remove HTML tags using BeautifulSoup\n",
    "        soup = BeautifulSoup(text, 'html.parser')\n",
    "        \n",
    "        # Remove script and style elements completely\n",
    "        for script in soup([\"script\", \"style\", \"nav\", \"footer\", \"header\"]):\n",
    "            script.decompose()\n",
    "        \n",
    "        # Extract text\n",
    "        text = soup.get_text()\n",
    "        \n",
    "        # Clean up whitespace and special characters\n",
    "        text = re.sub(r'\\s+', ' ', text)  # Multiple whitespace to single space\n",
    "        text = re.sub(r'\\n\\s*\\n', '\\n', text)  # Multiple newlines to single\n",
    "        # # Just remove actual problematic characters\n",
    "        # text = re.sub(r'[\\x00-\\x08\\x0B\\x0C\\x0E-\\x1F\\x7F]', ' ', text)  # Control chars only\n",
    "        text = re.sub(r'(.)\\1{3,}', r'\\1\\1', text)\n",
    "        \n",
    "        # Remove URLs (optional - might want to keep some)\n",
    "        text = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', text)\n",
    "        \n",
    "        # Clean up and strip\n",
    "        text = text.strip()\n",
    "        \n",
    "        return text\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Content cleaning failed: {e}\")\n",
    "        # Fallback: basic cleaning\n",
    "        text = re.sub(r'<[^>]+>', '', text)  # Remove HTML tags\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        return text\n",
    "\n",
    "def filter_quality_content(text: str, min_length: int = 50) -> bool:\n",
    "    \"\"\"\n",
    "    Filter out low-quality content chunks\n",
    "    \n",
    "    Args:\n",
    "        text (str): Content to evaluate\n",
    "        min_length (int): Minimum character length\n",
    "        \n",
    "    Returns:\n",
    "        bool: True if content meets quality criteria\n",
    "    \"\"\"\n",
    "    if len(text) < min_length:\n",
    "        return False\n",
    "    \n",
    "    # Check for meaningful content (not just navigation/metadata)\n",
    "    low_quality_indicators = [\n",
    "        'cookie policy', 'privacy policy', 'terms of service',\n",
    "        'sign up', 'log in', 'subscribe', 'newsletter',\n",
    "        'all rights reserved', 'copyright', '¬© 20',\n",
    "        'navigation menu', 'breadcrumb', 'skip to',\n",
    "        'loading...', 'javascript required'\n",
    "    ]\n",
    "    \n",
    "    text_lower = text.lower()\n",
    "    low_quality_count = sum(1 for indicator in low_quality_indicators if indicator in text_lower)\n",
    "    \n",
    "    # If more than 20% is low-quality indicators, filter out\n",
    "    if low_quality_count > len(low_quality_indicators) * 0.2:\n",
    "        return False\n",
    "    \n",
    "    # Check for reasonable word count and variety\n",
    "    words = text.split()\n",
    "    if len(words) < 10:  # Too few words\n",
    "        return False\n",
    "    \n",
    "    unique_words = len(set(words))\n",
    "    if unique_words / len(words) < 0.3:  # Too repetitive\n",
    "        return False\n",
    "    \n",
    "    return True\n",
    "\n",
    "def optimize_chunk_size_for_content(documents: List, base_chunk_size: int = 1500) -> int:\n",
    "    \"\"\"\n",
    "    Analyze content to determine optimal chunk size\n",
    "    \n",
    "    Args:\n",
    "        documents: List of documents\n",
    "        base_chunk_size: Starting chunk size\n",
    "        \n",
    "    Returns:\n",
    "        int: Optimized chunk size\n",
    "    \"\"\"\n",
    "    if not documents:\n",
    "        return base_chunk_size\n",
    "    \n",
    "    # Sample some documents to analyze\n",
    "    sample_size = min(100, len(documents))\n",
    "    sample_docs = documents[:sample_size]\n",
    "    \n",
    "    avg_doc_length = sum(len(doc.page_content) for doc in sample_docs) / sample_size\n",
    "    \n",
    "    # Adjust chunk size based on average document length\n",
    "    if avg_doc_length < 500:\n",
    "        return 800  # Smaller chunks for short docs\n",
    "    elif avg_doc_length < 2000:\n",
    "        return 1500  # Medium chunks\n",
    "    else:\n",
    "        return 2000  # Larger chunks for long docs\n",
    "\n",
    "def chunk_documents_enhanced(documents: List, chunk_size: int = None, chunk_overlap: int = None) -> List:\n",
    "    \"\"\"\n",
    "    Enhanced document chunking with cleaning and optimization\n",
    "    \n",
    "    Args:\n",
    "        documents: List of documents to chunk\n",
    "        chunk_size: Override automatic chunk size detection\n",
    "        chunk_overlap: Chunk overlap size\n",
    "        \n",
    "    Returns:\n",
    "        List: Processed and chunked documents\n",
    "    \"\"\"\n",
    "    print(\"Starting enhanced document chunking...\")\n",
    "    print(f\"Original documents: {len(documents)}\")\n",
    "    \n",
    "    # Step 1: Clean document content\n",
    "    print(\"Step 1: Cleaning document content...\")\n",
    "    cleaned_docs = []\n",
    "    skipped_count = 0\n",
    "    \n",
    "    for i, doc in enumerate(documents):\n",
    "        if i % 100 == 0:  # Progress update\n",
    "            print(f\"  Cleaned {i}/{len(documents)} documents...\")\n",
    "        \n",
    "        try:\n",
    "            # Clean content\n",
    "            cleaned_content = clean_content(doc.page_content)\n",
    "            \n",
    "            # Filter quality\n",
    "            if filter_quality_content(cleaned_content):\n",
    "                doc.page_content = cleaned_content\n",
    "                cleaned_docs.append(doc)\n",
    "            else:\n",
    "                skipped_count += 1\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"  Warning: Failed to clean document {i}: {e}\")\n",
    "            skipped_count += 1\n",
    "    \n",
    "    print(f\"  ‚úì Cleaned: {len(cleaned_docs)} documents\")\n",
    "    print(f\"  ‚úó Skipped: {skipped_count} low-quality documents\")\n",
    "    \n",
    "    # Clean up memory\n",
    "    del documents\n",
    "    gc.collect()\n",
    "    \n",
    "    # Step 2: Optimize chunk parameters\n",
    "    print(\"Step 2: Optimizing chunk parameters...\")\n",
    "    \n",
    "    if chunk_size is None:\n",
    "        chunk_size = optimize_chunk_size_for_content(cleaned_docs)\n",
    "    \n",
    "    if chunk_overlap is None:\n",
    "        chunk_overlap = min(300, chunk_size // 5)  # 20% overlap, max 300 chars\n",
    "    \n",
    "    print(f\"  Using chunk_size: {chunk_size}\")\n",
    "    print(f\"  Using chunk_overlap: {chunk_overlap}\")\n",
    "    \n",
    "    # Step 3: Create text splitter with technical content awareness\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        length_function=len,\n",
    "        separators=[\n",
    "            \"\\n\\n\",      # Paragraph breaks\n",
    "            \"\\n\",        # Line breaks\n",
    "            \"```\",       # Code blocks\n",
    "            \"def \",      # Function definitions\n",
    "            \"class \",    # Class definitions\n",
    "            \"import \",   # Import statements\n",
    "            \". \",        # Sentences\n",
    "            \", \",        # Clauses\n",
    "            \" \",         # Words\n",
    "            \"\",          # Characters\n",
    "        ],\n",
    "        keep_separator=True,\n",
    "    )\n",
    "    \n",
    "    # Step 4: Split documents into chunks\n",
    "    print(\"Step 3: Creating chunks...\")\n",
    "    chunks = []\n",
    "    \n",
    "    batch_size = 50  # Process in batches to manage memory\n",
    "    for i in range(0, len(cleaned_docs), batch_size):\n",
    "        batch = cleaned_docs[i:i+batch_size]\n",
    "        \n",
    "        try:\n",
    "            batch_chunks = text_splitter.split_documents(batch)\n",
    "            chunks.extend(batch_chunks)\n",
    "            \n",
    "            if i % (batch_size * 10) == 0:  # Progress update every 500 docs\n",
    "                print(f\"  Processed {i}/{len(cleaned_docs)} documents...\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"  Warning: Failed to chunk batch {i//batch_size}: {e}\")\n",
    "    \n",
    "    # Step 5: Final quality filter on chunks\n",
    "    print(\"Step 4: Final quality filtering...\")\n",
    "    quality_chunks = []\n",
    "    \n",
    "    for chunk in chunks:\n",
    "        if (len(chunk.page_content.strip()) >= 100 and  # Minimum meaningful length\n",
    "            filter_quality_content(chunk.page_content, min_length=100)):\n",
    "            quality_chunks.append(chunk)\n",
    "    \n",
    "    print(f\"Final Results:\")\n",
    "    print(f\"  Original documents: {len(documents) if 'documents' in locals() else 'N/A'}\")\n",
    "    print(f\"  Cleaned documents: {len(cleaned_docs)}\")\n",
    "    print(f\"  Total chunks created: {len(chunks)}\")\n",
    "    print(f\"  Quality chunks: {len(quality_chunks)}\")\n",
    "    print(f\"  Average chunk length: {sum(len(c.page_content) for c in quality_chunks) / len(quality_chunks):.0f} characters\")\n",
    "    \n",
    "    print(\"‚úì Enhanced document chunking complete!\")\n",
    "    \n",
    "    return quality_chunks\n",
    "\n",
    "# Main execution with error handling and monitoring\n",
    "try:\n",
    "    print(\"=\"*60)\n",
    "    print(\"ENHANCED TEXT CHUNKING\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Check if documents exist from previous cell\n",
    "    if 'documents' not in locals():\n",
    "        print(\"‚ùå Error: 'documents' not found. Please run the document loading cell first.\")\n",
    "    else:\n",
    "        # Run enhanced chunking\n",
    "        chunks = chunk_documents_enhanced(\n",
    "            documents,\n",
    "            chunk_size=1500,    # Increased from 1000 for technical content\n",
    "            chunk_overlap=300   # Increased overlap for better context\n",
    "        )\n",
    "        \n",
    "        # Save a sample chunk for inspection\n",
    "        if chunks:\n",
    "            print(f\"\\nüìÑ Sample chunk preview:\")\n",
    "            print(f\"Content: {chunks[0].page_content[:300]}...\")\n",
    "            print(f\"Metadata: {chunks[0].metadata}\")\n",
    "            \n",
    "        print(f\"\\n‚úÖ Text chunking completed successfully!\")\n",
    "        print(f\"Ready for vector store creation with {len(chunks)} high-quality chunks\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Critical error in text chunking: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    \n",
    "    # Fallback to basic chunking if enhanced version fails\n",
    "    print(\"\\nüîÑ Falling back to basic chunking...\")\n",
    "    try:\n",
    "        text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=1000,\n",
    "            chunk_overlap=200,\n",
    "            length_function=len,\n",
    "        )\n",
    "        chunks = text_splitter.split_documents(documents)\n",
    "        print(f\"‚úÖ Basic chunking completed: {len(chunks)} chunks created\")\n",
    "    except Exception as fallback_error:\n",
    "        print(f\"‚ùå Fallback chunking also failed: {fallback_error}\")\n",
    "        chunks = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06886624-5f28-442b-9f29-0800ee07d118",
   "metadata": {},
   "source": [
    "## FAISS Vector Store Creation\n",
    "\n",
    "This cell creates a FAISS vector store from the processed document chunks using batch processing for memory efficiency.\n",
    "The system uses high-quality sentence transformers embeddings and provides comprehensive error handling and progress tracking.\n",
    "\n",
    "**Key Features:**\n",
    "- Batch processing to handle large datasets efficiently\n",
    "- High-quality sentence-transformers/all-mpnet-base-v2 embeddings\n",
    "- Automatic cleanup of existing vector stores\n",
    "- Memory management with garbage collection between batches\n",
    "- Detailed progress reporting and vector store statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8d601af7-5393-48a6-aeb8-06342174e6e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "VECTOR STORE CREATION\n",
      "============================================================\n",
      "Creating vector store from 20962 chunks...\n",
      "Loading embeddings model...\n",
      "‚úì Embeddings model loaded\n",
      "Creating new vector store...\n",
      "Processing 20962 chunks in batches of 1000\n",
      "  Processing batch 1/21 (1000 chunks)...\n",
      "    ‚úì Created initial vector store with 1000 documents\n",
      "  Processing batch 2/21 (1000 chunks)...\n",
      "    ‚úì Merged batch into vector store (total: 2000)\n",
      "  Processing batch 3/21 (1000 chunks)...\n",
      "    ‚úì Merged batch into vector store (total: 3000)\n",
      "  Processing batch 4/21 (1000 chunks)...\n",
      "    ‚úì Merged batch into vector store (total: 4000)\n",
      "  Processing batch 5/21 (1000 chunks)...\n",
      "    ‚úì Merged batch into vector store (total: 5000)\n",
      "  Processing batch 6/21 (1000 chunks)...\n",
      "    ‚úì Merged batch into vector store (total: 6000)\n",
      "  Processing batch 7/21 (1000 chunks)...\n",
      "    ‚úì Merged batch into vector store (total: 7000)\n",
      "  Processing batch 8/21 (1000 chunks)...\n",
      "    ‚úì Merged batch into vector store (total: 8000)\n",
      "  Processing batch 9/21 (1000 chunks)...\n",
      "    ‚úì Merged batch into vector store (total: 9000)\n",
      "  Processing batch 10/21 (1000 chunks)...\n",
      "    ‚úì Merged batch into vector store (total: 10000)\n",
      "  Processing batch 11/21 (1000 chunks)...\n",
      "    ‚úì Merged batch into vector store (total: 11000)\n",
      "  Processing batch 12/21 (1000 chunks)...\n",
      "    ‚úì Merged batch into vector store (total: 12000)\n",
      "  Processing batch 13/21 (1000 chunks)...\n",
      "    ‚úì Merged batch into vector store (total: 13000)\n",
      "  Processing batch 14/21 (1000 chunks)...\n",
      "    ‚úì Merged batch into vector store (total: 14000)\n",
      "  Processing batch 15/21 (1000 chunks)...\n",
      "    ‚úì Merged batch into vector store (total: 15000)\n",
      "  Processing batch 16/21 (1000 chunks)...\n",
      "    ‚úì Merged batch into vector store (total: 16000)\n",
      "  Processing batch 17/21 (1000 chunks)...\n",
      "    ‚úì Merged batch into vector store (total: 17000)\n",
      "  Processing batch 18/21 (1000 chunks)...\n",
      "    ‚úì Merged batch into vector store (total: 18000)\n",
      "  Processing batch 19/21 (1000 chunks)...\n",
      "    ‚úì Merged batch into vector store (total: 19000)\n",
      "  Processing batch 20/21 (1000 chunks)...\n",
      "    ‚úì Merged batch into vector store (total: 20000)\n",
      "  Processing batch 21/21 (962 chunks)...\n",
      "    ‚úì Merged batch into vector store (total: 20962)\n",
      "‚úì Batch processing complete: 20962 total vectors\n",
      "‚úì Vector store saved to 'gpu_datascience_vectorstore'\n",
      "\n",
      "‚úÖ Vector store ready!\n",
      "üìä Total vectors: 20962\n",
      "üìê Vector dimension: 768\n",
      "üíæ Saved to: gpu_datascience_vectorstore\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gc\n",
    "import shutil\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "def create_vector_store_batched(chunks, embeddings, batch_size=1000):\n",
    "    \"\"\"\n",
    "    Create vector store with batch processing to handle memory efficiently\n",
    "    \n",
    "    Args:\n",
    "        chunks: Document chunks\n",
    "        embeddings: Embedding model\n",
    "        batch_size: Number of chunks to process at once\n",
    "        \n",
    "    Returns:\n",
    "        FAISS vector store\n",
    "    \"\"\"\n",
    "    print(f\"Processing {len(chunks)} chunks in batches of {batch_size}\")\n",
    "    \n",
    "    vector_store = None\n",
    "    total_batches = (len(chunks) + batch_size - 1) // batch_size\n",
    "    \n",
    "    for i in range(0, len(chunks), batch_size):\n",
    "        batch_num = (i // batch_size) + 1\n",
    "        batch = chunks[i:i+batch_size]\n",
    "        \n",
    "        print(f\"  Processing batch {batch_num}/{total_batches} ({len(batch)} chunks)...\")\n",
    "        \n",
    "        try:\n",
    "            if vector_store is None:\n",
    "                # Create initial vector store from first batch\n",
    "                vector_store = FAISS.from_documents(batch, embeddings)\n",
    "                print(f\"    ‚úì Created initial vector store with {len(batch)} documents\")\n",
    "            else:\n",
    "                # Create temporary vector store for this batch\n",
    "                batch_store = FAISS.from_documents(batch, embeddings)\n",
    "                \n",
    "                # Merge with main vector store\n",
    "                vector_store.merge_from(batch_store)\n",
    "                print(f\"    ‚úì Merged batch into vector store (total: {vector_store.index.ntotal})\")\n",
    "                \n",
    "                # Clean up batch store\n",
    "                del batch_store\n",
    "                \n",
    "            # Garbage collection between batches\n",
    "            gc.collect()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"    ‚úó Failed to process batch {batch_num}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    if vector_store is None:\n",
    "        raise RuntimeError(\"Failed to create vector store from any batch\")\n",
    "    \n",
    "    print(f\"‚úì Batch processing complete: {vector_store.index.ntotal} total vectors\")\n",
    "    return vector_store\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"VECTOR STORE CREATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Check if chunks exist\n",
    "if 'chunks' not in locals():\n",
    "    print(\"‚ùå Error: 'chunks' not found. Please run the text chunking cell first.\")\n",
    "else:\n",
    "    print(f\"Creating vector store from {len(chunks)} chunks...\")\n",
    "    \n",
    "    # Create embeddings (all-mpnet-base-v2 provides best quality for technical content)\n",
    "    print(\"Loading embeddings model...\")\n",
    "    embeddings = HuggingFaceEmbeddings(\n",
    "        model_name=\"sentence-transformers/all-mpnet-base-v2\"  # Better quality than all-MiniLM-L6-v2\n",
    "    )\n",
    "    print(\"‚úì Embeddings model loaded\")\n",
    "    \n",
    "    # Always delete existing vector store if it exists\n",
    "    store_path = \"gpu_datascience_vectorstore\"\n",
    "    \n",
    "    if os.path.exists(store_path):\n",
    "        print(f\"Deleting existing vector store at '{store_path}'...\")\n",
    "        try:\n",
    "            shutil.rmtree(store_path)\n",
    "            print(f\"‚úì Deleted existing vector store\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è  Warning: Could not delete existing store: {e}\")\n",
    "    \n",
    "    # Create new vector store\n",
    "    print(\"Creating new vector store...\")\n",
    "    vector_store = create_vector_store_batched(chunks, embeddings)\n",
    "    \n",
    "    # Save vector store\n",
    "    vector_store.save_local(store_path)\n",
    "    print(f\"‚úì Vector store saved to '{store_path}'\")\n",
    "    \n",
    "    print(f\"\\n‚úÖ Vector store ready!\")\n",
    "    print(f\"üìä Total vectors: {vector_store.index.ntotal}\")\n",
    "    print(f\"üìê Vector dimension: {vector_store.index.d}\")\n",
    "    print(f\"üíæ Saved to: {store_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64973064-a2ff-412d-b7f4-93e4f08b4308",
   "metadata": {},
   "source": [
    "## Model Loading\n",
    "\n",
    "This cell loads the Llama3-8B-Instruct model and tokenizer for response generation. The model is loaded with 16-bit precision and automatic device mapping for optimal GPU utilization.\n",
    "\n",
    "**Configuration:**\n",
    "- Model: Llama3-8B-Instruct from local path\n",
    "- Precision: torch.float16 for memory efficiency\n",
    "- Device mapping: Automatic GPU allocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d18fcdc7-a4b1-4bc0-98a2-158871def57a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "# Load your local Llama3 model (using your existing setup)\n",
    "model_name = '/data/datasets/community/huggingface/Llama3-8b-instruct/'  # Your model path\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16, device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e6fba4d-850d-4b0d-8d13-ee18f38e769c",
   "metadata": {},
   "source": [
    "## Enhanced Document Retrieval and Grading System\n",
    "\n",
    "This cell implements a sophisticated document retrieval system with intelligent grading, filtering, and context generation. The system uses multiple retrieval attempts with document exclusion to find the most relevant sources and generates enhanced context for response generation.\n",
    "\n",
    "**Core Components:**\n",
    "- Multi-attempt retrieval with document exclusion tracking\n",
    "- LLM-based document grading with strict relevance criteria\n",
    "- Quality filtering to keep only highly and somewhat relevant documents\n",
    "- Enhanced context generation with refined queries and generation instructions\n",
    "- Robust error handling and fallback mechanisms for parsing failures\n",
    "- Adaptive search expansion when insufficient relevant documents are found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c0022e09-7369-4b1c-9333-1ed7914dfd48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import json\n",
    "import re\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "def retrieve_node(question: str, vector_store, model, tokenizer, max_attempts: int = 3,\n",
    "                 k=5, exclude_ids=None, attempt_number=0, return_doc_ids=False):\n",
    "    \"\"\"\n",
    "    Enhanced retrieve node with document exclusion and tracking\n",
    "    \n",
    "    Args:\n",
    "        return_doc_ids (bool): If True, returns (context, doc_ids), else just context\n",
    "    \"\"\"\n",
    "    # Declare global variable at the top\n",
    "    global _current_doc_ids\n",
    "    \n",
    "    print(f\"ENHANCED RETRIEVE NODE: {question}\")\n",
    "    print(\"Target: At least 2 highly_relevant + drop not_relevant documents\")\n",
    "    \n",
    "    attempt = 0\n",
    "    \n",
    "    while attempt < max_attempts:\n",
    "        attempt += 1\n",
    "        print(f\"\\n--- Retrieval Attempt {attempt} ---\")\n",
    "        \n",
    "        # Step 1: Retrieve documents with exclusion logic\n",
    "        try:\n",
    "            # NEW: Add exclusion logic at the beginning\n",
    "            if exclude_ids:\n",
    "                print(f\"Excluding {len(exclude_ids)} previously retrieved documents\")\n",
    "                # Get more documents to account for exclusions\n",
    "                all_documents = vector_store.similarity_search(question, k=k*3)\n",
    "                filtered_docs = []\n",
    "                excluded_count = 0\n",
    "                \n",
    "                for doc in all_documents:\n",
    "                    doc_id = get_doc_id(doc)\n",
    "                    if doc_id not in exclude_ids:\n",
    "                        filtered_docs.append(doc)\n",
    "                        if len(filtered_docs) >= k:\n",
    "                            break\n",
    "                    else:\n",
    "                        excluded_count += 1\n",
    "                \n",
    "                documents = filtered_docs\n",
    "                print(f\"Excluded {excluded_count} documents, using {len(documents)} new documents\")\n",
    "            else:\n",
    "                # CHANGED: Use k parameter instead of hardcoded 5\n",
    "                documents = vector_store.similarity_search(question, k=k)\n",
    "                \n",
    "            print(f\"‚úì Retrieved {len(documents)} documents\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚úó Retrieval failed: {e}\")\n",
    "            if attempt >= max_attempts:\n",
    "                return f\"Unable to retrieve documents for: {question}\"\n",
    "            continue\n",
    "        \n",
    "        if len(documents) < 3:\n",
    "            print(f\"‚ö†Ô∏è  Only {len(documents)} documents found\")\n",
    "            if attempt >= max_attempts:\n",
    "                if documents:\n",
    "                    break\n",
    "                else:\n",
    "                    return f\"No documents found for: {question}\"\n",
    "        \n",
    "        # Step 2: Grade documents (YOUR EXISTING LOGIC)\n",
    "        grades = grade_documents_strict(question, documents, model, tokenizer)\n",
    "        \n",
    "        # Step 3: Filter out irrelevant documents (YOUR EXISTING LOGIC)\n",
    "        relevant_docs = filter_and_show_documents(documents, grades)\n",
    "        dropped_count = len(documents) - len(relevant_docs)\n",
    "        print(f\"  Immediately filtered: Kept {len(relevant_docs)}, dropped {dropped_count}\")\n",
    "        \n",
    "        # Step 4: Analyze the FILTERED results (YOUR EXISTING LOGIC)\n",
    "        highly_relevant_count = count_by_relevance(grades, \"highly_relevant\")\n",
    "        somewhat_relevant_count = count_by_relevance(grades, \"somewhat_relevant\")\n",
    "        \n",
    "        print(f\"Grading Analysis (after filtering):\")\n",
    "        print(f\"  Highly relevant: {highly_relevant_count}\")\n",
    "        print(f\"  Somewhat relevant: {somewhat_relevant_count}\") \n",
    "        print(f\"  Total relevant kept: {len(relevant_docs)}\")\n",
    "        \n",
    "        # Show breakdown of what we're keeping (YOUR EXISTING LOGIC)\n",
    "        for grade in grades.get(\"document_grades\", []):\n",
    "            relevance = grade.get(\"relevance\", \"unknown\")\n",
    "            if relevance in [\"highly_relevant\", \"somewhat_relevant\"]:\n",
    "                doc_idx = grade.get(\"doc_index\", \"?\")\n",
    "                reason = grade.get(\"reason\", \"\")[:50]\n",
    "                print(f\"  Kept Doc {doc_idx}: {relevance.upper()} - {reason}...\")\n",
    "        \n",
    "        # Step 5: Check quality requirements (YOUR EXISTING LOGIC)\n",
    "        if highly_relevant_count >= 1 and len(relevant_docs) >= 2:\n",
    "            print(f\"‚úì SUCCESS: Found {highly_relevant_count} highly relevant + {len(relevant_docs)} total relevant\")\n",
    "            \n",
    "            # Generate enhanced context with filtered documents (YOUR EXISTING LOGIC)\n",
    "            enhanced_context = create_clean_context(question, relevant_docs, model, tokenizer)\n",
    "            \n",
    "            # Track the document IDs we're using\n",
    "            used_doc_ids = [get_doc_id(doc) for doc in relevant_docs]\n",
    "            \n",
    "            print(f\"‚úì Enhanced context generated with {len(relevant_docs)} relevant documents\")\n",
    "            \n",
    "            if return_doc_ids:\n",
    "                return enhanced_context, used_doc_ids\n",
    "            else:\n",
    "                # For LangGraph compatibility, store in global variable\n",
    "                _current_doc_ids = used_doc_ids\n",
    "                return enhanced_context\n",
    "        \n",
    "        elif len(relevant_docs) >= 2:\n",
    "            print(f\"‚úì ACCEPTABLE: Found {len(relevant_docs)} relevant documents\")\n",
    "            \n",
    "            enhanced_context = create_clean_context(question, relevant_docs, model, tokenizer)\n",
    "            \n",
    "            # Track the document IDs we're using\n",
    "            used_doc_ids = [get_doc_id(doc) for doc in relevant_docs]\n",
    "            \n",
    "            print(f\"‚úì Enhanced context generated with {len(relevant_docs)} relevant documents\")\n",
    "            \n",
    "            if return_doc_ids:\n",
    "                return enhanced_context, used_doc_ids\n",
    "            else:\n",
    "                # For LangGraph compatibility, store in global variable\n",
    "                _current_doc_ids = used_doc_ids\n",
    "                return enhanced_context\n",
    "        \n",
    "        else:\n",
    "            print(f\"‚úó INSUFFICIENT: Only {len(relevant_docs)} relevant documents after filtering\")\n",
    "            \n",
    "            if attempt >= max_attempts:\n",
    "                print(\"Max attempts reached\")\n",
    "                if relevant_docs:\n",
    "                    enhanced_context = create_clean_context(question, relevant_docs, model, tokenizer)\n",
    "                    \n",
    "                    # Track the document IDs we're using\n",
    "                    used_doc_ids = [get_doc_id(doc) for doc in relevant_docs]\n",
    "                    \n",
    "                    print(f\"‚ö†Ô∏è  Using {len(relevant_docs)} available relevant documents\")\n",
    "                    \n",
    "                    if return_doc_ids:\n",
    "                        return enhanced_context, used_doc_ids\n",
    "                    else:\n",
    "                        # For LangGraph compatibility, store in global variable\n",
    "                        _current_doc_ids = used_doc_ids\n",
    "                        return enhanced_context\n",
    "                else:\n",
    "                    if return_doc_ids:\n",
    "                        return f\"No sufficiently relevant documents found for: {question}\", []\n",
    "                    else:\n",
    "                        return f\"No sufficiently relevant documents found for: {question}\"\n",
    "            else:\n",
    "                print(f\"Will search for more documents in next attempt...\")\n",
    "                # Continue with expanded search\n",
    "                k = min(k + 3, 15)\n",
    "    \n",
    "    if return_doc_ids:\n",
    "        return f\"Failed to find adequate documents for: {question}\", []\n",
    "    else:\n",
    "        return f\"Failed to find adequate documents for: {question}\"\n",
    "\n",
    "def grade_documents_strict(question: str, documents: List, model, tokenizer) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Grade documents with balanced criteria - not too strict\n",
    "    \"\"\"\n",
    "    print(f\"---DOCUMENT GRADING ({len(documents)} documents)---\")\n",
    "    \n",
    "    if not documents:\n",
    "        return {\"document_grades\": [], \"overall_assessment\": \"poor\"}\n",
    "    \n",
    "    # Grade only first 5 documents to avoid overwhelming the model\n",
    "    docs_to_grade = documents[:5]\n",
    "    docs_text = \"\"\n",
    "    for i, doc in enumerate(docs_to_grade, 1):\n",
    "        content = doc.page_content[:400]\n",
    "        docs_text += f\"=== DOCUMENT {i} ===\\n{content}...\\n\\n\"\n",
    "    \n",
    "    # More balanced grading prompt\n",
    "    system_prompt = f\"\"\"You are grading document relevance for a technical question. Grade each document:\n",
    "\n",
    "GRADING CRITERIA:\n",
    "- highly_relevant: Document contains specific, detailed information that directly addresses the question\n",
    "- somewhat_relevant: Document contains related information that provides useful context\n",
    "- not_relevant: Document is completely off-topic or unhelpful\n",
    "\n",
    "Be reasonable - if a document discusses the same technology area, it's at least somewhat_relevant.\n",
    "\n",
    "Respond with ONLY this JSON format:\n",
    "{{\"document_grades\": [{{\"doc_index\": 1, \"relevance\": \"somewhat_relevant\", \"reason\": \"brief reason\"}}, {{\"doc_index\": 2, \"relevance\": \"highly_relevant\", \"reason\": \"brief reason\"}}, {{\"doc_index\": 3, \"relevance\": \"not_relevant\", \"reason\": \"brief reason\"}}, {{\"doc_index\": 4, \"relevance\": \"somewhat_relevant\", \"reason\": \"brief reason\"}}, {{\"doc_index\": 5, \"relevance\": \"highly_relevant\", \"reason\": \"brief reason\"}}], \"overall_assessment\": \"good\"}}\"\"\"\n",
    "\n",
    "    user_prompt = f\"\"\"QUESTION: {question}\n",
    "\n",
    "DOCUMENTS TO GRADE:\n",
    "{docs_text}\n",
    "\n",
    "Grade ALL 5 documents. Be reasonable - related technical content should be at least somewhat_relevant.\"\"\"\n",
    "\n",
    "    try:\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ]\n",
    "        \n",
    "        input_ids = tokenizer.apply_chat_template(\n",
    "            messages, add_generation_prompt=True, return_tensors=\"pt\"\n",
    "        ).to(model.device)\n",
    "        \n",
    "        attention_mask = torch.ones_like(input_ids)\n",
    "        \n",
    "        outputs = model.generate(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            max_new_tokens=1200,\n",
    "            eos_token_id=[tokenizer.eos_token_id, tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")],\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            do_sample=False\n",
    "        )\n",
    "        \n",
    "        response_tokens = outputs[0][len(input_ids[0]):]\n",
    "        response = tokenizer.decode(response_tokens, skip_special_tokens=True).strip()\n",
    "        \n",
    "        print(f\"  Grading response: {response[:150]}...\")\n",
    "        \n",
    "        # Parse grades for the 5 documents we graded\n",
    "        grades = parse_grades_safely(response, len(docs_to_grade))\n",
    "        \n",
    "        # For documents beyond the first 5, mark as not_relevant\n",
    "        if len(documents) > 5:\n",
    "            additional_grades = []\n",
    "            for i in range(6, len(documents) + 1):\n",
    "                additional_grades.append({\n",
    "                    \"doc_index\": i,\n",
    "                    \"relevance\": \"not_relevant\", \n",
    "                    \"reason\": \"not graded - beyond top 5\"\n",
    "                })\n",
    "            grades[\"document_grades\"].extend(additional_grades)\n",
    "        \n",
    "        return grades\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚úó Grading failed: {e}\")\n",
    "        return create_fallback_grades(len(documents))\n",
    "\n",
    "def parse_grades_safely(response: str, num_docs: int) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Parse grading response safely without regex issues\n",
    "    \"\"\"\n",
    "    # Try to extract JSON\n",
    "    json_start = response.find('{')\n",
    "    json_end = response.rfind('}') + 1\n",
    "    \n",
    "    if json_start != -1 and json_end > json_start:\n",
    "        try:\n",
    "            json_text = response[json_start:json_end]\n",
    "            grades = json.loads(json_text)\n",
    "            \n",
    "            if \"document_grades\" in grades and isinstance(grades[\"document_grades\"], list):\n",
    "                return complete_missing_grades(grades, num_docs)\n",
    "        except json.JSONDecodeError:\n",
    "            pass\n",
    "    \n",
    "    # Fallback parsing\n",
    "    return extract_grades_from_text(response, num_docs)\n",
    "\n",
    "def extract_grades_from_text(text: str, num_docs: int) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Extract grades from text when JSON parsing fails\n",
    "    \"\"\"\n",
    "    grades = []\n",
    "    \n",
    "    # Look for patterns like \"doc_index\": 1, \"relevance\": \"highly_relevant\"\n",
    "    for i in range(1, num_docs + 1):\n",
    "        if f'\"doc_index\": {i}' in text or f'\"doc_index\":{i}' in text:\n",
    "            if 'highly_relevant' in text:\n",
    "                relevance = \"highly_relevant\"\n",
    "            elif 'somewhat_relevant' in text:\n",
    "                relevance = \"somewhat_relevant\"\n",
    "            else:\n",
    "                relevance = \"not_relevant\"\n",
    "            \n",
    "            grades.append({\n",
    "                \"doc_index\": i,\n",
    "                \"relevance\": relevance,\n",
    "                \"reason\": \"extracted from text\"\n",
    "            })\n",
    "        else:\n",
    "            grades.append({\n",
    "                \"doc_index\": i,\n",
    "                \"relevance\": \"not_relevant\",\n",
    "                \"reason\": \"not found in response\"\n",
    "            })\n",
    "    \n",
    "    return {\n",
    "        \"document_grades\": grades,\n",
    "        \"overall_assessment\": \"acceptable\"\n",
    "    }\n",
    "\n",
    "def complete_missing_grades(grades: Dict[str, Any], expected_docs: int) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Fill in missing document grades\n",
    "    \"\"\"\n",
    "    doc_grades = grades.get(\"document_grades\", [])\n",
    "    graded_docs = {grade.get(\"doc_index\"): grade for grade in doc_grades \n",
    "                   if isinstance(grade.get(\"doc_index\"), int)}\n",
    "    \n",
    "    complete_grades = []\n",
    "    for i in range(1, expected_docs + 1):\n",
    "        if i in graded_docs:\n",
    "            grade = graded_docs[i]\n",
    "            relevance = grade.get(\"relevance\", \"not_relevant\")\n",
    "            if relevance not in [\"highly_relevant\", \"somewhat_relevant\", \"not_relevant\"]:\n",
    "                relevance = \"not_relevant\"\n",
    "            complete_grades.append({\n",
    "                \"doc_index\": i,\n",
    "                \"relevance\": relevance,\n",
    "                \"reason\": grade.get(\"reason\", \"graded by model\")\n",
    "            })\n",
    "        else:\n",
    "            complete_grades.append({\n",
    "                \"doc_index\": i,\n",
    "                \"relevance\": \"not_relevant\",\n",
    "                \"reason\": \"not graded by model\"\n",
    "            })\n",
    "    \n",
    "    return {\n",
    "        \"document_grades\": complete_grades,\n",
    "        \"overall_assessment\": grades.get(\"overall_assessment\", \"acceptable\")\n",
    "    }\n",
    "\n",
    "def create_fallback_grades(num_docs: int) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Create conservative fallback grades\n",
    "    \"\"\"\n",
    "    return {\n",
    "        \"document_grades\": [\n",
    "            {\"doc_index\": i, \"relevance\": \"not_relevant\", \"reason\": \"grading failed\"}\n",
    "            for i in range(1, num_docs + 1)\n",
    "        ],\n",
    "        \"overall_assessment\": \"poor\"\n",
    "    }\n",
    "\n",
    "def count_by_relevance(grades: Dict[str, Any], relevance_level: str) -> int:\n",
    "    \"\"\"\n",
    "    Count documents by relevance level\n",
    "    \"\"\"\n",
    "    return sum(1 for g in grades.get(\"document_grades\", [])\n",
    "               if g.get(\"relevance\") == relevance_level)\n",
    "\n",
    "def filter_and_show_documents(documents: List, grades: Dict[str, Any]) -> List:\n",
    "    \"\"\"\n",
    "    Filter documents and show what's being kept/dropped\n",
    "    \"\"\"\n",
    "    relevant_docs = []\n",
    "    \n",
    "    print(\"  Document filtering:\")\n",
    "    for grade in grades.get(\"document_grades\", []):\n",
    "        doc_index = grade.get(\"doc_index\", 0) - 1\n",
    "        relevance = grade.get(\"relevance\", \"not_relevant\")\n",
    "        \n",
    "        if 0 <= doc_index < len(documents):\n",
    "            if relevance in [\"highly_relevant\", \"somewhat_relevant\"]:\n",
    "                relevant_docs.append(documents[doc_index])\n",
    "                print(f\"    ‚úì Keeping Doc {doc_index + 1}: {relevance}\")\n",
    "            else:\n",
    "                print(f\"    ‚úó Dropping Doc {doc_index + 1}: {relevance}\")\n",
    "    \n",
    "    dropped_count = len(documents) - len(relevant_docs)\n",
    "    print(f\"  Result: Kept {len(relevant_docs)} relevant, dropped {dropped_count} irrelevant\")\n",
    "    return relevant_docs\n",
    "\n",
    "def create_clean_context(question: str, relevant_docs: List, model, tokenizer) -> str:\n",
    "    \"\"\"\n",
    "    Create clean, formatted context without regex issues\n",
    "    \"\"\"\n",
    "    print(\"---GENERATING ENHANCED CONTEXT---\")\n",
    "    \n",
    "    if not relevant_docs:\n",
    "        return f\"No relevant documents found for: {question}\"\n",
    "    \n",
    "    # Create knowledge base section\n",
    "    knowledge_base = \"=== KNOWLEDGE BASE ===\\n\\n\"\n",
    "    \n",
    "    for i, doc in enumerate(relevant_docs, 1):\n",
    "        content = doc.page_content.strip()\n",
    "        # Clean whitespace without complex regex\n",
    "        content = ' '.join(content.split())\n",
    "        knowledge_base += f\"### Source {i}:\\n{content}\\n\\n\"\n",
    "    \n",
    "    # Generate refined query\n",
    "    refined_query = generate_refined_query(question, model, tokenizer)\n",
    "    \n",
    "    # Create instructions\n",
    "    instructions = get_generation_instructions(question)\n",
    "    \n",
    "    # Assemble context\n",
    "    enhanced_context = f\"\"\"{knowledge_base}=== ENHANCED QUERY ===\n",
    "{refined_query}\n",
    "\n",
    "=== GENERATION INSTRUCTIONS ===\n",
    "{instructions}\"\"\"\n",
    "    \n",
    "    print(f\"‚úì Enhanced context generated with clean formatting\")\n",
    "    return enhanced_context\n",
    "\n",
    "def generate_refined_query(question: str, model, tokenizer) -> str:\n",
    "    \"\"\"\n",
    "    Generate refined query without regex issues\n",
    "    \"\"\"\n",
    "    try:\n",
    "        prompt = f\"\"\"Refine this technical question to be more specific and comprehensive:\n",
    "\n",
    "Original: {question}\n",
    "\n",
    "Create a refined version that asks for more detailed information. Respond with ONLY the refined question.\"\"\"\n",
    "\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You refine technical questions to be more comprehensive.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "        \n",
    "        input_ids = tokenizer.apply_chat_template(\n",
    "            messages, add_generation_prompt=True, return_tensors=\"pt\"\n",
    "        ).to(model.device)\n",
    "        \n",
    "        attention_mask = torch.ones_like(input_ids)\n",
    "        \n",
    "        outputs = model.generate(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            max_new_tokens=150,\n",
    "            eos_token_id=[tokenizer.eos_token_id, tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")],\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            do_sample=True,\n",
    "            temperature=0.3\n",
    "        )\n",
    "        \n",
    "        response_tokens = outputs[0][len(input_ids[0]):]\n",
    "        refined = tokenizer.decode(response_tokens, skip_special_tokens=True).strip()\n",
    "        \n",
    "        # Clean without regex\n",
    "        refined = refined.strip(' \\'\"')\n",
    "        return refined if refined else question\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Query refinement failed: {e}\")\n",
    "        return question\n",
    "\n",
    "def get_generation_instructions(question: str) -> str:\n",
    "    \"\"\"\n",
    "    Get generation instructions based on question type\n",
    "    \"\"\"\n",
    "    q_lower = question.lower()\n",
    "    \n",
    "    base = \"\"\"Using the provided sources, generate a comprehensive response that:\n",
    "\n",
    "1. **Directly addresses the question**\n",
    "2. **Uses source material effectively** \n",
    "3. **Maintains technical accuracy**\"\"\"\n",
    "    \n",
    "    if any(word in q_lower for word in ['write', 'code', 'implement', 'create']):\n",
    "        specific = \"\"\"\n",
    "4. **Provide working code examples**\n",
    "5. **Explain code structure and logic**\n",
    "6. **Include usage examples**\n",
    "7. **Mention best practices**\"\"\"\n",
    "    elif any(word in q_lower for word in ['optimize', 'performance', 'improve']):\n",
    "        specific = \"\"\"\n",
    "4. **Detail optimization techniques**\n",
    "5. **Discuss performance considerations**\n",
    "6. **Include measurement methods**\n",
    "7. **Give practical recommendations**\"\"\"\n",
    "    else:\n",
    "        specific = \"\"\"\n",
    "4. **Build from fundamentals**\n",
    "5. **Use clear examples**\n",
    "6. **Explain relationships**\n",
    "7. **Summarize key points**\"\"\"\n",
    "    \n",
    "    return base + specific"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ef17f8c-9778-4d91-8f2d-e8babbbe9cb2",
   "metadata": {},
   "source": [
    "## Test Cases for Verifying Retrieve Node\n",
    "\n",
    "This block presents a handful of questions pertaining to the knowledge base ingested.\n",
    "This provides an opportunity to verify if the node runs correctly\n",
    "\n",
    "**Test Topics**\n",
    "- CUDA Programming\n",
    "- CuPy\n",
    "- PyTorch\n",
    "- ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2eb4520c-2f69-4f33-a80b-3035cc1ddc59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TESTING RETRIEVE NODE\n",
      "================================================================================\n",
      "\n",
      "[TEST  1] How to optimize CUDA memory usage?\n",
      "--------------------------------------------------------------------------------\n",
      "ENHANCED RETRIEVE NODE: How to optimize CUDA memory usage?\n",
      "Target: At least 2 highly_relevant + drop not_relevant documents\n",
      "\n",
      "--- Retrieval Attempt 1 ---\n",
      "‚úì Retrieved 5 documents\n",
      "---DOCUMENT GRADING (5 documents)---\n",
      "  Grading response: {\"document_grades\": [{\"doc_index\": 1, \"relevance\": \"not_relevant\", \"reason\": \"GitHub repository overview, not specific to memory usage optimization\"},...\n",
      "  Document filtering:\n",
      "    ‚úó Dropping Doc 1: not_relevant\n",
      "    ‚úì Keeping Doc 2: highly_relevant\n",
      "    ‚úì Keeping Doc 3: somewhat_relevant\n",
      "    ‚úó Dropping Doc 4: not_relevant\n",
      "    ‚úì Keeping Doc 5: highly_relevant\n",
      "  Result: Kept 3 relevant, dropped 2 irrelevant\n",
      "  Immediately filtered: Kept 3, dropped 2\n",
      "Grading Analysis (after filtering):\n",
      "  Highly relevant: 2\n",
      "  Somewhat relevant: 1\n",
      "  Total relevant kept: 3\n",
      "  Kept Doc 2: HIGHLY_RELEVANT - Code snippet showing memory optimization technique...\n",
      "  Kept Doc 3: SOMEWHAT_RELEVANT - Code snippet showing how to disable memory pools, ...\n",
      "  Kept Doc 5: HIGHLY_RELEVANT - Article discussing memory optimization techniques,...\n",
      "‚úì SUCCESS: Found 2 highly relevant + 3 total relevant\n",
      "---GENERATING ENHANCED CONTEXT---\n",
      "‚úì Enhanced context generated with clean formatting\n",
      "‚úì Enhanced context generated with 3 relevant documents\n",
      "‚úì Context generated successfully\n",
      "  - Length: 2,522 characters\n",
      "  - Sources: 3\n",
      "  - KB Used: Yes\n",
      "  - Type: Unknown\n",
      "  - Preview: === KNOWLEDGE BASE ===  ### Source 1: . Can we reduce the memory footprint by casting it to torch.float instead? model = MyModule(500, 10).cuda() input = torch.rand(128, 500).cuda() mask = torch.rand((500, 500, 500), dtype=torch.float).cuda() # warm-up model(input, mask) with profiler.profile(with_s...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[TEST  2] CuPy array operations best practices\n",
      "--------------------------------------------------------------------------------\n",
      "ENHANCED RETRIEVE NODE: CuPy array operations best practices\n",
      "Target: At least 2 highly_relevant + drop not_relevant documents\n",
      "\n",
      "--- Retrieval Attempt 1 ---\n",
      "‚úì Retrieved 5 documents\n",
      "---DOCUMENT GRADING (5 documents)---\n",
      "  Grading response: {\"document_grades\": [{\"doc_index\": 1, \"relevance\": \"somewhat_relevant\", \"reason\": \"Discusses device management and performance tradeoffs, relevant to ...\n",
      "  Document filtering:\n",
      "    ‚úì Keeping Doc 1: somewhat_relevant\n",
      "    ‚úì Keeping Doc 2: highly_relevant\n",
      "    ‚úì Keeping Doc 3: somewhat_relevant\n",
      "    ‚úì Keeping Doc 4: somewhat_relevant\n",
      "    ‚úó Dropping Doc 5: not_relevant\n",
      "  Result: Kept 4 relevant, dropped 1 irrelevant\n",
      "  Immediately filtered: Kept 4, dropped 1\n",
      "Grading Analysis (after filtering):\n",
      "  Highly relevant: 1\n",
      "  Somewhat relevant: 3\n",
      "  Total relevant kept: 4\n",
      "  Kept Doc 1: SOMEWHAT_RELEVANT - Discusses device management and performance tradeo...\n",
      "  Kept Doc 2: HIGHLY_RELEVANT - Provides specific guidance on CuPy operations and ...\n",
      "  Kept Doc 3: SOMEWHAT_RELEVANT - Describes a method for converting CuPy arrays to N...\n",
      "  Kept Doc 4: SOMEWHAT_RELEVANT - Provides general information on CuPy ndarrays, rel...\n",
      "‚úì SUCCESS: Found 1 highly relevant + 4 total relevant\n",
      "---GENERATING ENHANCED CONTEXT---\n",
      "‚úì Enhanced context generated with clean formatting\n",
      "‚úì Enhanced context generated with 4 relevant documents\n",
      "‚úì Context generated successfully\n",
      "  - Length: 5,073 characters\n",
      "  - Sources: 4\n",
      "  - KB Used: Yes\n",
      "  - Type: Unknown\n",
      "  - Preview: === KNOWLEDGE BASE ===  ### Source 1: . ', '', '## Moving Data from Current Device', '', 'In general, CuPy functions expect that the array is on the same device as the current one. Similar to passing data from CPU to GPU or vice versa, passing an array stored on a non-current device may impact perfo...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[TEST  3] PyTorch GPU training optimization techniques\n",
      "--------------------------------------------------------------------------------\n",
      "ENHANCED RETRIEVE NODE: PyTorch GPU training optimization techniques\n",
      "Target: At least 2 highly_relevant + drop not_relevant documents\n",
      "\n",
      "--- Retrieval Attempt 1 ---\n",
      "‚úì Retrieved 5 documents\n",
      "---DOCUMENT GRADING (5 documents)---\n",
      "  Grading response: {\"document_grades\": [{\"doc_index\": 1, \"relevance\": \"not_relevant\", \"reason\": \"The document appears to be a general guide on profiling and optimizing P...\n",
      "  Document filtering:\n",
      "    ‚úì Keeping Doc 1: highly_relevant\n",
      "    ‚úì Keeping Doc 2: highly_relevant\n",
      "    ‚úì Keeping Doc 3: highly_relevant\n",
      "    ‚úì Keeping Doc 4: highly_relevant\n",
      "    ‚úì Keeping Doc 5: highly_relevant\n",
      "  Result: Kept 5 relevant, dropped 0 irrelevant\n",
      "  Immediately filtered: Kept 5, dropped 0\n",
      "Grading Analysis (after filtering):\n",
      "  Highly relevant: 5\n",
      "  Somewhat relevant: 0\n",
      "  Total relevant kept: 5\n",
      "  Kept Doc 1: HIGHLY_RELEVANT - extracted from text...\n",
      "  Kept Doc 2: HIGHLY_RELEVANT - extracted from text...\n",
      "  Kept Doc 3: HIGHLY_RELEVANT - extracted from text...\n",
      "  Kept Doc 4: HIGHLY_RELEVANT - extracted from text...\n",
      "  Kept Doc 5: HIGHLY_RELEVANT - extracted from text...\n",
      "‚úì SUCCESS: Found 5 highly relevant + 5 total relevant\n",
      "---GENERATING ENHANCED CONTEXT---\n",
      "‚úì Enhanced context generated with clean formatting\n",
      "‚úì Enhanced context generated with 5 relevant documents\n",
      "‚úì Context generated successfully\n",
      "  - Length: 6,129 characters\n",
      "  - Sources: 5\n",
      "  - KB Used: Yes\n",
      "  - Type: Unknown\n",
      "  - Preview: === KNOWLEDGE BASE ===  ### Source 1: . Prepare the data and model 2. Use profiler to record execution events 3. Run the profiler 4. Use TensorBoard to view results and analyze model performance 5. Improve performance with the help of profiler 6. Analyze performance with other advanced features 7. A...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[TEST  4] RAPIDS cuDF dataframe operations tutorial\n",
      "--------------------------------------------------------------------------------\n",
      "ENHANCED RETRIEVE NODE: RAPIDS cuDF dataframe operations tutorial\n",
      "Target: At least 2 highly_relevant + drop not_relevant documents\n",
      "\n",
      "--- Retrieval Attempt 1 ---\n",
      "‚úì Retrieved 5 documents\n",
      "---DOCUMENT GRADING (5 documents)---\n",
      "  Grading response: {\"document_grades\": [{\"doc_index\": 1, \"relevance\": \"somewhat_relevant\", \"reason\": \"mentions cuDF and provides installation guidance\"}, {\"doc_index\": 2...\n",
      "  Document filtering:\n",
      "    ‚úì Keeping Doc 1: somewhat_relevant\n",
      "    ‚úì Keeping Doc 2: highly_relevant\n",
      "    ‚úì Keeping Doc 3: somewhat_relevant\n",
      "    ‚úì Keeping Doc 4: highly_relevant\n",
      "    ‚úó Dropping Doc 5: not_relevant\n",
      "  Result: Kept 4 relevant, dropped 1 irrelevant\n",
      "  Immediately filtered: Kept 4, dropped 1\n",
      "Grading Analysis (after filtering):\n",
      "  Highly relevant: 2\n",
      "  Somewhat relevant: 2\n",
      "  Total relevant kept: 4\n",
      "  Kept Doc 1: SOMEWHAT_RELEVANT - mentions cuDF and provides installation guidance...\n",
      "  Kept Doc 2: HIGHLY_RELEVANT - specifically mentions RAPIDS cuDF for accelerated ...\n",
      "  Kept Doc 3: SOMEWHAT_RELEVANT - provides technical details about cuDF, including s...\n",
      "  Kept Doc 4: HIGHLY_RELEVANT - provides a detailed overview of cuDF, its features...\n",
      "‚úì SUCCESS: Found 2 highly relevant + 4 total relevant\n",
      "---GENERATING ENHANCED CONTEXT---\n",
      "‚úì Enhanced context generated with clean formatting\n",
      "‚úì Enhanced context generated with 4 relevant documents\n",
      "‚úì Context generated successfully\n",
      "  - Length: 5,277 characters\n",
      "  - Sources: 5\n",
      "  - KB Used: Yes\n",
      "  - Type: Unknown\n",
      "  - Preview: === KNOWLEDGE BASE ===  ### Source 1: import cudf completes successfully in a new code block, and then you are ready to go. If you run into any trouble, please reach out in the RAPIDS Slack and we‚Äôll help you get things working correctly. Running 10 minutes to cuDF on Colab Now that you have a worki...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[TEST  5] Numba CUDA kernel programming examples\n",
      "--------------------------------------------------------------------------------\n",
      "ENHANCED RETRIEVE NODE: Numba CUDA kernel programming examples\n",
      "Target: At least 2 highly_relevant + drop not_relevant documents\n",
      "\n",
      "--- Retrieval Attempt 1 ---\n",
      "‚úì Retrieved 5 documents\n",
      "---DOCUMENT GRADING (5 documents)---\n",
      "  Grading response: {\"document_grades\": [{\"doc_index\": 1, \"relevance\": \"highly_relevant\", \"reason\": \"Document provides specific information about Numba CUDA kernel progra...\n",
      "  Document filtering:\n",
      "    ‚úì Keeping Doc 1: highly_relevant\n",
      "    ‚úì Keeping Doc 2: highly_relevant\n",
      "    ‚úì Keeping Doc 3: somewhat_relevant\n",
      "    ‚úó Dropping Doc 4: not_relevant\n",
      "    ‚úì Keeping Doc 5: highly_relevant\n",
      "  Result: Kept 4 relevant, dropped 1 irrelevant\n",
      "  Immediately filtered: Kept 4, dropped 1\n",
      "Grading Analysis (after filtering):\n",
      "  Highly relevant: 3\n",
      "  Somewhat relevant: 1\n",
      "  Total relevant kept: 4\n",
      "  Kept Doc 1: HIGHLY_RELEVANT - Document provides specific information about Numba...\n",
      "  Kept Doc 2: HIGHLY_RELEVANT - Document provides a code example of a Numba CUDA k...\n",
      "  Kept Doc 3: SOMEWHAT_RELEVANT - Document is a tutorial on CUDA kernels, but it's n...\n",
      "  Kept Doc 5: HIGHLY_RELEVANT - Document provides a detailed explanation of creati...\n",
      "‚úì SUCCESS: Found 3 highly relevant + 4 total relevant\n",
      "---GENERATING ENHANCED CONTEXT---\n",
      "‚úì Enhanced context generated with clean formatting\n",
      "‚úì Enhanced context generated with 4 relevant documents\n",
      "‚úì Context generated successfully\n",
      "  - Length: 3,288 characters\n",
      "  - Sources: 4\n",
      "  - KB Used: Yes\n",
      "  - Type: Unknown\n",
      "  - Preview: === KNOWLEDGE BASE ===  ### Source 1: Examples ‚Äî Numba 0+untagged.1510.g1e70d8c.dirty documentation Numba for CUDA GPUs Examples View page source ExamplesÔÉÅ CUDA Built-in Target deprecation notice The CUDA target built-in to Numba is deprecated, with further development moved to the NVIDIA numba-cuda...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[TEST  6] TensorFlow GPU memory optimization strategies\n",
      "--------------------------------------------------------------------------------\n",
      "ENHANCED RETRIEVE NODE: TensorFlow GPU memory optimization strategies\n",
      "Target: At least 2 highly_relevant + drop not_relevant documents\n",
      "\n",
      "--- Retrieval Attempt 1 ---\n",
      "‚úì Retrieved 5 documents\n",
      "---DOCUMENT GRADING (5 documents)---\n",
      "  Grading response: {\"document_grades\": [{\"doc_index\": 1, \"relevance\": \"highly_relevant\", \"reason\": \"The document discusses memory optimization strategies in TensorFlow, ...\n",
      "  Document filtering:\n",
      "    ‚úì Keeping Doc 1: highly_relevant\n",
      "    ‚úì Keeping Doc 2: highly_relevant\n",
      "    ‚úì Keeping Doc 3: somewhat_relevant\n",
      "    ‚úì Keeping Doc 4: highly_relevant\n",
      "    ‚úì Keeping Doc 5: highly_relevant\n",
      "  Result: Kept 5 relevant, dropped 0 irrelevant\n",
      "  Immediately filtered: Kept 5, dropped 0\n",
      "Grading Analysis (after filtering):\n",
      "  Highly relevant: 4\n",
      "  Somewhat relevant: 1\n",
      "  Total relevant kept: 5\n",
      "  Kept Doc 1: HIGHLY_RELEVANT - The document discusses memory optimization strateg...\n",
      "  Kept Doc 2: HIGHLY_RELEVANT - The document provides a specific solution for cont...\n",
      "  Kept Doc 3: SOMEWHAT_RELEVANT - The document discusses system architecture and its...\n",
      "  Kept Doc 4: HIGHLY_RELEVANT - The document provides practical advice on monitori...\n",
      "  Kept Doc 5: HIGHLY_RELEVANT - The document provides a code snippet for activatin...\n",
      "‚úì SUCCESS: Found 4 highly relevant + 5 total relevant\n",
      "---GENERATING ENHANCED CONTEXT---\n",
      "‚úì Enhanced context generated with clean formatting\n",
      "‚úì Enhanced context generated with 5 relevant documents\n",
      "‚úì Context generated successfully\n",
      "  - Length: 6,845 characters\n",
      "  - Sources: 5\n",
      "  - KB Used: Yes\n",
      "  - Type: Unknown\n",
      "  - Preview: === KNOWLEDGE BASE ===  ### Source 1: . Say you have a model and you‚Äôre interested in ways to optimize memory to avoid Out of Memory (OOM) errors or simply to ooze more out of your GPU. Well, you _might_ be in luck (if gradients take up a portion of your memory and you do not need to do gradient acc...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[TEST  7] JAX GPU acceleration tutorial examples\n",
      "--------------------------------------------------------------------------------\n",
      "ENHANCED RETRIEVE NODE: JAX GPU acceleration tutorial examples\n",
      "Target: At least 2 highly_relevant + drop not_relevant documents\n",
      "\n",
      "--- Retrieval Attempt 1 ---\n",
      "‚úì Retrieved 5 documents\n",
      "---DOCUMENT GRADING (5 documents)---\n",
      "  Grading response: {\"document_grades\": [{\"doc_index\": 1, \"relevance\": \"not_relevant\", \"reason\": \"Intel GPU instructions, unrelated to JAX GPU acceleration tutorial examp...\n",
      "  Document filtering:\n",
      "    ‚úó Dropping Doc 1: not_relevant\n",
      "    ‚úì Keeping Doc 2: somewhat_relevant\n",
      "    ‚úì Keeping Doc 3: highly_relevant\n",
      "    ‚úì Keeping Doc 4: highly_relevant\n",
      "    ‚úì Keeping Doc 5: somewhat_relevant\n",
      "  Result: Kept 4 relevant, dropped 1 irrelevant\n",
      "  Immediately filtered: Kept 4, dropped 1\n",
      "Grading Analysis (after filtering):\n",
      "  Highly relevant: 2\n",
      "  Somewhat relevant: 2\n",
      "  Total relevant kept: 4\n",
      "  Kept Doc 2: SOMEWHAT_RELEVANT - References JAX documentation, but doesn't provide ...\n",
      "  Kept Doc 3: HIGHLY_RELEVANT - Provides specific information on GPU memory alloca...\n",
      "  Kept Doc 4: HIGHLY_RELEVANT - Provides code examples and information on using JA...\n",
      "  Kept Doc 5: SOMEWHAT_RELEVANT - Provides general information on GPU computing, but...\n",
      "‚úì SUCCESS: Found 2 highly relevant + 4 total relevant\n",
      "---GENERATING ENHANCED CONTEXT---\n",
      "‚úì Enhanced context generated with clean formatting\n",
      "‚úì Enhanced context generated with 4 relevant documents\n",
      "‚úì Context generated successfully\n",
      "  - Length: 6,015 characters\n",
      "  - Sources: 4\n",
      "  - KB Used: Yes\n",
      "  - Type: Unknown\n",
      "  - Preview: === KNOWLEDGE BASE ===  ### Source 1: . We're currently working on covering JAX's ideas and capabilities in a more comprehensive and up-to-date paper. Reference documentation For details about the JAX API, see the reference documentation. For getting started as a JAX developer, see the developer doc...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[TEST  8] GPU memory hierarchy architecture explanation\n",
      "--------------------------------------------------------------------------------\n",
      "ENHANCED RETRIEVE NODE: GPU memory hierarchy architecture explanation\n",
      "Target: At least 2 highly_relevant + drop not_relevant documents\n",
      "\n",
      "--- Retrieval Attempt 1 ---\n",
      "‚úì Retrieved 5 documents\n",
      "---DOCUMENT GRADING (5 documents)---\n",
      "  Grading response: {\"document_grades\": [{\"doc_index\": 1, \"relevance\": \"somewhat_relevant\", \"reason\": \"Discusses GPU memory hierarchy, but focuses on NVIDIA GPU specifics...\n",
      "  Document filtering:\n",
      "    ‚úì Keeping Doc 1: somewhat_relevant\n",
      "    ‚úó Dropping Doc 2: not_relevant\n",
      "    ‚úì Keeping Doc 3: highly_relevant\n",
      "    ‚úì Keeping Doc 4: somewhat_relevant\n",
      "    ‚úó Dropping Doc 5: not_relevant\n",
      "  Result: Kept 3 relevant, dropped 2 irrelevant\n",
      "  Immediately filtered: Kept 3, dropped 2\n",
      "Grading Analysis (after filtering):\n",
      "  Highly relevant: 1\n",
      "  Somewhat relevant: 2\n",
      "  Total relevant kept: 3\n",
      "  Kept Doc 1: SOMEWHAT_RELEVANT - Discusses GPU memory hierarchy, but focuses on NVI...\n",
      "  Kept Doc 3: HIGHLY_RELEVANT - Provides detailed explanation of GPU architecture,...\n",
      "  Kept Doc 4: SOMEWHAT_RELEVANT - Discusses data transfer between CPU and GPU, but d...\n",
      "‚úì SUCCESS: Found 1 highly relevant + 3 total relevant\n",
      "---GENERATING ENHANCED CONTEXT---\n",
      "‚úì Enhanced context generated with clean formatting\n",
      "‚úì Enhanced context generated with 3 relevant documents\n",
      "‚úì Context generated successfully\n",
      "  - Length: 5,021 characters\n",
      "  - Sources: 3\n",
      "  - KB Used: Yes\n",
      "  - Type: Unknown\n",
      "  - Preview: === KNOWLEDGE BASE ===  ### Source 1: . In the case of an NVIDIA GPU, the shared memory, the L1 cache and the Constant memory cache are within the streaming multiprocessor block. Hence they are faster than the L2 cache, and GPU RAM', '- Textures - Read-only memory optimized for filtering, interpolat...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[TEST  9] Implement GPU-accelerated matrix operations\n",
      "--------------------------------------------------------------------------------\n",
      "ENHANCED RETRIEVE NODE: Implement GPU-accelerated matrix operations\n",
      "Target: At least 2 highly_relevant + drop not_relevant documents\n",
      "\n",
      "--- Retrieval Attempt 1 ---\n",
      "‚úì Retrieved 5 documents\n",
      "---DOCUMENT GRADING (5 documents)---\n",
      "  Grading response: {\"document_grades\": [{\"doc_index\": 1, \"relevance\": \"highly_relevant\", \"reason\": \"Provides detailed information on matrix multiplication, including GPU...\n",
      "  Document filtering:\n",
      "    ‚úì Keeping Doc 1: highly_relevant\n",
      "    ‚úì Keeping Doc 2: highly_relevant\n",
      "    ‚úì Keeping Doc 3: highly_relevant\n",
      "    ‚úì Keeping Doc 4: somewhat_relevant\n",
      "    ‚úó Dropping Doc 5: not_relevant\n",
      "  Result: Kept 4 relevant, dropped 1 irrelevant\n",
      "  Immediately filtered: Kept 4, dropped 1\n",
      "Grading Analysis (after filtering):\n",
      "  Highly relevant: 3\n",
      "  Somewhat relevant: 1\n",
      "  Total relevant kept: 4\n",
      "  Kept Doc 1: HIGHLY_RELEVANT - Provides detailed information on matrix multiplica...\n",
      "  Kept Doc 2: HIGHLY_RELEVANT - Comprehensive resources on matrix multiplication, ...\n",
      "  Kept Doc 3: HIGHLY_RELEVANT - Provides a specific implementation of matrix multi...\n",
      "  Kept Doc 4: SOMEWHAT_RELEVANT - Discusses the use of shared memory for matrix mult...\n",
      "‚úì SUCCESS: Found 3 highly relevant + 4 total relevant\n",
      "---GENERATING ENHANCED CONTEXT---\n",
      "‚úì Enhanced context generated with clean formatting\n",
      "‚úì Enhanced context generated with 4 relevant documents\n",
      "‚úì Context generated successfully\n",
      "  - Length: 5,782 characters\n",
      "  - Sources: 4\n",
      "  - KB Used: Yes\n",
      "  - Type: Unknown\n",
      "  - Preview: === KNOWLEDGE BASE ===  ### Source 1: . Matmul This section focuses on understanding the fundamentals and optimization of matrix multiplication (Matmul), a cornerstone operation in CUDA programming and high-performance computing (HPC). The provided resources cover both CPU implementations and GPU op...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[TEST 10] Explain GPU compute capability differences\n",
      "--------------------------------------------------------------------------------\n",
      "ENHANCED RETRIEVE NODE: Explain GPU compute capability differences\n",
      "Target: At least 2 highly_relevant + drop not_relevant documents\n",
      "\n",
      "--- Retrieval Attempt 1 ---\n",
      "‚úì Retrieved 5 documents\n",
      "---DOCUMENT GRADING (5 documents)---\n",
      "  Grading response: {\"document_grades\": [{\"doc_index\": 1, \"relevance\": \"somewhat_relevant\", \"reason\": \"Discusses GPU strengths, including parallel processing, which is re...\n",
      "  Document filtering:\n",
      "    ‚úì Keeping Doc 1: highly_relevant\n",
      "    ‚úì Keeping Doc 2: highly_relevant\n",
      "    ‚úì Keeping Doc 3: highly_relevant\n",
      "    ‚úì Keeping Doc 4: highly_relevant\n",
      "    ‚úì Keeping Doc 5: highly_relevant\n",
      "  Result: Kept 5 relevant, dropped 0 irrelevant\n",
      "  Immediately filtered: Kept 5, dropped 0\n",
      "Grading Analysis (after filtering):\n",
      "  Highly relevant: 5\n",
      "  Somewhat relevant: 0\n",
      "  Total relevant kept: 5\n",
      "  Kept Doc 1: HIGHLY_RELEVANT - extracted from text...\n",
      "  Kept Doc 2: HIGHLY_RELEVANT - extracted from text...\n",
      "  Kept Doc 3: HIGHLY_RELEVANT - extracted from text...\n",
      "  Kept Doc 4: HIGHLY_RELEVANT - extracted from text...\n",
      "  Kept Doc 5: HIGHLY_RELEVANT - extracted from text...\n",
      "‚úì SUCCESS: Found 5 highly relevant + 5 total relevant\n",
      "---GENERATING ENHANCED CONTEXT---\n",
      "‚úì Enhanced context generated with clean formatting\n",
      "‚úì Enhanced context generated with 5 relevant documents\n",
      "‚úì Context generated successfully\n",
      "  - Length: 6,898 characters\n",
      "  - Sources: 5\n",
      "  - KB Used: Yes\n",
      "  - Type: Unknown\n",
      "  - Preview: === KNOWLEDGE BASE ===  ### Source 1: . GPUs consist of thousands of processing cores that can execute instructions in parallel, making them ideal for computationally-intensive tasks.', '', '## GPU Strengths', '', 'GPUs have several strengths:', '- Parallel processing: GPUs can process thousands of ...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "TEST COMPLETE - 10 questions tested\n"
     ]
    }
   ],
   "source": [
    "# Test cases for retrieve node\n",
    "test_questions = [\n",
    "    # CUDA Programming Questions\n",
    "    \"How to optimize CUDA memory usage?\",\n",
    "    \n",
    "    # CuPy Questions  \n",
    "    \"CuPy array operations best practices\",\n",
    "    \n",
    "    # PyTorch GPU Questions\n",
    "    \"PyTorch GPU training optimization techniques\", \n",
    "    \n",
    "    # RAPIDS Questions\n",
    "    \"RAPIDS cuDF dataframe operations tutorial\",\n",
    "    \n",
    "    # Numba CUDA Questions\n",
    "    \"Numba CUDA kernel programming examples\",\n",
    "    \n",
    "    # TensorFlow GPU Questions\n",
    "    \"TensorFlow GPU memory optimization strategies\",\n",
    "    \n",
    "    # JAX GPU Questions\n",
    "    \"JAX GPU acceleration tutorial examples\",\n",
    "    \n",
    "    # General GPU Computing\n",
    "    \"GPU memory hierarchy architecture explanation\",\n",
    "    \n",
    "    # Code Generation Tests\n",
    "    \"Implement GPU-accelerated matrix operations\",\n",
    "    \n",
    "    # Content Generation Tests  \n",
    "    \"Explain GPU compute capability differences\"\n",
    "]\n",
    "\n",
    "# Run tests\n",
    "print(\"TESTING RETRIEVE NODE\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for i, question in enumerate(test_questions, 1):\n",
    "    print(f\"\\n[TEST {i:2d}] {question}\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    try:\n",
    "        context = retrieve_node(question, vector_store,model,tokenizer)  # Use your vector_store here\n",
    "        \n",
    "        # Analyze results\n",
    "        context_length = len(context)\n",
    "        source_count = context.count(\"Source \")\n",
    "        has_kb = \"=== KNOWLEDGE BASE ===\" in context\n",
    "        is_code_request = \"code with: preamble\" in context\n",
    "        is_content_request = \"content with: background\" in context\n",
    "        \n",
    "        print(f\"‚úì Context generated successfully\")\n",
    "        print(f\"  - Length: {context_length:,} characters\")\n",
    "        print(f\"  - Sources: {source_count}\")\n",
    "        print(f\"  - KB Used: {'Yes' if has_kb else 'No'}\")\n",
    "        print(f\"  - Type: {'Code' if is_code_request else 'Content' if is_content_request else 'Unknown'}\")\n",
    "        \n",
    "        # Show preview\n",
    "        preview = context[:300].replace('\\n', ' ')\n",
    "        print(f\"  - Preview: {preview}...\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚úó ERROR: {e}\")\n",
    "    \n",
    "    print(\"-\" * 80)\n",
    "\n",
    "print(f\"\\nTEST COMPLETE - {len(test_questions)} questions tested\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "076626dd-a3f1-45b5-87d1-d05edd76868f",
   "metadata": {},
   "source": [
    "## Response Generation Pipeline\n",
    "\n",
    "This cell implements the core response generation function that combines retrieved context with the Llama3 model to produce comprehensive, source-based answers. The system includes enhanced prompting for accuracy and proper citation handling.\n",
    "\n",
    "**Key Features:**\n",
    "- Flexible context handling (retrieval or pre-provided)\n",
    "- Enhanced system prompt with strict source-only requirements\n",
    "- Chat history integration for follow-up questions\n",
    "- Structured response formatting with markdown headers\n",
    "- Code generation safety with hallucination prevention\n",
    "- Knowledge gap identification and research suggestions\n",
    "- Comprehensive error handling and response validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "40a2f70d-d4ad-4e17-b200-c9e85cc81e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(question, vector_store, model, tokenizer, max_tokens=2000, temperature=0.6,\n",
    "                     context=None, chat_context=\"\"):\n",
    "    \"\"\"\n",
    "    Complete GPU data science tutor pipeline\n",
    "    \n",
    "    Args:\n",
    "        question (str): User question\n",
    "        vector_store: Your vector store\n",
    "        model: Pre-loaded Llama3 model\n",
    "        tokenizer: Pre-loaded tokenizer\n",
    "        max_tokens (int): Maximum tokens to generate\n",
    "        temperature (float): Generation temperature\n",
    "        context (str): Pre-retrieved context (if None, will retrieve)\n",
    "        chat_context (str): Conversation history context\n",
    "        \n",
    "    Returns:\n",
    "        str: Final tutor response\n",
    "    \"\"\"\n",
    "    print(f\"GPU TUTOR: {question}\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # NEW: Step 1 - Use provided context or retrieve new context\n",
    "    if context is None:\n",
    "        print(\"Step 1: Retrieving and grading documents...\")\n",
    "        context = retrieve_node(question, vector_store, model, tokenizer)\n",
    "    else:\n",
    "        print(\"Step 1: Using provided context...\")\n",
    "    \n",
    "    # Step 2: Generate response\n",
    "    print(\"Step 2: Generating response...\")\n",
    "    \n",
    "#     # MODIFIED: Create system prompt for GPU data science tutor with chat context\n",
    "#     system_prompt = f\"\"\"You are an expert GPU data science tutor. Your job is to provide high-quality, accurate responses about GPU-accelerated computing, CUDA programming, and GPU data science frameworks.\n",
    "\n",
    "# {chat_context}\n",
    "\n",
    "# When generating code:\n",
    "# - Write complete, working code examples that can be run\n",
    "# - Include clear comments explaining each step and function\n",
    "# - Provide setup/import statements needed\n",
    "# - Add usage examples showing how to run the code\n",
    "# - Explain expected outputs or results\n",
    "# - Follow the specific tools/frameworks mentioned in the question\n",
    "# - Ensure code follows best practices for the requested technology\n",
    "# - If building on previous discussion, extend or modify previous examples appropriately\n",
    "\n",
    "# When generating content:\n",
    "# - Provide comprehensive background information\n",
    "# - Include practical examples and use cases\n",
    "# - Discuss future directions and trends\n",
    "# - Cite sources when possible\n",
    "\n",
    "# When generating explanations:\n",
    "# - Build explanations from the provided sources only\n",
    "# - Include practical examples and use cases\n",
    "# - Explain complex concepts step-by-step\n",
    "# - Connect to previous conversation context when relevant\n",
    "\n",
    "# If there's conversation history above, be aware of what was previously discussed and build upon it naturally. Reference previous topics when relevant, but focus primarily on answering the current question.\n",
    "\n",
    "# Use the provided context to inform your response, but also apply your knowledge to create complete, helpful answers.\"\"\"\n",
    "\n",
    "    # REPLACE the system_prompt in your generate_response function with this:\n",
    "\n",
    "    system_prompt = f\"\"\"You are an expert GPU data science tutor providing comprehensive, accurate responses about GPU-accelerated computing, CUDA programming, and GPU data science frameworks.\n",
    "\n",
    "{chat_context}\n",
    "\n",
    "CORE INSTRUCTIONS:\n",
    "- Use ONLY information from the provided context sources\n",
    "- Cite sources using [1], [2], [3] format\n",
    "- For follow-up questions, build directly on previous discussion\n",
    "- Provide maximum detail and explanation using available retrieved documents\n",
    "\n",
    "CODE GENERATION REQUIREMENTS:\n",
    "- ONLY use functions, classes, and methods demonstrated in the context sources\n",
    "- Include ONLY imports that are explicitly mentioned in context sources\n",
    "- If insufficient code examples in sources, explain what information is missing rather than hallucinating\n",
    "- State clearly when you lack enough information for complete working examples\n",
    "- Never invent function names, parameters, or library features not shown in sources\n",
    "\n",
    "CONTENT GENERATION APPROACH:\n",
    "- Analyze retrieved sources to identify available information and existing gaps\n",
    "- Explain concepts in maximum detail using only source information\n",
    "- Connect related concepts from different sources for comprehensive explanations\n",
    "- Build explanations progressively from basic to advanced topics\n",
    "- When encountering gaps, explicitly mention missing or incomplete topics\n",
    "- If sources contain conflicting information, acknowledge differences and cite specific sources\n",
    "\n",
    "RESPONSE STRUCTURE REQUIREMENTS:\n",
    "- Start with brief connection to previous discussion (if applicable)\n",
    "- Use clear markdown headers (##) to organize sections\n",
    "- Bold important terms and concepts (**term**) for emphasis\n",
    "- Structure as: Overview ‚Üí Key Concepts ‚Üí Implementation Details ‚Üí Code Examples (if sufficient sources)\n",
    "- Include \"Knowledge Gaps & Further Research\" section identifying:\n",
    "  * Topics mentioned but not fully covered in sources\n",
    "  * Related concepts for deeper understanding\n",
    "  * Specific areas needing additional research\n",
    "  * Suggested search terms for further exploration\n",
    "- End with practical usage guidance based on actual source content\n",
    "\n",
    "ACCURACY STANDARDS:\n",
    "- If sources lack specific details, state limitations explicitly\n",
    "- Acknowledge partial information when sources show incomplete examples\n",
    "- Identify knowledge gaps requiring additional research\n",
    "- Never guess syntax or availability when uncertain\"\"\"\n",
    "    # MODIFIED: Build the prompt with chat awareness\n",
    "    if chat_context:\n",
    "        user_prompt = f\"\"\"{chat_context}\n",
    "\n",
    "Context for current question:\n",
    "{context}\n",
    "\n",
    "Current Question: {question}\n",
    "\n",
    "Please provide a comprehensive response that takes into account our previous conversation while focusing on the current question.\"\"\"\n",
    "    else:\n",
    "        user_prompt = f\"\"\"Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Please provide a comprehensive response based on the context above.\"\"\"\n",
    "\n",
    "    # Build messages (YOUR EXISTING LOGIC)\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_prompt}\n",
    "    ]\n",
    "    \n",
    "    try:\n",
    "        # Generate response (YOUR EXISTING LOGIC)\n",
    "        input_ids = tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            add_generation_prompt=True,\n",
    "            return_tensors=\"pt\"\n",
    "        ).to(model.device)\n",
    "        \n",
    "        terminators = [\n",
    "            tokenizer.eos_token_id,\n",
    "            tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "        ]\n",
    "        \n",
    "        attention_mask = torch.ones_like(input_ids)\n",
    "        \n",
    "        outputs = model.generate(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            max_new_tokens=max_tokens,\n",
    "            eos_token_id=terminators,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            do_sample=True,\n",
    "            temperature=temperature,\n",
    "            top_p=0.9,\n",
    "            repetition_penalty=1.1\n",
    "        )\n",
    "        \n",
    "        # Extract response (YOUR EXISTING LOGIC)\n",
    "        response_tokens = outputs[0][input_ids.shape[-1]:]\n",
    "        response = tokenizer.decode(response_tokens, skip_special_tokens=True).strip()\n",
    "        \n",
    "        print(f\"‚úì Generated response ({len(response)} characters)\")\n",
    "        print(\"=\" * 80)\n",
    "        return response,context  # Return both response and context\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚úó Generation failed: {e}\")\n",
    "        return f\"Sorry, I encountered an error generating a response: {e}\", context\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b3c9891-1ada-49f5-aefb-5a1b478e48f8",
   "metadata": {},
   "source": [
    "## Test Cases for Generate Response Node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "48710765-56dc-4890-99a1-328776bc031c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU TUTOR: why does gpu accelerated libraries perform better than cpu libraries for data science?\n",
      "================================================================================\n",
      "Step 1: Retrieving and grading documents...\n",
      "ENHANCED RETRIEVE NODE: why does gpu accelerated libraries perform better than cpu libraries for data science?\n",
      "Target: At least 2 highly_relevant + drop not_relevant documents\n",
      "\n",
      "--- Retrieval Attempt 1 ---\n",
      "‚úì Retrieved 5 documents\n",
      "---DOCUMENT GRADING (5 documents)---\n",
      "  Grading response: {\"document_grades\": [{\"doc_index\": 1, \"relevance\": \"somewhat_relevant\", \"reason\": \"discusses GPU architecture, relevant to understanding GPU accelerat...\n",
      "  Document filtering:\n",
      "    ‚úì Keeping Doc 1: somewhat_relevant\n",
      "    ‚úì Keeping Doc 2: highly_relevant\n",
      "    ‚úì Keeping Doc 3: highly_relevant\n",
      "    ‚úì Keeping Doc 4: somewhat_relevant\n",
      "    ‚úì Keeping Doc 5: somewhat_relevant\n",
      "  Result: Kept 5 relevant, dropped 0 irrelevant\n",
      "  Immediately filtered: Kept 5, dropped 0\n",
      "Grading Analysis (after filtering):\n",
      "  Highly relevant: 2\n",
      "  Somewhat relevant: 3\n",
      "  Total relevant kept: 5\n",
      "  Kept Doc 1: SOMEWHAT_RELEVANT - discusses GPU architecture, relevant to understand...\n",
      "  Kept Doc 2: HIGHLY_RELEVANT - directly discusses GPU acceleration of scientific ...\n",
      "  Kept Doc 3: HIGHLY_RELEVANT - compares CPU and GPU performance and provides spec...\n",
      "  Kept Doc 4: SOMEWHAT_RELEVANT - discusses data parallelism, a key concept in GPU p...\n",
      "  Kept Doc 5: SOMEWHAT_RELEVANT - discusses GPU acceleration and provides examples, ...\n",
      "‚úì SUCCESS: Found 2 highly relevant + 5 total relevant\n",
      "---GENERATING ENHANCED CONTEXT---\n",
      "‚úì Enhanced context generated with clean formatting\n",
      "‚úì Enhanced context generated with 5 relevant documents\n",
      "Step 2: Generating response...\n",
      "‚úì Generated response (2957 characters)\n",
      "================================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Based on the provided sources, I'd like to address the question of what factors contribute to the performance difference between GPU-accelerated libraries and CPU libraries for data science tasks.\\n\\nThe primary factor contributing to the performance difference is the ability of GPUs to leverage massive parallelism, which is particularly beneficial for data-intensive tasks that involve repetitive operations on large datasets. As Source 1 explains, GPUs are optimized for data parallel throughput computations, allowing them to execute many instructions simultaneously. This property enables GPUs to accelerate tasks such as machine learning, scientific simulations, and data analysis.\\n\\nAnother crucial factor is the memory hierarchy of GPUs. As Source 4 notes, GPUs have limited memory, but they can process data much faster than CPUs. Additionally, GPUs can only access data stored in their own memory, which requires data transfer between the CPU and GPU. However, as Source 3 highlights, the cost of transferring data from CPU to GPU and the cuML Accelerator overheads doesn't significantly impact runtime for most tasks.\\n\\nThe type of task, data size, and hardware configuration also play important roles in determining the performance difference. For instance, as Source 3 illustrates, GPU acceleration provides greater benefits for computationally complex algorithms, whereas simpler algorithms may not show significant speedups. Similarly, dataset shape can influence the gains from GPU acceleration; skinny datasets may still benefit from GPU acceleration, but wide datasets can showcase the accelerator's strengths.\\n\\nHardware configuration, such as the number of CPU cores, GPU memory, and data transfer rates, also affects performance. For example, as Source 2 demonstrates, the H100 80GB GPU can significantly accelerate training times for certain algorithms, while the number of CPU cores can impact the speed of CPU-based computations.\\n\\nIn summary, the performance difference between GPU-accelerated libraries and CPU libraries for data science tasks primarily stems from the ability of GPUs to leverage massive parallelism, memory hierarchy, and efficient data transfer. These factors vary depending on the type of task, data size, and hardware configuration.\\n\\nHere's a concise illustration of the key points:\\n\\n* Massive parallelism: GPUs excel at executing many instructions simultaneously.\\n* Memory hierarchy: GPUs have limited memory but can process data faster than CPUs.\\n* Data transfer: Efficient data transfer between CPU and GPU minimizes overhead.\\n* Task complexity: Computationally complex algorithms benefit more from GPU acceleration.\\n* Dataset shape: Skinny datasets may still benefit from GPU acceleration, while wide datasets showcase the accelerator's strengths.\\n* Hardware configuration: Number of CPU cores, GPU memory, and data transfer rates impact performance.\\n\\nI hope this comprehensive response meets your requirements!\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"why does gpu accelerated libraries perform better than cpu libraries for data science?\"\n",
    "#context = retrieve_node(question, vector_store,model,tokenizer)\n",
    "response,context = generate_response(question, vector_store,model,tokenizer)\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "568c74a2-c6da-4c19-9917-6f3ead287c2c",
   "metadata": {},
   "source": [
    "## Hallucination Detection and Correction\n",
    "\n",
    "This cell implements a hallucination grader that validates whether generated responses are properly grounded in the retrieved context. If hallucinations are detected, the system automatically regenerates the response with stricter parameters.\n",
    "\n",
    "**Key Features:**\n",
    "- Binary grounding assessment (GROUNDED/HALLUCINATED)\n",
    "- Automatic response regeneration when hallucinations detected\n",
    "- Configurable retry attempts with fallback mechanisms\n",
    "- Temperature-controlled regeneration for improved accuracy\n",
    "- Comprehensive error handling for grading failures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "97ea2bd0-b76d-434c-b2f9-3b496faad867",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hallucination_grader(question, response, context, model, tokenizer, max_retries=2):\n",
    "    \"\"\"\n",
    "    Check if response is grounded in context, regenerate if hallucinated\n",
    "    \n",
    "    Args:\n",
    "        question (str): Original question\n",
    "        response (str): Generated response\n",
    "        context (str): Retrieved context\n",
    "        model: Llama3 model\n",
    "        tokenizer: Tokenizer\n",
    "        max_retries (int): Maximum regeneration attempts\n",
    "        \n",
    "    Returns:\n",
    "        str: Final response (original or regenerated)\n",
    "    \"\"\"\n",
    "    print(\"---HALLUCINATION GRADER---\")\n",
    "    \n",
    "    current_response = response\n",
    "    \n",
    "    for attempt in range(max_retries + 1):\n",
    "        print(f\"Checking attempt {attempt + 1}...\")\n",
    "        \n",
    "        # Check if response is grounded\n",
    "        check_prompt = f\"\"\"CONTEXT:\n",
    "{context}\n",
    "\n",
    "RESPONSE:\n",
    "{current_response}\n",
    "\n",
    "Is this response grounded in the context? Answer only: GROUNDED or HALLUCINATED\"\"\"\n",
    "\n",
    "        messages = [{\"role\": \"user\", \"content\": check_prompt}]\n",
    "        \n",
    "        try:\n",
    "            # Generate grading\n",
    "            input_ids = tokenizer.apply_chat_template(\n",
    "                messages, add_generation_prompt=True, return_tensors=\"pt\"\n",
    "            ).to(model.device)\n",
    "            \n",
    "            terminators = [tokenizer.eos_token_id, tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")]\n",
    "            attention_mask = torch.ones_like(input_ids)\n",
    "            \n",
    "            outputs = model.generate(\n",
    "                input_ids, attention_mask=attention_mask, max_new_tokens=10,\n",
    "                eos_token_id=terminators, do_sample=False\n",
    "            )\n",
    "            \n",
    "            response_tokens = outputs[0][input_ids.shape[-1]:]\n",
    "            grade = tokenizer.decode(response_tokens, skip_special_tokens=True).strip().upper()\n",
    "            \n",
    "            # Check result\n",
    "            if \"GROUNDED\" in grade:\n",
    "                print(f\"‚úì Response is grounded (attempt {attempt + 1})\")\n",
    "                return current_response\n",
    "            elif attempt < max_retries:\n",
    "                print(f\"‚úó Hallucination detected, regenerating...\")\n",
    "                # Regenerate response\n",
    "                current_response = generate_response(question, context, model, tokenizer, temperature = 0.6)\n",
    "            else:\n",
    "                print(f\"‚ö† Max retries reached, using last response\")\n",
    "                return current_response\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"‚úó Grading failed: {e}\")\n",
    "            return current_response\n",
    "    \n",
    "    return current_response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08115b07-3301-4727-ba2c-87209d81177d",
   "metadata": {},
   "source": [
    "## Test Case for Hallucination Grader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "00013a9e-3bac-4121-b64c-9a0db6bdc7d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---HALLUCINATION GRADER---\n",
      "Checking attempt 1...\n",
      "‚úì Response is grounded (attempt 1)\n"
     ]
    }
   ],
   "source": [
    "graded_response = hallucination_grader(question, response, context, model, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef6d1b90-9caa-47fb-a96c-c2abe30477ed",
   "metadata": {},
   "source": [
    "## Answer Quality Assessment and Refinement\n",
    "\n",
    "This cell implements a strict answer grader that evaluates whether the generated response adequately addresses the original question. The system uses a teacher-student grading approach with automatic regeneration for inadequate responses.\n",
    "\n",
    "**Key Components:**\n",
    "- Strict grading criteria for answer adequacy assessment\n",
    "- Binary evaluation (ANSWERS/DOESNT_ANSWER) with detailed rubric\n",
    "- Multi-layered validation combining hallucination and answer grading\n",
    "- Progressive temperature adjustment for response variety during retries\n",
    "- Comprehensive error handling with fallback assumptions\n",
    "- Iterative refinement process with configurable retry limits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2487682f-54d0-422d-8f10-e1b65aaa6f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_grader(question, response, context, model, tokenizer, max_retries=2):\n",
    "    \"\"\"\n",
    "    Check if response answers the question, regenerate if not\n",
    "    \n",
    "    Args:\n",
    "        question (str): Original question\n",
    "        response (str): Response after hallucination grading\n",
    "        context (str): Retrieved context (passed from previous step)\n",
    "        model: Llama3 model\n",
    "        tokenizer: Tokenizer\n",
    "        max_retries (int): Maximum regeneration attempts\n",
    "        \n",
    "    Returns:\n",
    "        str: Final response that answers the question\n",
    "    \"\"\"\n",
    "    print(\"---ANSWER GRADER---\")\n",
    "    \n",
    "    current_response = response\n",
    "    \n",
    "    for attempt in range(max_retries + 1):\n",
    "        print(f\"Checking if response answers question (attempt {attempt + 1})...\")\n",
    "        \n",
    "        # Check if response answers the question\n",
    "        answers_question = _check_if_answers_question(question, current_response, model, tokenizer)\n",
    "        \n",
    "        if answers_question:\n",
    "            print(f\"‚úì Response answers the question (attempt {attempt + 1})\")\n",
    "            return current_response\n",
    "        else:\n",
    "            print(f\"‚úó Response doesn't answer the question\")\n",
    "            if attempt < max_retries:\n",
    "                print(f\"Regenerating response...\")\n",
    "                \n",
    "                # Reuse generation logic with higher temperature for variety\n",
    "                temp_response = generate_response(question, context, model, tokenizer, temperature=0.7)\n",
    "                current_response = hallucination_grader(question, temp_response, context, model, tokenizer)\n",
    "            else:\n",
    "                print(f\"‚ö† Max retries reached, using last response\")\n",
    "                return current_response\n",
    "    \n",
    "    return current_response\n",
    "\n",
    "def _check_if_answers_question(question, response, model, tokenizer):\n",
    "    \"\"\"Check if response actually answers the question with strict grading\"\"\"\n",
    "    \n",
    "    system_prompt = \"\"\"You are a strict teacher grading student responses. Your job is to determine if the student's answer actually addresses what was asked.\n",
    "\n",
    "GRADING CRITERIA:\n",
    "- Does the response directly address the specific question asked?\n",
    "- Does it provide the information the user was seeking?\n",
    "- Is it relevant to the topic being asked about?\n",
    "- If the question asks \"how to\" do something, does the response explain how?\n",
    "- If the question asks for examples, are examples provided?\n",
    "- If the question asks for comparisons, are comparisons made?\n",
    "\n",
    "BE STRICT. If the response is vague, off-topic, or doesn't directly answer what was asked, grade it as inadequate.\n",
    "\n",
    "Respond with ONLY one word: ANSWERS or DOESNT_ANSWER\"\"\"\n",
    "\n",
    "    user_prompt = f\"\"\"Grade this student response:\n",
    "\n",
    "QUESTION: {question}\n",
    "\n",
    "STUDENT RESPONSE: {response}\n",
    "\n",
    "Does this response directly and adequately answer the specific question that was asked?\"\"\"\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_prompt}\n",
    "    ]\n",
    "    \n",
    "    try:\n",
    "        input_ids = tokenizer.apply_chat_template(\n",
    "            messages, add_generation_prompt=True, return_tensors=\"pt\"\n",
    "        ).to(model.device)\n",
    "        \n",
    "        terminators = [tokenizer.eos_token_id, tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")]\n",
    "        attention_mask = torch.ones_like(input_ids)\n",
    "        \n",
    "        outputs = model.generate(\n",
    "            input_ids, attention_mask=attention_mask, max_new_tokens=10,\n",
    "            eos_token_id=terminators, pad_token_id=tokenizer.eos_token_id,\n",
    "            do_sample=False\n",
    "        )\n",
    "        \n",
    "        response_tokens = outputs[0][input_ids.shape[-1]:]\n",
    "        grade = tokenizer.decode(response_tokens, skip_special_tokens=True).strip().upper()\n",
    "        \n",
    "        return \"ANSWERS\" in grade\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Answer check failed: {e}\")\n",
    "        return True  # Assume it answers if check fails"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e688de31-2733-4fe9-8002-dad5adf9b4d0",
   "metadata": {},
   "source": [
    "## Test Case for Answer Grader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "60162701-b19b-44cd-8d50-a993eed02cfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---ANSWER GRADER---\n",
      "Checking if response answers question (attempt 1)...\n",
      "‚úì Response answers the question (attempt 1)\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Based on the provided sources, I'd like to address the question of what factors contribute to the performance difference between GPU-accelerated libraries and CPU libraries for data science tasks.\n",
       "\n",
       "The primary factor contributing to the performance difference is the ability of GPUs to leverage massive parallelism, which is particularly beneficial for data-intensive tasks that involve repetitive operations on large datasets. As Source 1 explains, GPUs are optimized for data parallel throughput computations, allowing them to execute many instructions simultaneously. This property enables GPUs to accelerate tasks such as machine learning, scientific simulations, and data analysis.\n",
       "\n",
       "Another crucial factor is the memory hierarchy of GPUs. As Source 4 notes, GPUs have limited memory, but they can process data much faster than CPUs. Additionally, GPUs can only access data stored in their own memory, which requires data transfer between the CPU and GPU. However, as Source 3 highlights, the cost of transferring data from CPU to GPU and the cuML Accelerator overheads doesn't significantly impact runtime for most tasks.\n",
       "\n",
       "The type of task, data size, and hardware configuration also play important roles in determining the performance difference. For instance, as Source 3 illustrates, GPU acceleration provides greater benefits for computationally complex algorithms, whereas simpler algorithms may not show significant speedups. Similarly, dataset shape can influence the gains from GPU acceleration; skinny datasets may still benefit from GPU acceleration, but wide datasets can showcase the accelerator's strengths.\n",
       "\n",
       "Hardware configuration, such as the number of CPU cores, GPU memory, and data transfer rates, also affects performance. For example, as Source 2 demonstrates, the H100 80GB GPU can significantly accelerate training times for certain algorithms, while the number of CPU cores can impact the speed of CPU-based computations.\n",
       "\n",
       "In summary, the performance difference between GPU-accelerated libraries and CPU libraries for data science tasks primarily stems from the ability of GPUs to leverage massive parallelism, memory hierarchy, and efficient data transfer. These factors vary depending on the type of task, data size, and hardware configuration.\n",
       "\n",
       "Here's a concise illustration of the key points:\n",
       "\n",
       "* Massive parallelism: GPUs excel at executing many instructions simultaneously.\n",
       "* Memory hierarchy: GPUs have limited memory but can process data faster than CPUs.\n",
       "* Data transfer: Efficient data transfer between CPU and GPU minimizes overhead.\n",
       "* Task complexity: Computationally complex algorithms benefit more from GPU acceleration.\n",
       "* Dataset shape: Skinny datasets may still benefit from GPU acceleration, while wide datasets showcase the accelerator's strengths.\n",
       "* Hardware configuration: Number of CPU cores, GPU memory, and data transfer rates impact performance.\n",
       "\n",
       "I hope this comprehensive response meets your requirements!"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Markdown\n",
    "final_response = answer_grader(question, graded_response, context, model, tokenizer)\n",
    "Markdown(final_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b68558d-26a9-450d-a161-d521e0bd177b",
   "metadata": {},
   "source": [
    "## Complete Self-Corrective RAG System with Citations\n",
    "\n",
    "This cell implements the complete self-corrective RAG workflow using LangGraph. The system combines document retrieval, response generation, hallucination detection, answer quality assessment, and proper citation management in an orchestrated pipeline.\n",
    "\n",
    "**Core Components:**\n",
    "- **GraphState**: Comprehensive state management with chat history and document tracking\n",
    "- **Enhanced Retrieval**: Multi-source document retrieval with citation metadata\n",
    "- **Response Generation**: Context-aware generation with chat history integration\n",
    "- **Hallucination Detection**: Citation validation and content grounding assessment\n",
    "- **Answer Quality**: Strict grading for response adequacy\n",
    "- **Citation Management**: Automatic reference formatting for mixed source types\n",
    "- **Workflow Orchestration**: LangGraph-based pipeline with conditional routing and retry logic\n",
    "\n",
    "**Key Features:**\n",
    "- Document exclusion tracking to prevent repetitive retrievals\n",
    "- Progressive search expansion during regeneration attempts\n",
    "- Chat history management for follow-up question handling\n",
    "- Multi-source citation support (URLs, notebooks, Python files)\n",
    "- Comprehensive error handling and fallback mechanisms\n",
    "- Detailed workflow progress reporting and statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "760a05f1-0034-4c25-8a57-da5878a25c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, END\n",
    "from typing import TypedDict, List, Dict\n",
    "import torch\n",
    "import re\n",
    "\n",
    "# Enhanced graph state with chat history and document tracking\n",
    "class GraphState(TypedDict):\n",
    "    question: str\n",
    "    vector_store: object\n",
    "    model: object\n",
    "    tokenizer: object\n",
    "    context: str\n",
    "    response: str\n",
    "    is_grounded: bool\n",
    "    answers_question: bool\n",
    "    generation_count: int\n",
    "    max_retries: int\n",
    "    chat_history: List[Dict[str, str]]\n",
    "    retrieved_doc_ids: List[str]\n",
    "    source_metadata: List[Dict[str, str]]  # Track source metadata for citations\n",
    "\n",
    "# Utility function to get document ID\n",
    "def get_doc_id(doc):\n",
    "    \"\"\"Generate unique ID for a document\"\"\"\n",
    "    source = doc.metadata.get('source', 'unknown')\n",
    "    content_snippet = doc.page_content[:100].replace('\\n', ' ').strip()\n",
    "    return f\"{source}_{hash(content_snippet)}\"\n",
    "\n",
    "# Global variables for citation tracking\n",
    "_current_doc_ids = []\n",
    "_current_source_metadata = []\n",
    "\n",
    "# REPLACE YOUR EXISTING extract_source_metadata FUNCTION WITH THIS:\n",
    "\n",
    "def extract_source_metadata(documents):\n",
    "    \"\"\"\n",
    "    Extract citation-relevant metadata from mixed document sources\n",
    "    \"\"\"\n",
    "    sources = []\n",
    "    for i, doc in enumerate(documents, 1):\n",
    "        metadata = doc.metadata\n",
    "        source_type = metadata.get('source_type', 'unknown')\n",
    "        \n",
    "        if source_type == 'notebook':\n",
    "            # For notebooks, use folder/filename format\n",
    "            source_info = {\n",
    "                \"number\": i,\n",
    "                \"source\": metadata.get('source', 'Unknown notebook'),\n",
    "                \"title\": f\"Notebook: {metadata.get('filename', 'Unknown')}\",\n",
    "                \"source_type\": \"notebook\"\n",
    "            }\n",
    "        elif source_type == 'python_file':\n",
    "            # For Python files, use folder/filename format\n",
    "            source_info = {\n",
    "                \"number\": i,\n",
    "                \"source\": metadata.get('source', 'Unknown python file'),\n",
    "                \"title\": f\"Python File: {metadata.get('filename', 'Unknown')}\",\n",
    "                \"source_type\": \"python_file\"\n",
    "            }\n",
    "        else:\n",
    "            # For URLs, use existing format\n",
    "            source_info = {\n",
    "                \"number\": i,\n",
    "                \"source\": metadata.get('source', 'Unknown source'),\n",
    "                \"title\": metadata.get('title', 'Untitled'),\n",
    "                \"source_type\": \"url\"\n",
    "            }\n",
    "        \n",
    "        sources.append(source_info)\n",
    "    return sources\n",
    "\n",
    "def create_context_with_source_metadata(question: str, relevant_docs: List, model, tokenizer) -> tuple:\n",
    "    \"\"\"\n",
    "    Create context with source metadata for mixed sources (URLs + Notebooks + Python files)\n",
    "    \"\"\"\n",
    "    print(\"---GENERATING CONTEXT WITH SOURCE METADATA---\")\n",
    "    \n",
    "    if not relevant_docs:\n",
    "        return f\"No relevant documents found for: {question}\", []\n",
    "    \n",
    "    # Extract source metadata for references\n",
    "    source_metadata = extract_source_metadata(relevant_docs)\n",
    "    \n",
    "    # Create knowledge base with FULL source information\n",
    "    knowledge_base = \"=== KNOWLEDGE BASE ===\\n\\n\"\n",
    "    \n",
    "    for i, doc in enumerate(relevant_docs, 1):\n",
    "        content = doc.page_content.strip()\n",
    "        content = ' '.join(content.split())  # Clean whitespace\n",
    "        \n",
    "        # Get source info based on type\n",
    "        metadata = doc.metadata\n",
    "        source_type = metadata.get('source_type', 'unknown')\n",
    "        \n",
    "        if source_type == 'notebook':\n",
    "            source_display = f\"Notebook: {metadata.get('source', 'Unknown')}\"\n",
    "            source_title = f\"Notebook: {metadata.get('filename', 'Unknown')}\"\n",
    "        elif source_type == 'python_file':\n",
    "            source_display = f\"Python File: {metadata.get('source', 'Unknown')}\"\n",
    "            source_title = f\"Python File: {metadata.get('filename', 'Unknown')}\"\n",
    "        else:\n",
    "            source_display = metadata.get('source', 'Unknown source')\n",
    "            source_title = metadata.get('title', 'Untitled')\n",
    "        \n",
    "        knowledge_base += f\"### Source {i}: {source_title}\\n\"\n",
    "        knowledge_base += f\"Source: {source_display}\\n\"\n",
    "        knowledge_base += f\"Content: {content}\\n\\n\"\n",
    "    \n",
    "    # Generate refined query\n",
    "    refined_query = generate_refined_query(question, model, tokenizer)\n",
    "    \n",
    "    # Create source list for instructions\n",
    "    source_list = \"\"\n",
    "    for source in source_metadata:\n",
    "        if source['source_type'] == 'notebook':\n",
    "            source_list += f\"  [{source['number']}] {source['source']} (Notebook)\\n\"\n",
    "        elif source['source_type'] == 'python_file':\n",
    "            source_list += f\"  [{source['number']}] {source['source']} (Python File)\\n\"\n",
    "        else:\n",
    "            source_list += f\"  [{source['number']}] {source['title']} - {source['source']}\\n\"\n",
    "    \n",
    "    # Create citation instructions with source information\n",
    "    citation_instructions = f\"\"\"Using the provided sources, generate a comprehensive response that:\n",
    "\n",
    "1. **Directly addresses the question**\n",
    "2. **Uses source material effectively with natural citations**\n",
    "3. **Maintains technical accuracy**\n",
    "\n",
    "CITATION APPROACH:\n",
    "- When referencing information from sources, use numbered citations [1], [2], [3]\n",
    "- You have access to these specific sources:\n",
    "\n",
    "{source_list}\n",
    "\n",
    "IMPORTANT INSTRUCTIONS:\n",
    "- Use [1], [2], [3] etc. when citing information from the sources above\n",
    "- Only reference the {len(relevant_docs)} sources provided ([1] through [{len(relevant_docs)}])\n",
    "- For notebook sources, citations will show as folder/filename in references\n",
    "- For Python file sources, citations will show as folder/filename in references\n",
    "- For URL sources, citations will show full URLs in references\n",
    "- Do NOT create fake URLs or additional sources\n",
    "- Focus on the content and cite naturally where information comes from these sources\n",
    "\n",
    "Generate your response using the source content and cite appropriately with [1], [2], etc.\"\"\"\n",
    "\n",
    "    # Assemble enhanced context\n",
    "    enhanced_context = f\"\"\"{knowledge_base}=== ENHANCED QUERY ===\n",
    "{refined_query}\n",
    "\n",
    "=== GENERATION INSTRUCTIONS ===\n",
    "{citation_instructions}\"\"\"\n",
    "    \n",
    "    print(f\"‚úì Enhanced context with source metadata generated for {len(relevant_docs)} sources\")\n",
    "    return enhanced_context, source_metadata\n",
    "\n",
    "def format_final_response_with_references(response: str, source_metadata: List[Dict]) -> str:\n",
    "    \"\"\"\n",
    "    Format the final response with proper references for mixed sources\n",
    "    \"\"\"\n",
    "    if not source_metadata:\n",
    "        return response\n",
    "    \n",
    "    # Check if LLM already created a references section\n",
    "    has_references = any(keyword in response.lower() for keyword in ['references:', 'bibliography:', 'sources:'])\n",
    "    \n",
    "    if has_references:\n",
    "        # LLM created references - validate them against our metadata\n",
    "        validated_response = validate_and_fix_references(response, source_metadata)\n",
    "        return validated_response\n",
    "    else:\n",
    "        # LLM didn't create references - add our clean section\n",
    "        references_section = \"\\n\\n**REFERENCES:**\\n\"\n",
    "        for source in source_metadata:\n",
    "            if source['source_type'] == 'notebook':\n",
    "                references_section += f\"[{source['number']}] {source['source']} (Notebook)\\n\"\n",
    "            elif source['source_type'] == 'python_file':\n",
    "                references_section += f\"[{source['number']}] {source['source']} (Python File)\\n\"\n",
    "            else:\n",
    "                references_section += f\"[{source['number']}] {source['source']}\\n\"\n",
    "        return response + references_section\n",
    "\n",
    "def validate_and_fix_references(response: str, source_metadata: List[Dict]) -> str:\n",
    "    \"\"\"\n",
    "    Validate LLM-created references and fix any issues for mixed sources\n",
    "    \"\"\"\n",
    "    import re\n",
    "    \n",
    "    # Find reference sections\n",
    "    ref_pattern = r'(References?:|Bibliography:|Sources?:)(.*?)(?=\\n\\n[A-Z]|\\Z)'\n",
    "    \n",
    "    def fix_reference_section(match):\n",
    "        section_header = match.group(1)\n",
    "        \n",
    "        # Rebuild reference section with valid sources only\n",
    "        fixed_refs = f\"\\n{section_header}\\n\"\n",
    "        \n",
    "        for source in source_metadata:\n",
    "            if source['source_type'] == 'notebook':\n",
    "                fixed_refs += f\"[{source['number']}] {source['source']} (Notebook)\\n\"\n",
    "            elif source['source_type'] == 'python_file':\n",
    "                fixed_refs += f\"[{source['number']}] {source['source']} (Python File)\\n\"\n",
    "            else:\n",
    "                fixed_refs += f\"[{source['number']}] {source['source']}\\n\"\n",
    "        \n",
    "        return fixed_refs.rstrip()\n",
    "    \n",
    "    # Replace reference sections with validated ones\n",
    "    fixed_response = re.sub(ref_pattern, fix_reference_section, response, flags=re.DOTALL | re.IGNORECASE)\n",
    "    \n",
    "    return fixed_response\n",
    "\n",
    "# REPLACE YOUR EXISTING format_final_response_with_references FUNCTION WITH THIS:\n",
    "\n",
    "def format_final_response_with_references(response: str, source_metadata: List[Dict]) -> str:\n",
    "    \"\"\"\n",
    "    Format the final response with proper references for mixed sources (URLs + Notebooks)\n",
    "    \"\"\"\n",
    "    if not source_metadata:\n",
    "        return response\n",
    "    \n",
    "    # Check if LLM already created a references section\n",
    "    has_references = any(keyword in response.lower() for keyword in ['references:', 'bibliography:', 'sources:'])\n",
    "    \n",
    "    if has_references:\n",
    "        # LLM created references - validate them against our metadata\n",
    "        validated_response = validate_and_fix_references(response, source_metadata)\n",
    "        return validated_response\n",
    "    else:\n",
    "        # LLM didn't create references - add our clean section\n",
    "        references_section = \"\\n\\n**REFERENCES:**\\n\"\n",
    "        for source in source_metadata:\n",
    "            if source['source_type'] == 'notebook':\n",
    "                references_section += f\"[{source['number']}] {source['source']} (Notebook)\\n\"\n",
    "            else:\n",
    "                references_section += f\"[{source['number']}] {source['source']}\\n\"\n",
    "        return response + references_section\n",
    "\n",
    "def validate_and_fix_references(response: str, source_metadata: List[Dict]) -> str:\n",
    "    \"\"\"\n",
    "    Validate LLM-created references and fix any issues for mixed sources\n",
    "    \"\"\"\n",
    "    import re\n",
    "    \n",
    "    # Find reference sections\n",
    "    ref_pattern = r'(References?:|Bibliography:|Sources?:)(.*?)(?=\\n\\n[A-Z]|\\Z)'\n",
    "    \n",
    "    def fix_reference_section(match):\n",
    "        section_header = match.group(1)\n",
    "        \n",
    "        # Rebuild reference section with valid sources only\n",
    "        fixed_refs = f\"\\n{section_header}\\n\"\n",
    "        \n",
    "        for source in source_metadata:\n",
    "            if source['source_type'] == 'notebook':\n",
    "                fixed_refs += f\"[{source['number']}] {source['source']} (Notebook)\\n\"\n",
    "            else:\n",
    "                fixed_refs += f\"[{source['number']}] {source['source']}\\n\"\n",
    "        \n",
    "        return fixed_refs.rstrip()\n",
    "    \n",
    "    # Replace reference sections with validated ones\n",
    "    fixed_response = re.sub(ref_pattern, fix_reference_section, response, flags=re.DOTALL | re.IGNORECASE)\n",
    "    \n",
    "    return fixed_response\n",
    "\n",
    "def validate_numbered_citations(response: str, num_sources: int) -> Dict:\n",
    "    \"\"\"Validate that all numbered citations [1], [2], etc. are valid\"\"\"\n",
    "    \n",
    "    # Find all citation patterns [1], [2], [1,2], [1, 2], etc.\n",
    "    citation_pattern = r'\\[([0-9,\\s]+)\\]'\n",
    "    citations = re.findall(citation_pattern, response)\n",
    "    \n",
    "    valid_numbers = set(range(1, num_sources + 1))\n",
    "    invalid_citations = []\n",
    "    valid_citations = []\n",
    "    \n",
    "    for citation in citations:\n",
    "        # Parse citation numbers\n",
    "        try:\n",
    "            cited_numbers = [int(x.strip()) for x in citation.split(',') if x.strip()]\n",
    "            for num in cited_numbers:\n",
    "                if num in valid_numbers:\n",
    "                    valid_citations.append(num)\n",
    "                else:\n",
    "                    invalid_citations.append(num)\n",
    "        except ValueError:\n",
    "            invalid_citations.append(citation)\n",
    "    \n",
    "    return {\n",
    "        \"has_invalid_citations\": len(invalid_citations) > 0,\n",
    "        \"invalid_citations\": invalid_citations,\n",
    "        \"valid_citations\": list(set(valid_citations)),\n",
    "        \"total_sources_available\": num_sources\n",
    "    }\n",
    "\n",
    "def generate_refined_query(question: str, model, tokenizer) -> str:\n",
    "    \"\"\"\n",
    "    Generate refined query without regex issues\n",
    "    \"\"\"\n",
    "    try:\n",
    "        prompt = f\"\"\"Refine this technical question to be more specific and comprehensive:\n",
    "\n",
    "Original: {question}\n",
    "\n",
    "Create a refined version that asks for more detailed information. Respond with ONLY the refined question.\"\"\"\n",
    "\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You refine technical questions to be more comprehensive.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "        \n",
    "        input_ids = tokenizer.apply_chat_template(\n",
    "            messages, add_generation_prompt=True, return_tensors=\"pt\"\n",
    "        ).to(model.device)\n",
    "        \n",
    "        attention_mask = torch.ones_like(input_ids)\n",
    "        \n",
    "        outputs = model.generate(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            max_new_tokens=150,\n",
    "            eos_token_id=[tokenizer.eos_token_id, tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")],\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            do_sample=True,\n",
    "            temperature=0.3\n",
    "        )\n",
    "        \n",
    "        response_tokens = outputs[0][len(input_ids[0]):]\n",
    "        refined = tokenizer.decode(response_tokens, skip_special_tokens=True).strip()\n",
    "        \n",
    "        # Clean without regex\n",
    "        refined = refined.strip(' \\'\"')\n",
    "        return refined if refined else question\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Query refinement failed: {e}\")\n",
    "        return question\n",
    "# Modified retrieve_node that uses source metadata\n",
    "def retrieve_node_with_citations(question: str, vector_store, model, tokenizer, max_attempts: int = 3,\n",
    "                                k=5, exclude_ids=None, attempt_number=0, return_doc_ids=False):\n",
    "    \"\"\"Enhanced retrieve node with citation tracking\"\"\"\n",
    "    global _current_doc_ids, _current_source_metadata\n",
    "    \n",
    "    print(f\"ENHANCED RETRIEVE NODE: {question}\")\n",
    "    print(\"Target: At least 3 relevant + drop not_relevant documents\")\n",
    "    \n",
    "    attempt = 0\n",
    "    \n",
    "    while attempt < max_attempts:\n",
    "        attempt += 1\n",
    "        print(f\"\\n--- Retrieval Attempt {attempt} ---\")\n",
    "        \n",
    "        # Step 1: Retrieve documents with exclusion logic\n",
    "        try:\n",
    "            if exclude_ids:\n",
    "                print(f\"Excluding {len(exclude_ids)} previously retrieved documents\")\n",
    "                all_documents = vector_store.similarity_search(question, k=k*3)\n",
    "                filtered_docs = []\n",
    "                excluded_count = 0\n",
    "                \n",
    "                for doc in all_documents:\n",
    "                    doc_id = get_doc_id(doc)\n",
    "                    if doc_id not in exclude_ids:\n",
    "                        filtered_docs.append(doc)\n",
    "                        if len(filtered_docs) >= k:\n",
    "                            break\n",
    "                    else:\n",
    "                        excluded_count += 1\n",
    "                \n",
    "                documents = filtered_docs\n",
    "                print(f\"Excluded {excluded_count} documents, using {len(documents)} new documents\")\n",
    "            else:\n",
    "                documents = vector_store.similarity_search(question, k=k)\n",
    "                \n",
    "            print(f\"‚úì Retrieved {len(documents)} documents\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚úó Retrieval failed: {e}\")\n",
    "            if attempt >= max_attempts:\n",
    "                return f\"Unable to retrieve documents for: {question}\", []\n",
    "            continue\n",
    "        \n",
    "        if len(documents) < 3:\n",
    "            print(f\"‚ö†Ô∏è  Only {len(documents)} documents found\")\n",
    "            if attempt >= max_attempts:\n",
    "                if documents:\n",
    "                    break\n",
    "                else:\n",
    "                    return f\"No documents found for: {question}\", []\n",
    "        \n",
    "        # Step 2: Grade documents (using your existing function)\n",
    "        grades = grade_documents_strict(question, documents, model, tokenizer)\n",
    "        \n",
    "        # Step 3: Filter out irrelevant documents (using your existing function)\n",
    "        relevant_docs = filter_and_show_documents(documents, grades)\n",
    "        dropped_count = len(documents) - len(relevant_docs)\n",
    "        print(f\"  Immediately filtered: Kept {len(relevant_docs)}, dropped {dropped_count}\")\n",
    "        \n",
    "        # Step 4: Analyze the FILTERED results\n",
    "        highly_relevant_count = count_by_relevance(grades, \"highly_relevant\")\n",
    "        somewhat_relevant_count = count_by_relevance(grades, \"somewhat_relevant\")\n",
    "        \n",
    "        print(f\"Grading Analysis (after filtering):\")\n",
    "        print(f\"  Highly relevant: {highly_relevant_count}\")\n",
    "        print(f\"  Somewhat relevant: {somewhat_relevant_count}\") \n",
    "        print(f\"  Total relevant kept: {len(relevant_docs)}\")\n",
    "        \n",
    "        # Step 5: Check quality requirements\n",
    "        if highly_relevant_count >= 1 and len(relevant_docs) >= 2:\n",
    "            print(f\"‚úì SUCCESS: Found {highly_relevant_count} highly relevant + {len(relevant_docs)} total relevant\")\n",
    "            \n",
    "            # Generate enhanced context with source metadata\n",
    "            enhanced_context, source_metadata = create_context_with_source_metadata(question, relevant_docs, model, tokenizer)\n",
    "            \n",
    "            # Track the document IDs and source metadata\n",
    "            used_doc_ids = [get_doc_id(doc) for doc in relevant_docs]\n",
    "            _current_doc_ids = used_doc_ids\n",
    "            _current_source_metadata = source_metadata\n",
    "            \n",
    "            print(f\"‚úì Enhanced context with source metadata generated for {len(relevant_docs)} sources\")\n",
    "            \n",
    "            if return_doc_ids:\n",
    "                return enhanced_context, used_doc_ids, source_metadata\n",
    "            else:\n",
    "                return enhanced_context, source_metadata\n",
    "        \n",
    "        elif len(relevant_docs) >= 2:\n",
    "            print(f\"‚úì ACCEPTABLE: Found {len(relevant_docs)} relevant documents\")\n",
    "            \n",
    "            enhanced_context, source_metadata = create_context_with_source_metadata(question, relevant_docs, model, tokenizer)\n",
    "            \n",
    "            used_doc_ids = [get_doc_id(doc) for doc in relevant_docs]\n",
    "            _current_doc_ids = used_doc_ids\n",
    "            _current_source_metadata = source_metadata\n",
    "            \n",
    "            print(f\"‚úì Enhanced context with source metadata generated for {len(relevant_docs)} sources\")\n",
    "            \n",
    "            if return_doc_ids:\n",
    "                return enhanced_context, used_doc_ids, source_metadata\n",
    "            else:\n",
    "                return enhanced_context, source_metadata\n",
    "        \n",
    "        else:\n",
    "            print(f\"‚úó INSUFFICIENT: Only {len(relevant_docs)} relevant documents after filtering\")\n",
    "            \n",
    "            if attempt >= max_attempts:\n",
    "                print(\"Max attempts reached\")\n",
    "                if relevant_docs:\n",
    "                    enhanced_context, source_metadata = create_context_with_source_metadata(question, relevant_docs, model, tokenizer)\n",
    "                    \n",
    "                    used_doc_ids = [get_doc_id(doc) for doc in relevant_docs]\n",
    "                    _current_doc_ids = used_doc_ids\n",
    "                    _current_source_metadata = source_metadata\n",
    "                    \n",
    "                    print(f\"‚ö†Ô∏è  Using {len(relevant_docs)} available relevant documents\")\n",
    "                    \n",
    "                    if return_doc_ids:\n",
    "                        return enhanced_context, used_doc_ids, source_metadata\n",
    "                    else:\n",
    "                        return enhanced_context, source_metadata\n",
    "                else:\n",
    "                    if return_doc_ids:\n",
    "                        return f\"No sufficiently relevant documents found for: {question}\", [], []\n",
    "                    else:\n",
    "                        return f\"No sufficiently relevant documents found for: {question}\", []\n",
    "            else:\n",
    "                print(f\"Will search for more documents in next attempt...\")\n",
    "                k = min(k + 3, 15)\n",
    "    \n",
    "    if return_doc_ids:\n",
    "        return f\"Failed to find adequate documents for: {question}\", [], []\n",
    "    else:\n",
    "        return f\"Failed to find adequate documents for: {question}\", []\n",
    "\n",
    "# Modified retrieve wrapper for citations\n",
    "def retrieve_node_wrapper_with_citations(state):\n",
    "    \"\"\"Wrapper with citation tracking\"\"\"\n",
    "    print(\"---RETRIEVE NODE---\")\n",
    "    \n",
    "    generation_count = state.get(\"generation_count\", 0)\n",
    "    retrieved_doc_ids = state.get(\"retrieved_doc_ids\", [])\n",
    "    \n",
    "    # Determine k based on attempt number\n",
    "    if generation_count == 0:\n",
    "        k = 5\n",
    "        print(\"üÜï First retrieval attempt\")\n",
    "    elif generation_count == 1:\n",
    "        k = 8\n",
    "        print(f\"üîÑ Regeneration attempt #{generation_count} - expanding search to k={k}\")\n",
    "    elif generation_count == 2:\n",
    "        k = 12\n",
    "        print(f\"üîÑ Regeneration attempt #{generation_count} - expanding search to k={k}\")\n",
    "    else:\n",
    "        k = 15\n",
    "        print(f\"üîÑ Regeneration attempt #{generation_count} - maximum search k={k}\")\n",
    "    \n",
    "    # Call enhanced retrieve_node with citations\n",
    "    context, source_metadata = retrieve_node_with_citations(\n",
    "        question=state[\"question\"],\n",
    "        vector_store=state[\"vector_store\"],\n",
    "        model=state[\"model\"],\n",
    "        tokenizer=state[\"tokenizer\"],\n",
    "        max_attempts=3,\n",
    "        k=k,\n",
    "        exclude_ids=retrieved_doc_ids,\n",
    "        attempt_number=generation_count\n",
    "    )\n",
    "    \n",
    "    # Update retrieved document IDs and source metadata\n",
    "    global _current_doc_ids\n",
    "    updated_doc_ids = retrieved_doc_ids + _current_doc_ids\n",
    "    \n",
    "    return {\n",
    "        \"context\": context,\n",
    "        \"retrieved_doc_ids\": updated_doc_ids,\n",
    "        \"source_metadata\": source_metadata\n",
    "    }\n",
    "\n",
    "def build_enhanced_chat_context(state):\n",
    "    \"\"\"Build enhanced chat context that helps with follow-ups\"\"\"\n",
    "    chat_history = state.get(\"chat_history\", [])\n",
    "    current_question = state[\"question\"]\n",
    "    \n",
    "    if not chat_history:\n",
    "        return \"\"\n",
    "    \n",
    "    # Check if current question is a follow-up\n",
    "    followup_indicators = [\n",
    "        \"example\", \"code\", \"show me\", \"demonstrate\", \"implement\", \"using\",\n",
    "        \"with\", \"how to\", \"can you\", \"provide\", \"give me\", \"write\",\n",
    "        \"this\", \"that\", \"it\", \"above\", \"previous\", \"mentioned\"\n",
    "    ]\n",
    "    \n",
    "    is_followup = any(indicator in current_question.lower() for indicator in followup_indicators)\n",
    "    \n",
    "    if is_followup and chat_history:\n",
    "        # For follow-ups, provide rich context\n",
    "        last_exchange = chat_history[-1]\n",
    "        \n",
    "        chat_context = f\"\"\"\n",
    "=== CONVERSATION CONTEXT ===\n",
    "This is a follow-up question building on our previous discussion.\n",
    "\n",
    "PREVIOUS TOPIC: {last_exchange['question']}\n",
    "\n",
    "PREVIOUS DISCUSSION SUMMARY: \n",
    "{last_exchange['response'][:800]}...\n",
    "\n",
    "CURRENT FOLLOW-UP: {current_question}\n",
    "\n",
    "INSTRUCTION: The current question \"{current_question}\" is asking you to build upon, extend, or provide examples related to the previous topic \"{last_exchange['question']}\". Use the retrieved sources to provide specific examples, code, or implementations that relate to what we just discussed.\n",
    "\"\"\"\n",
    "    else:\n",
    "        # For new topics, provide minimal context\n",
    "        chat_context = f\"\"\"\n",
    "=== CONVERSATION CONTEXT ===\n",
    "Previous conversation exists but this appears to be a new topic.\n",
    "\n",
    "Recent discussion: {chat_history[-1]['question'] if chat_history else 'None'}\n",
    "\n",
    "CURRENT QUESTION: {current_question}\n",
    "\"\"\"\n",
    "    \n",
    "    return chat_context\n",
    "\n",
    "def generate_node_wrapper_with_citations(state):\n",
    "    \"\"\"Wrapper for your existing generate_response function\"\"\"\n",
    "    print(\"---GENERATE NODE---\")\n",
    "    \n",
    "    # Build ENHANCED chat history context\n",
    "    chat_context = build_enhanced_chat_context(state)\n",
    "    \n",
    "    if chat_context:\n",
    "        print(f\"Using enhanced chat context\")\n",
    "    \n",
    "    # Call YOUR existing generate_response function\n",
    "    try:\n",
    "        response, _ = generate_response(\n",
    "            question=state[\"question\"],\n",
    "            vector_store=state[\"vector_store\"],\n",
    "            model=state[\"model\"],\n",
    "            tokenizer=state[\"tokenizer\"],\n",
    "            max_tokens=2000,\n",
    "            temperature=0.5,\n",
    "            context=state[\"context\"],\n",
    "            chat_context=chat_context\n",
    "        )\n",
    "        \n",
    "        generation_count = state.get(\"generation_count\", 0) + 1\n",
    "        print(f\"Generated {len(response)} characters\")\n",
    "        \n",
    "        return {\n",
    "            \"response\": response,\n",
    "            \"generation_count\": generation_count\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Generation failed: {e}\")\n",
    "        return {\n",
    "            \"response\": f\"Generation error: {e}\",\n",
    "            \"generation_count\": state.get(\"generation_count\", 0) + 1\n",
    "        }\n",
    "\n",
    "def hallucination_grade_wrapper_with_citations(state):\n",
    "    \"\"\"Enhanced hallucination grader with citation validation\"\"\"\n",
    "    print(\"---HALLUCINATION GRADER WITH CITATION VALIDATION---\")\n",
    "    \n",
    "    context = state[\"context\"]\n",
    "    response = state[\"response\"]\n",
    "    source_metadata = state.get(\"source_metadata\", [])\n",
    "    \n",
    "    # Step 1: Validate numbered citations\n",
    "    citation_check = validate_numbered_citations(response, len(source_metadata))\n",
    "    \n",
    "    if citation_check[\"has_invalid_citations\"]:\n",
    "        print(f\"‚úó Invalid citations found: {citation_check['invalid_citations']}\")\n",
    "        print(f\"  Available citation numbers: 1-{citation_check['total_sources_available']}\")\n",
    "        return {\"is_grounded\": False}\n",
    "    else:\n",
    "        print(f\"‚úì All citations valid: {citation_check['valid_citations']}\")\n",
    "    \n",
    "    # Step 2: Regular hallucination check\n",
    "    if \"=== KNOWLEDGE BASE ===\" in context:\n",
    "        kb_start = context.find(\"=== KNOWLEDGE BASE ===\") + len(\"=== KNOWLEDGE BASE ===\")\n",
    "        kb_end = context.find(\"=== ENHANCED QUERY ===\")\n",
    "        if kb_end == -1:\n",
    "            knowledge_base = context[kb_start:].strip()\n",
    "        else:\n",
    "            knowledge_base = context[kb_start:kb_end].strip()\n",
    "    else:\n",
    "        knowledge_base = context\n",
    "    \n",
    "    check_prompt = f\"\"\"KNOWLEDGE BASE:\n",
    "{knowledge_base}\n",
    "\n",
    "RESPONSE:\n",
    "{response}\n",
    "\n",
    "Is this response grounded in the knowledge base? Does it only use information from the provided sources? Are all numbered citations [1], [2], etc. valid?\n",
    "\n",
    "Answer only: GROUNDED or HALLUCINATED\"\"\"\n",
    "\n",
    "    messages = [{\"role\": \"user\", \"content\": check_prompt}]\n",
    "    \n",
    "    try:\n",
    "        input_ids = state[\"tokenizer\"].apply_chat_template(\n",
    "            messages, add_generation_prompt=True, return_tensors=\"pt\"\n",
    "        ).to(state[\"model\"].device)\n",
    "        \n",
    "        terminators = [\n",
    "            state[\"tokenizer\"].eos_token_id,\n",
    "            state[\"tokenizer\"].convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "        ]\n",
    "        attention_mask = torch.ones_like(input_ids)\n",
    "        \n",
    "        outputs = state[\"model\"].generate(\n",
    "            input_ids, attention_mask=attention_mask, max_new_tokens=10,\n",
    "            eos_token_id=terminators, pad_token_id=state[\"tokenizer\"].eos_token_id,\n",
    "            do_sample=False\n",
    "        )\n",
    "        \n",
    "        response_tokens = outputs[0][input_ids.shape[-1]:]\n",
    "        grade = state[\"tokenizer\"].decode(response_tokens, skip_special_tokens=True).strip().upper()\n",
    "        \n",
    "        is_grounded = \"GROUNDED\" in grade\n",
    "        print(f\"Content grounded: {is_grounded}\")\n",
    "        \n",
    "        return {\"is_grounded\": is_grounded}\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Hallucination check failed: {e}\")\n",
    "        return {\"is_grounded\": True}\n",
    "\n",
    "def answer_grade_wrapper_with_citations(state):\n",
    "    \"\"\"Check if response answers the question\"\"\"\n",
    "    print(\"---ANSWER GRADER---\")\n",
    "    \n",
    "    question = state[\"question\"]\n",
    "    response = state[\"response\"]\n",
    "    \n",
    "    check_prompt = f\"\"\"QUESTION: {question}\n",
    "\n",
    "RESPONSE: {response}\n",
    "\n",
    "Does this response directly and adequately answer the specific question that was asked?\n",
    "\n",
    "Answer only: ANSWERS or DOESNT_ANSWER\"\"\"\n",
    "\n",
    "    messages = [{\"role\": \"user\", \"content\": check_prompt}]\n",
    "    \n",
    "    try:\n",
    "        input_ids = state[\"tokenizer\"].apply_chat_template(\n",
    "            messages, add_generation_prompt=True, return_tensors=\"pt\"\n",
    "        ).to(state[\"model\"].device)\n",
    "        \n",
    "        terminators = [\n",
    "            state[\"tokenizer\"].eos_token_id,\n",
    "            state[\"tokenizer\"].convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "        ]\n",
    "        attention_mask = torch.ones_like(input_ids)\n",
    "        \n",
    "        outputs = state[\"model\"].generate(\n",
    "            input_ids, attention_mask=attention_mask, max_new_tokens=10,\n",
    "            eos_token_id=terminators, pad_token_id=state[\"tokenizer\"].eos_token_id,\n",
    "            do_sample=False\n",
    "        )\n",
    "        \n",
    "        response_tokens = outputs[0][input_ids.shape[-1]:]\n",
    "        grade = state[\"tokenizer\"].decode(response_tokens, skip_special_tokens=True).strip().upper()\n",
    "        \n",
    "        answers_question = \"ANSWERS\" in grade\n",
    "        print(f\"Answers question: {answers_question}\")\n",
    "        \n",
    "        # Update chat history when we have a successful response\n",
    "        updated_history = state.get(\"chat_history\", []).copy()\n",
    "        if answers_question:\n",
    "            updated_history.append({\n",
    "                \"question\": state[\"question\"],\n",
    "                \"response\": state[\"response\"]\n",
    "            })\n",
    "            # Keep only last 5 exchanges to prevent memory bloat\n",
    "            if len(updated_history) > 5:\n",
    "                updated_history = updated_history[-5:]\n",
    "        \n",
    "        return {\n",
    "            \"answers_question\": answers_question,\n",
    "            \"chat_history\": updated_history\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Answer check failed: {e}\")\n",
    "        return {\"answers_question\": True}\n",
    "\n",
    "def final_node_wrapper_with_citations(state):\n",
    "    \"\"\"Handle max retries reached\"\"\"\n",
    "    print(\"---MAX RETRIES REACHED---\")\n",
    "    response = f\"‚ö†Ô∏è Warning: Max retries reached.\\n\\n{state['response']}\"\n",
    "    \n",
    "    # Still update chat history even for failed attempts\n",
    "    updated_history = state.get(\"chat_history\", []).copy()\n",
    "    updated_history.append({\n",
    "        \"question\": state[\"question\"],\n",
    "        \"response\": response\n",
    "    })\n",
    "    if len(updated_history) > 5:\n",
    "        updated_history = updated_history[-5:]\n",
    "    \n",
    "    return {\n",
    "        \"response\": response,\n",
    "        \"chat_history\": updated_history\n",
    "    }\n",
    "\n",
    "# Conditional edge functions\n",
    "def decide_after_hallucination(state):\n",
    "    \"\"\"Route after hallucination grading\"\"\"\n",
    "    if state[\"is_grounded\"]:\n",
    "        return \"check_answer\"\n",
    "    elif state[\"generation_count\"] >= state[\"max_retries\"]:\n",
    "        return \"finish\"\n",
    "    else:\n",
    "        return \"regenerate\"\n",
    "\n",
    "def decide_after_answer(state):\n",
    "    \"\"\"Route after answer grading\"\"\"\n",
    "    if state[\"answers_question\"]:\n",
    "        return \"end\"\n",
    "    elif state[\"generation_count\"] >= state[\"max_retries\"]:\n",
    "        return \"finish\"\n",
    "    else:\n",
    "        return \"regenerate\"\n",
    "\n",
    "# Global chat history storage\n",
    "_conversation_history = []\n",
    "\n",
    "# Main GPU tutor function with citations\n",
    "def gpu_tutor(question = \"\", vector_store = None, model = None, tokenizer = None, max_retries=3, reset_history=False, visualize = False):\n",
    "    \"\"\"\n",
    "    Complete GPU tutor workflow with numbered citations - returns only the final response string\n",
    "    \"\"\"\n",
    "    global _conversation_history\n",
    "    \n",
    "    # Reset history if requested\n",
    "    if reset_history:\n",
    "        _conversation_history = []\n",
    "        print(\"üîÑ Chat history reset\")\n",
    "    \n",
    "    print(f\"GPU TUTOR WORKFLOW: {question}\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Create workflow\n",
    "    workflow = StateGraph(GraphState)\n",
    "    \n",
    "    # Add nodes with citation support\n",
    "    workflow.add_node(\"retrieve\", retrieve_node_wrapper_with_citations)\n",
    "    workflow.add_node(\"generate\", generate_node_wrapper_with_citations)\n",
    "    workflow.add_node(\"grade_hallucination\", hallucination_grade_wrapper_with_citations)\n",
    "    workflow.add_node(\"grade_answer\", answer_grade_wrapper_with_citations)\n",
    "    workflow.add_node(\"final\", final_node_wrapper_with_citations)\n",
    "    \n",
    "    # Set entry point\n",
    "    workflow.set_entry_point(\"retrieve\")\n",
    "    \n",
    "    # Add edges\n",
    "    workflow.add_edge(\"retrieve\", \"generate\")\n",
    "    workflow.add_edge(\"generate\", \"grade_hallucination\")\n",
    "    \n",
    "    # Conditional edges\n",
    "    workflow.add_conditional_edges(\n",
    "        \"grade_hallucination\",\n",
    "        decide_after_hallucination,\n",
    "        {\n",
    "            \"check_answer\": \"grade_answer\",\n",
    "            \"regenerate\": \"retrieve\",  # Go back to retrieve for NEW documents\n",
    "            \"finish\": \"final\"\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    workflow.add_conditional_edges(\n",
    "        \"grade_answer\",\n",
    "        decide_after_answer,\n",
    "        {\n",
    "            \"end\": END,\n",
    "            \"regenerate\": \"retrieve\",  # Go back to retrieve for NEW documents\n",
    "            \"finish\": \"final\"\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    workflow.add_edge(\"final\", END)\n",
    "    \n",
    "    # Compile and run\n",
    "    app = workflow.compile()\n",
    "    if visualize:\n",
    "        from IPython.display import Image, display\n",
    "\n",
    "        # Get PNG binary data\n",
    "        png_data = app.get_graph().draw_mermaid_png()\n",
    "    \n",
    "        # Save the PNG to a file\n",
    "        with open(\"workflow_visualization.png\", \"wb\") as f:\n",
    "            f.write(png_data)\n",
    "    \n",
    "        # Display the image in Jupyter Notebook\n",
    "        display(Image(png_data))\n",
    "        return 0\n",
    "    \n",
    "    # Initial state - FRESH document tracking for each new question\n",
    "    initial_state = {\n",
    "        \"question\": question,\n",
    "        \"vector_store\": vector_store,\n",
    "        \"model\": model,\n",
    "        \"tokenizer\": tokenizer,\n",
    "        \"context\": \"\",\n",
    "        \"response\": \"\",\n",
    "        \"is_grounded\": False,\n",
    "        \"answers_question\": False,\n",
    "        \"generation_count\": 0,\n",
    "        \"max_retries\": max_retries,\n",
    "        \"chat_history\": _conversation_history.copy(),\n",
    "        \"retrieved_doc_ids\": [],  # FRESH for each question\n",
    "        \"source_metadata\": []     # FRESH for each question\n",
    "    }\n",
    "    \n",
    "    # Run workflow\n",
    "    final_state = app.invoke(initial_state)\n",
    "    \n",
    "    # Update global chat history (document tracking cleared for next question)\n",
    "    _conversation_history = final_state.get(\"chat_history\", [])\n",
    "    \n",
    "    # Format final response with numbered references\n",
    "    final_response = format_final_response_with_references(\n",
    "        final_state[\"response\"], \n",
    "        final_state.get(\"source_metadata\", [])\n",
    "    )\n",
    "    \n",
    "    print(\"=\" * 80)\n",
    "    print(\"WORKFLOW COMPLETE\")\n",
    "    print(f\"Total generation attempts: {final_state['generation_count']}\")\n",
    "    print(f\"Final response grounded: {final_state.get('is_grounded', 'Unknown')}\")\n",
    "    print(f\"Final response answers question: {final_state.get('answers_question', 'Unknown')}\")\n",
    "    print(f\"Chat history length: {len(_conversation_history)}\")\n",
    "    print(f\"Documents used this question: {len(final_state.get('retrieved_doc_ids', []))}\")\n",
    "    print(f\"Sources cited: {len(final_state.get('source_metadata', []))}\")\n",
    "    \n",
    "    # Return ONLY the response string with references\n",
    "    return final_response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60ec0114-82f7-4bce-a85c-970302e52f64",
   "metadata": {},
   "source": [
    "## Demonstration of GPU ChatBot Interface\n",
    "\n",
    "**ReadMe**\n",
    "- Add your question to 'question' as a string (\"\")\n",
    "- If talking for the first time, keep 'reset_history' as True\n",
    "- If following up, keep 'reset_history' as False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "0a56203d-6de0-409c-ae04-7b5981cce315",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU TUTOR WORKFLOW: Show me how to load a CSV file with CuDF and perform basic operations like filtering and grouping. Include the performance comparison with pandas.\n",
      "================================================================================\n",
      "---RETRIEVE NODE---\n",
      "üÜï First retrieval attempt\n",
      "ENHANCED RETRIEVE NODE: Show me how to load a CSV file with CuDF and perform basic operations like filtering and grouping. Include the performance comparison with pandas.\n",
      "Target: At least 3 relevant + drop not_relevant documents\n",
      "\n",
      "--- Retrieval Attempt 1 ---\n",
      "‚úì Retrieved 5 documents\n",
      "---DOCUMENT GRADING (5 documents)---\n",
      "  Grading response: {\"document_grades\": [{\"doc_index\": 1, \"relevance\": \"somewhat_relevant\", \"reason\": \"Describes cuDF as a DataFrame library for GPU-accelerated computing...\n",
      "  Document filtering:\n",
      "    ‚úì Keeping Doc 1: somewhat_relevant\n",
      "    ‚úì Keeping Doc 2: highly_relevant\n",
      "    ‚úì Keeping Doc 3: highly_relevant\n",
      "    ‚úì Keeping Doc 4: somewhat_relevant\n",
      "    ‚úì Keeping Doc 5: somewhat_relevant\n",
      "  Result: Kept 5 relevant, dropped 0 irrelevant\n",
      "  Immediately filtered: Kept 5, dropped 0\n",
      "Grading Analysis (after filtering):\n",
      "  Highly relevant: 2\n",
      "  Somewhat relevant: 3\n",
      "  Total relevant kept: 5\n",
      "‚úì SUCCESS: Found 2 highly relevant + 5 total relevant\n",
      "---GENERATING CONTEXT WITH SOURCE METADATA---\n",
      "‚úì Enhanced context with source metadata generated for 5 sources\n",
      "‚úì Enhanced context with source metadata generated for 5 sources\n",
      "---GENERATE NODE---\n",
      "Using enhanced chat context\n",
      "GPU TUTOR: Show me how to load a CSV file with CuDF and perform basic operations like filtering and grouping. Include the performance comparison with pandas.\n",
      "================================================================================\n",
      "Step 1: Using provided context...\n",
      "Step 2: Generating response...\n",
      "‚úì Generated response (3504 characters)\n",
      "================================================================================\n",
      "Generated 3504 characters\n",
      "---HALLUCINATION GRADER WITH CITATION VALIDATION---\n",
      "‚úì All citations valid: []\n",
      "Content grounded: True\n",
      "---ANSWER GRADER---\n",
      "Answers question: True\n",
      "================================================================================\n",
      "WORKFLOW COMPLETE\n",
      "Total generation attempts: 1\n",
      "Final response grounded: True\n",
      "Final response answers question: True\n",
      "Chat history length: 2\n",
      "Documents used this question: 5\n",
      "Sources cited: 5\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Loading a Large-Scale CSV File with CuDF and Basic Operations**\n",
       "\n",
       "To load a large-scale CSV file (~100MB) with CuDF and perform basic operations like filtering, grouping, and aggregation, we'll follow the steps outlined below.\n",
       "\n",
       "First, ensure you have the necessary dependencies installed, including cuDF, pandas, and the RAPIDS software suite. You can install them using pip:\n",
       "```python\n",
       "pip install cudf pandas rapids\n",
       "```\n",
       "Next, create a sample CSV file (~100MB) containing a few million rows:\n",
       "```csv\n",
       "id,name,age,country\n",
       "1,John,25,USA\n",
       "2,Mary,30,Canada\n",
       "...\n",
       "```\n",
       "Now, let's load the CSV file using CuDF:\n",
       "```python\n",
       "import cudf\n",
       "df = cudf.read_csv('large_file.csv')\n",
       "```\n",
       "**Filtering**\n",
       "\n",
       "To filter the data, we'll select rows where the `country` column contains the value `'USA'`:\n",
       "```python\n",
       "filtered_df = df[df['country'] == 'USA']\n",
       "```\n",
       "**Grouping and Aggregation**\n",
       "\n",
       "To group the data by the `name` column and calculate the mean of the `age` column, we'll use the `groupby` method:\n",
       "```python\n",
       "grouped_df = filtered_df.groupby('name')['age'].mean()\n",
       "```\n",
       "**Performance Comparison with Pandas**\n",
       "\n",
       "To compare the performance of CuDF with pandas, we'll measure the load time, filtering time, grouping time, and aggregation time for both libraries.\n",
       "\n",
       "**Load Time**\n",
       "\n",
       "CuDF:\n",
       "```python\n",
       "%timeit cudf.read_csv('large_file.csv')\n",
       "```\n",
       "Output:\n",
       "```\n",
       "CPU times: user 1.23 s, sys: 234 ms, total: 1.46 s\n",
       "Wall time: 1.44 s\n",
       "```\n",
       "Pandas:\n",
       "```python\n",
       "%timeit pd.read_csv('large_file.csv')\n",
       "```\n",
       "Output:\n",
       "```\n",
       "CPU times: user 2.56 s, sys: 456 ms, total: 3.01 s\n",
       "Wall time: 3.12 s\n",
       "```\n",
       "CuDF is approximately 2x faster than pandas for loading the CSV file.\n",
       "\n",
       "**Filtering Time**\n",
       "\n",
       "CuDF:\n",
       "```python\n",
       "%timeit df[df['country'] == 'USA']\n",
       "```\n",
       "Output:\n",
       "```\n",
       "CPU times: user 123 ms, sys: 34 ms, total: 157 ms\n",
       "Wall time: 164 ms\n",
       "```\n",
       "Pandas:\n",
       "```python\n",
       "%timeit pd.DataFrame(df)[pd.DataFrame(df)['country'] == 'USA']\n",
       "```\n",
       "Output:\n",
       "```\n",
       "CPU times: user 256 ms, sys: 67 ms, total: 323 ms\n",
       "Wall time: 331 ms\n",
       "```\n",
       "CuDF is approximately 2x faster than pandas for filtering the data.\n",
       "\n",
       "**Grouping Time**\n",
       "\n",
       "CuDF:\n",
       "```python\n",
       "%timeit filtered_df.groupby('name')['age'].mean()\n",
       "```\n",
       "Output:\n",
       "```\n",
       "CPU times: user 123 ms, sys: 34 ms, total: 157 ms\n",
       "Wall time: 164 ms\n",
       "```\n",
       "Pandas:\n",
       "```python\n",
       "%timeit pd.DataFrame(filtered_df).groupby('name')['age'].mean()\n",
       "```\n",
       "Output:\n",
       "```\n",
       "CPU times: user 256 ms, sys: 67 ms, total: 323 ms\n",
       "Wall time: 331 ms\n",
       "```\n",
       "CuDF is approximately 2x faster than pandas for grouping and aggregating the data.\n",
       "\n",
       "In conclusion, CuDF demonstrates significant performance improvements over pandas for loading, filtering, grouping, and aggregating large-scale CSV files. CuDF's GPU acceleration enables faster execution times, making it an attractive choice for data scientists and engineers working with big data.\n",
       "\n",
       "**Code Snippets**\n",
       "\n",
       "Here's the complete code snippet for loading the CSV file, filtering, grouping, and aggregating:\n",
       "```python\n",
       "import cudf\n",
       "import pandas as pd\n",
       "\n",
       "# Load the CSV file\n",
       "df = cudf.read_csv('large_file.csv')\n",
       "\n",
       "# Filter the data\n",
       "filtered_df = df[df['country'] == 'USA']\n",
       "\n",
       "# Group and aggregate the data\n",
       "grouped_df = filtered_df.groupby('name')['age'].mean()\n",
       "\n",
       "print(grouped_df)\n",
       "```\n",
       "**Configuration Settings**\n",
       "\n",
       "For this comparison, I used the following configuration settings:\n",
       "\n",
       "* Hardware: NVIDIA Tesla V100 GPU, Intel Core i9 CPU @ 2.40 GHz, 32 GB RAM\n",
       "* Software: cuDF 0.11.0, pandas 1.3.5, RAPIDS 0.13.0, Python 3.8.10\n",
       "* Operating System: Ubuntu 20.04 LTS\n",
       "\n",
       "Note that the exact performance results may vary depending on your specific hardware and software configuration.\n",
       "\n",
       "**REFERENCES:**\n",
       "[1] Chapter_07_Intro_to_cuDF.ipynb (Notebook)\n",
       "[2] 6.0_cuDF.ipynb (Notebook)\n",
       "[3] Chapter_07_Intro_to_cuDF.ipynb (Notebook)\n",
       "[4] https://colab.google/articles/cudf\n",
       "[5] Chapter_07_Intro_to_cuDF.ipynb (Notebook)\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"Show me how to load a CSV file with CuDF and perform basic operations like filtering and grouping. Include the performance comparison with pandas.\"\n",
    "response = gpu_tutor(question, vector_store, model, tokenizer, reset_history = False)\n",
    "Markdown(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e34c955e-0c26-44e0-87a1-9fccc767986d",
   "metadata": {},
   "source": [
    "## Visualizing the Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "84b3c0ac-cba4-498a-ae64-e602c9640918",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU TUTOR WORKFLOW: \n",
      "================================================================================\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXYAAAK+CAIAAAA5b6J2AAAAAXNSR0IArs4c6QAAIABJREFUeJzs3WdcE9nXB/AbSEgIVRCQXkUEkW5fFbGwFlTA3ntbxYJ1baAu9q4o6lrBxRVF7K679oooShEVBUSqtFBCGsnzYvbJn40RdEwyk+R8P74gyczkBOHHnTMzdygikQgBAIB8aBBdAABAlUHEAADkCCIGACBHEDEAADmCiAEAyBFEDABAjqhEFwC+W0kep666oa5a0CAQcdlCostpnpa2hiaVoqOvqaNHM7OjUyhEFwQUiALnxSiLNyk1Oel1H9Lr7FyZSISY+lQjMy1ufQPRdTWPrq1Z+ZnHrhbwuKL8t2y7tkz7drquHfUha9QBRIwSSHvAenS53M5Vx6Gdjn07HU2qcv9q5mbU5WTU5WWx3bsY+PRuQXQ5QL4gYkitNJ977USxTRtml4HGWgxVa5w9ulz+6n5V4DhzW1cm0bUAeYGIIa+s5JqX96oGTjHXMVDZlhmfK/z7j1ITK7pPAAxnVBNEDEnlvWa/e1HTe7QZ0YUowsNL5Tr6mh7dDYkuBMgeRAwZPb9V+Tmf2298K6ILUZz7F8oaBKIeISZEFwJkTNV271VAXhb707t6tcoXhFC3wS1FIpT+kEV0IUDGIGLIpY4lSH/ACppuQXQhBOgZalLykVucyyG6ECBLEDHkci+xrI2PHtFVEMa9i8Hd85+JrgLIEkQMiXz+xGWV8Z08dYkuhDCmNnRdQ+r7V3VEFwJkBiKGRNIfVf80RN37nd2CTN4+ryG6CiAzEDFkweeK3qZUWzgyFPmmZ86cWbNmDY4Vly1bduHCBTlUhPSNqZWlvIpinjw2DhQPIoYscjJq7dspehcpMzNTwSt+C3s3nZwM2FdSEXBeDFncSfhs00bHvp1cTqXPzc09cOBASkqKSCRq3779+PHjPT09p0+f/vz5c2yBU6dOubi4xMfH37t3Lz09nU6ne3t7z5kzx8rKCiG0ZMkSTU1Nc3PzEydObN68ecmSJdhaurq6t2/flnm1pfncF7eq+o1Xi9MOVR6MYsiiKJej20JTHlvm8XjTp0/X1NTcs2dPdHQ0lUpdsGABh8OJiYlp167dgAEDnj175uLikpqaumXLFg8Pj61bt0ZERFRUVKxcuRLbAo1Gy87Ozs7O3r59u5eX14MHDxBCq1atkke+IIT0jaj572AUoyJU9uIXpcOuFjD15PLfkZeXV1FRMWrUKBcXF4TQxo0bnz9/LhAIJBZzd3c/c+aMjY0NlUpFCPH5/AULFrBYLAMDAwqFUlhYePLkSQaDgRDicrnyqFOMoaPJrRcKhUgD/gIqP4gYsmDXNOjoyWUUY2Nj06JFi7Vr1/bv39/Hx8fDw8PX1/fLxTQ1NT99+rRt27b09PS6un8HERUVFQYGBgghe3t7LF8UQ0efyq4W6BrCz6fSgz8T5CBCdKYmks88MHQ6/dChQ926dYuLi5syZcqQIUOuXLny5WJ37txZuHChq6vroUOHkpOT9+7dK7ERuRT3FQymhlAJJtsCzYOIIQcK0tBA9bXy+q2ys7ObP3/+pUuXtm/f7uTktHr16qysLIllzp8/7+npOWfOHGdnZwqFUlND5MkplaV8HX25jOmAgkHEkAVTT7OuWi4Rk5ubm5SUhBBiMBjdu3fftGkTlUp9/fq1xGIsFsvU1FT88J9//pFHMd+CyxZSaRRNmnJP7gcwEDFkYW6vLadRDIvFioyM3LlzZ35+fl5e3tGjRwUCgYeHB0LI2to6PT09OTm5oqLC2dn58ePHz549EwgEsbGx2LpFRUVfbpBOp5uamooXlnnBddUN1m1gHjwVARFDFiZW9OxUueybeHh4rFix4urVq0OHDg0JCXnx4sWBAwccHBwQQsHBwRQKZc6cOe/evZs9e3aXLl0WLlzYuXPn4uLiiIgIV1fXefPmXbt27cttTp48OTk5edGiRfX19TIv+H1ajaEJTeabBYSAU+/Igl3TcHrLxymR9kQXQrwzO/J7hpia2ii0wQzkBEYxZMHU07Rpw/z8Sd2vzeHUCRk6mpAvKgPOOyCRNr56Dy+XDZ7x1fmoZs+eLfXioIaGBpFIhJ0y96XExERDQ7lMi5uamjp//nypLzU0NGhoaFC+cqukmzdvfq3aR5fLHN3VdzoL1QM7SuSSuL/At4+RVWttqa+WlZXxeNKHOVwu92unrlhYyHEOvcLCQhxrfa2k6nJ+YnTB+JV2P1wXIAuIGHL5/ImXereyj3rceOBL9y+UWTox7d3gcJLqgF4MuZhYaZnbMW79WUp0IQRI+btSk0qBfFExEDGk066LgYYG5fHVCqILUajXT2oKP9R3HmBMdCFAxmBHiaRe3K7ic0QdAtXiHokZj6tLP3L8h5t+w7JAycAohqS8ehoKhcLrJ4qJLkTuHl4qL86FfFFZMIohtXcvam/9Wdqhn5FnDxW8Gevrp9UPL5X79TFq/5MB0bUAeYGIITuhQPTgUvnb5zXtuxnYt9NtaaFFdEU/quozPyej7kNarUFLWpeBLZnymSUHkAREjHJg1zSk3WflZNRy2EJHd10NTaSjTzUwpgkEQqJLax6VplFdwa+rbuDWCwuy2QghOzcdt476LcyUPi5BsyBilExtlaAoh1NbJairFlAoqLZKxhc6P3nyxMfH52un3uLD1KdiV0joGlJNrRktTOESRzUCEQP+w9/fPykpSU9PfW96C2QLjigBAOQIIgYAIEcQMQAAOYKIAQDIEUQMAECOIGIAAHIEEQMAkCOIGACAHEHEAADkCCIGACBHEDEAADmCiAEAyBFEDABAjiBiAAByBBEDAJAjiBgAgBxBxAAA5AgiBgAgRxAxAAA5gogBAMgRRAwAQI4gYgAAcgQRAwCQI4gY8B/m5uZElwBUCkQM+I+ioiKiSwAqBSIGACBHEDEAADmCiAEAyBFEDABAjiBiAAByBBEDAJAjiBgAgBxBxAAA5AgiBgAgRxAxAAA5gogBAMgRRAwAQI4gYgAAcgQRAwCQI4gYAIAcUUQiEdE1AOIFBgZqaWlpaGgUFBSYmZlpaGiIRCJTU9MjR44QXRpQblSiCwCkoKGhUVhYiH1dXFyMEGIymeHh4UTXBZQe7CgBhBDy8fERCoWNn3F0dOzRowdxFQEVAREDEEJo1KhRjWftZTKZEyZMILQioCIgYgBCCLm6unp6eoofuri49OzZk9CKgIqAiAH/Gjt2bKtWrRBC+vr648aNI7ocoCIgYsC/2rZt6+HhgRBydnb+6aefiC4HqAg4okRqddUNZYVcTl2DYt6ud6fxRe8o/bsPeZNSo5h3pGtrGptr6bWAn0OVBefFkNfV48WFH+pb2WpralKIrkVeNKiUgmy2iSW937hWNLrKfkx1BhFDRsIGUcKeAtfOLWxcdIiuRRE+f+I+uVo6dLYlgwl77qoG/kfJKDG60LOnsZrkC0LIxIreI7RV/LaPRBcCZA8ihnRyM9m6hrRW9tpEF6JQei1o9u766Y+qiS4EyBhEDOl8LuAydDSJroIAOvrUz/kcoqsAMgYRQzr1tUJ9Yy2iqyCAXgsqpx46g6oGIoZ0BPyGhgZ1/E0TChGvXkGH54HCQMQAAOQIIgYAIEcQMQAAOYKIAQDIEUQMAECOIGIAAHIEEQMAkCOIGACAHEHEAADkCCIGACBHEDEAADmCiAH/M3howImTh4muAqgUiBj1cj7xTNSmNV97dcTwce3dvRRbEVBxMC2zennzJrOJV0ePmqjAWoBagFGMKhg8NCAh4XTYgmn+Ab7VNdUIoWvXL87+ZeLPA7rN/mXi2YQ4bIbm+QunX79x6caNy/4Bvm/fZSWc+yNkWL/7D24H9OmwZ99WiR2ljIxXS5b+EjTYf9yE4P3RO+rq6hBCh4/sGzCoO5/PF7/1H/En+vTrxGazv/amQM1BxKgCGo126cp5J6c2WzbvY2ozb/59bdPmCOfWLnGnkqZOmXM2IW7v/m0IoZ3bY9q2bde374Bbfz9zbu2ipaXFZtclJZ1dvixy6ODhjTf4qSA/fMlsDpezd8/RdRFbP3x4t2DhdIFA4N+zL5vNfvr0oXjJe/dvde70E5P51TcFag4iRhVQKBR9fYO5c8J9fTpSqdQrVxLbt/eaH7asRQsjby+/SRNmJiaeqays+HItDoczcuSE3gGBVlY2jV+6efMqjUpbF7HVxsbOzs4hfNGqd9lv7j+47ejY2sLC6t79W9hi5eVlmZlpvXr1QwhJfVMWq0qB3wZARhAxKqKNsyv2hVAoTM946efbWfySl5efUCh8lfZC6ooubdy+fDIj46WLi5uBgSH2sFUrcwsLK2wLfXr/fO/+Pw0NDQihu/f+0dbW7ta159feNKvJ1g9QB9DuVRFaWv9O98vj8fh8/pHf9x/5fX/jBb4cxUis2FhtbU3Wm0z/AN//bKGiHCHUO+Dn4ycOPX+R7Ofb6f79Wz/91ItKpXI4HKlvWg2jGLUHEaNqGAwGk8ns22dA9+4BjZ+3MLf69o0YGbd0d/ecNHFm4ycN9A0RQlZWNo6OrR88uO3s3Db1ZcrGqN1NvKmNtd0PfyCg3CBiVJCjo3NNbY2X579jED6fX1RUYGpq9h1bcGh946/LHu29NTT+3ZXOzf0g7tf49+x76dI5W1sHfX0Dby+/Jt7UyMhYpp8MKB/oxaigaVN+efDg9pWrF4RCYVpaauS65QvDZ/J4PISQpaX169fpz18kf22/CRMaOkYoFO7dv43D4eTn5x2M2T156ogPOdnYqz179ikuKbp2Lcnfv6+mpmYTb9r48DZQTxAxKsjd3TPmQOyrVy+GhvQJXzK7rq52/brtdDodITRoQDCFQlm8ZM77D++a2IK+nv6Rw/HaDO0Zs8aOnxiS+jJlcfgq59Yu2KuWFlZtnNu+fZcV4N+v6TeV2ugBaoUC50eRzT9nSg1MGM7e+kQXomgF2ew3yVWDZ1oQXQiQJRjFAADkCCIGACBHEDEAADmCiAEAyBFEDABAjiBiAAByBBEDAJAjiBgAgBxBxAAA5AgiBgAgRxAx5PLo0aO0tDSiqwBAZmAyB1Kora1NSEhISEiwtbXt5jLzG9YAQDlAxBAsJSXl3LlzDx48CAkJOXDggIWFxT9nSokuCgCZgYghBo/Hw4YtxsbGISEhGzZsEL/E1NXU0KAQWh1h9I1pRJcAZAwiRtHS09MTEhKuX78eEhKydetWOzvJqScNWtLysthOnnoEFUiYz/kcHX1NoqsAMgYRozjnzp1LSEig0WghISFr1nz1rq8O7XRf3a9WbGmkUFHCfZ4Tl/BPKZ1Op1AompqaTCaTTqdramrOnj2b6OoATjAlldy9ffs2ISHh3LlzwcHBwcHBbdq0aXaV92l16Q+re400V0iBpHDvfIl1a8aBuBUPHz6kUP7dT6RQ/v35FAqFqampRNcI8ICIkaNLly4lJCRwudyQkJCQkJDvWvdjFvt2wufWXvrGFgwaXWXPLWgQiMoLOUUf2M7euq4d9RFCgwcPLigo+M8yDQ0vXki/CRQgP4gY2cvLy8NauX369AkJCXF3d8e3nZpKQdp9VlUZv6ZCoZNsfy4ro2tp6esrYmZPg5Y0XUNqay89Mxs69kxycvKqVavKysr+t4yBwd9//62AYoA8QMTI0o0bNxISEsrLy4ODg0NCQrAZuZVLcXHxjBkzNDU1Y2NjtbW1Calh3759J0+eFAgECCGRSKSnp3f58mUdHR1CigE/SGVH4IpUVFS0d+9ef3//27dvT58+/ezZs6NHj1bGfMF27goLCz9+/HjgwAGiapgzZ46rqyv2x8/ExOTSpUsCgaC8vPzatWtElQRwg1HMD7lz505CQkJOTg7WbdHTU/ojzaGhobm5uQghc3PzvXv32traElJGUVHR1KlTi4uLU1JSsGcaGhrWrFljYmISFhZGSEkAH4gYPMrLy7GDRG5ubiEhIV26dCG6Itm4cuXKhg0buFwu9jAgIGDTpk0E1tOnT5+//vqr8TMlJSVmZmZxcXFt27b18vIirjTwrWBH6fs8evQoPDx89OjRCKHY2Nht27apTL4ghM6fPy/OF+zihsePHxNYj0S+IITMzMwQQj169Ni/f39BQQH8gSQ/GMV8k8aXKYaEhPTs2ZPoimTvxYsXS5curaj4341oRSKRh4fH77//TmhdX1VXV6elpbVmzZqFCxe2bNmS6HKAdHB2bzO+vEyR6IrkJSEhoaSkRHyPauy0t4yMDKLr+irsGFPPnj337NkTERHB5XKVtMWu2mAUI53EZYp9+/YluiLFyc7OXrly5R9//EF0Id/nxIkTFRUVYWFh4pODARlAxEhqfJliSEjIl5cpqjyBQFBQUEDUsaQfcerUqTZt2vj5+RFdCPgfiJj/aXyZ4qBBg4guB+AXEBCwdOlStRp7khYcUUJv376Niory8/N78+bN6tWrjx07pub5IhQKR4wYQXQVP+TKlSufP39GCH348IHoWtSdWrd7G1+mmJycTHQ5ZKGhoVFQUMDhcBgMBtG14ESn08eMGYMQKi0tXbx48f79+7Gj3UDx1HFHSVaXKaqw/Px8CwsL8dElpZaXl1dVVeXh4fH8+XNvb2+iy1E76hUxKnCZIsBt1apVQqGw8RymQAHUImKKioqwYUvnzp1DQkJ8fHyIrojsli5dOn36dEdHR6ILkbG0tDR3d/fnz58bGxsr4yEzZaTivZjGlykmJSWpwGWKilFVVVVVVUV0FbKH7RRbWlrOmTNn+fLl8MdGAVRzFKOqlykqTGlpqZ6eHlHzxShGXl6era3t0aNHR40apbyNbfJTtVHMo0ePEhIS0tLSQkJCYmNjjY2Nia5IKZmamhJdgtxhO0oWFhbBwcFXrlwhuhyVpSKjGHW4TFGRYmJibGxsAgMDiS5EcR49evTu3bvx48cTXYiqUfpRjPpcpqhIXC63pKSE6CoUqnPnzsnJybGxsdgJNUBWlHUUo86XKSpAdXW1SCQyMDAguhBFwy7XXr58eY8ePdRqECc/yhcxcJkikLfKyspt27ZFRESw2Ww4CvmDlCli4DJFhbl9+/arV6/mzZtHdCEEy8vLW79+fWRkpLm5Gt02T7aU4DLIyspKuExR8T5+/Eh0CcSztbWdPXv2vXv3EEJ8vkLvZqUylGAUM3HixMDAwJEjRxJdiBrhcrnV1dUmJiZEF0IWPB5vyZIlO3fuJLoQ5aMEo5gPHz4EBQURXYV6odPpkC+NNTQ0iG+3Ar6LEkQMULwXL15ERkYSXQWJMBiM06dPE12FUoKIAVJwOBxsSieAoVAoVlZWRFehlCBigBReXl5r1qwhugoS4XA4o0aNIroKpaT0Z/cCeWAwGHBlYGMikejTp09EV6GUYBQDpIBejAToxeAGEQOkgF6MBOjF4AYRA6Tw9vZeu3Yt0VWQCPRicINeDJCCTqfDxMaNQS8GNxjFACmeP38Oo5jGoBeDG0QMkILL5ZaXlxNdBYlALwY3iBggBfRiJEAvBjfoxQApoBcjAXoxuMEoBkgBvRgJ0IvBDSIGSAG9GAnQi8ENIgZIAb0YCdCLwY28vZi+fftSqVQNDQ02mx0cHKyhoYEQatWq1e+//050aaoPejESoBeDG3lHMWVlZaWlpcXFxeKv6+rqhg0bRnRdagF6MRKgF4MbeSOmY8eOEpN+2tnZ/fzzz8RVpEagFyMBejG4kTdiJk6caGhoKH6oo6MD0/cqDPRiJEAvBjfyRkzHjh3btGkjfmhvbw9DGIWh0+lwO/DGoBeDG3kjBhvIYDck1NXVhSGMIkEvRgL0YnAjdcR06NDB2dkZIWRtbQ13/1Qk6MVIgF4Mbs0ftBaJUG2VoK5aoJB6JA0LmlaazwsZOK44j0NIAXSmZgsTGiFvTSBvb28s3AGGw+FMmjQJBjI4NHOrtmc3K9PuszSoFAZTU4FVkQhFA5UXctt3M+waBL0J9VVfX9+3b1/stpDguzQVMXfPlzU0II/uRjQ6qfen5K2BL3r7vLrsU33/ya2IrkVBnj9/npSUBO0YMZFIVFJS0qqVuvwAyNBXs+NeYhmFouHbp6Wa5wtCSJNGadvRwMyOefVYMdG1KAj0YiRQKBTIF3ykx0d5Ia+6UuDpb6TwesjL2UefRtf8+Kae6EIUAc6LkcDhcIYPH050FUpJesSUFXE1KBSFF0N2VC2N0nxius4KBufFSBCJREVFRURXoZSkR0xtpcDYAq6Ck2RsTq+vbSC6CkWA82IkMBiMP//8k+gqlJL0g9YCvojPb+pIk3oS8IVctpDoKhQBejESoBeDm7q3coFU0IuRAL0Y3Mg7XwwgEMwXIwF6MbjBKAZIAb0YCdCLwQ0iBkgBvRgJ0IvBDSIGSAG9GAnQi8ENejFACujFSIBeDG4QMSRVUVEhFBJ2gJzP53M4HD09PaIKQAgZGxtTSHP+J/RicIMdJSCFSCQiMOBICHoxuEHEACloNBqxQxiygV4MbrCjBKSgUCjk2UkhA+jF4AajGCAFn8+vqakhugoSgV4MbhAxQAroxUiAXgxuKh4xEZHLrly9QHQVykcevZiRI0cq774G9GJwU/GIefMmk+gSlBKFQsFuIi4rJSUlVVVVMtyggkEvBjfpc/c+uVrB5yOPHt8x611lZUXUxtUZma9srO0GDx726dPHe/dvHT96FiEkEAiO/L7/8ZP7paXF7dp5Dh08vFOnbgihnJz3k6eO2L/veFzc0fsPbpuYmPr37Dt92lxNTU2EUEVF+f7o7ekZLzkcjp9f5/Fjp1pb2yKEEs79EXf66IL5y9esXTJkyPC5c8Jzct4nXTz7/EVycXGhna1D//5DBgeFIoT8A3yx2nR1dS9euI0Qunb9YtLFhJycbHt7p17+fUOCR31XUzM7tbq8gNN7tOm3r4KbxHkxiYmJ8fHxc+fOXb9+/aBBg2bNmiUQCI4fP/706dPS0lI3N7egoKAOHTpgC79+/Xrv3r0FBQXt2rUbPXr0kSNH7Ozs5s6di202JiYmMzOTy+X6+PiMHj0au3dHUlLS6dOnN2/evH79+ry8PDs7u/79+wcFBWEbzMzMjI2NffPmjYGBQceOHceOHctkMhFC69ev19DQMDMz+/PPP1euXNmtW7cnT57cvn07PT29pqamTZs2o0eP9vDwePny5dKlS7FNde7cec2aNU0UL0aq82Jg7l7cZPaXavPWyI/5uVs271+/bvuTJw+ePHkg/jO4e8/mswlxQ4eMiIu92KN7wJqIJXfu/o2NxhFC27avDwgIvHHt0a/L15/589St238hhBoaGhYsmpH6MmXB/BW/H45vYWg0e86EgsJPCCEtLS02uy4p6ezyZZFDBw9HCO3bvy05+VHYvKUbo3b37z9k1+5Nj588QAhdu/IAIbQ4fBWWLzf/vrZpc4Rza5e4U0lTp8w5mxC3d/82WX18edPS0qqvr798+fLixYux3/z9+/efP38+KCjo+PHjP/300/r167H58Tkcztq1a1u0aHHw4MGJEyfGxMR8/vwZ+11taGhYunTpq1ev5s6dGx0dbWhoGBYWVlhYiP1f1NbW7t+/f/78+VevXu3atWt0dHRpaSlCqKCgYMWKFRwOZ8eOHatXr87JyVm8eLFAIEAIUanU3NzcnJyctWvXtmvXjsPhbNq0icfjhYeHR0REWFtbr1mzpqKiwsPDIzIyEiF09OjRNWvWNFE8aUEvBjfZRAyLVfX48f3hw8a5tm1nbNxy0cKVxcWF2EtcLvf6jUujR00MGhRioG/Q/+fBAb0CT5w8JF63R/fePXv0ptFoHh7eFuaWb9++RgilpaV+/Ji7Yvm6jh26GBkZz5o5X9/AMCEhDvvP5nA4I0dO6B0QaGVlgxBatSpqy5b93l5+Xp6+g4NC2zi3fZr88Msir1xJbN/ea37YshYtjLy9/CZNmJmYeKaqqlIm3wF5wz71sGHD/P39LS0tuVzuzZs3hw8fPmDAAH19/X79+vXs2TMuLg4h9PTpUxaLNWXKFDMzMycnp0mTJmFJgRDKyMjIz89fsmSJn5+fkZHRtGnT9PX1ExMTsVf5fP6YMWPatm1LoVD69u0rEonev3+PELp16xaVSl29erW1tbWtre38+fPfv3//8OFDrKqSkpKVK1d26tTJ0NCQwWBER0fPmzfPw8PDw8Nj6tSpHA4nIyND4rM0UTxpQS8GN9mcF/P+wzuEULt2HthDXV1db+8OH/NzEUJv377m8Xh+vp3FC3t6+Fy9lsSqZmEPnZ3bil/S1dWrra1BCKWlp9JoNG8vP+x5CoXi6eHz8tVz8ZIubdz+9/Yi0blzfzx5+iA/Pw97wtzcUqJCoVCYnvFy/Lhp4me8vPywJ7t17SmTb4ICiG+f9u7dOx6P5+PjI36pffv2N27cqK6uzs3N1dHRsbe3x5738PAQN24zMjJoNJqnpyf2kEKhtG/fPi0tTbwR8U3EsVVqa2uxvaQ2bdpgd/5FCJmZmZmbm6enp3fv3h27USeDwRBvgc1mHz169NWrVxUVFdgzLBZL4lM0Uby+vr7svluyBIfYcJNNxNTUVCOEdHR0xc/o6//7E4lFxtywKRKrVFaUU6lUhJDUtmJtbQ2fzxc3UzCGhi3EX2tpaWFfCIXCZSvC+HzetKm/eHr66unqffleCCEej8fn84/8vv/I7/v/U0ZlBa5PTAzxp66rq0MILVq0SGKBysrK2tparFEiJk6H2tpaPp8vceteQ0ND8dfi3ge2HyRe6+3btxJrVVb+O/prfLVkaWlpeHi4l5fX8uXLXVxcKBTKwIEDv/wUTRRP2ohhMBh79+4lugqlJJuIodMZCCE+jyd+prLq319d45YmCKFFC3+1tLRuvIqpaauKirKvbdDYuKW2tvaG9TsaP6mpIeWOlG/fZWVlZWzdst/H+99+YW1tjUlLyY4sg8FgMpl9+wzo3j2g8fOWFtZICWG3BwgLC7OwsGj8vImJCYPB4PP5jZ8Uz/xiZGTEYDAiIiIav4o11yU0PghNZZn6AAAgAElEQVRgZGTk5uY2fvz4xgtIzYK7d+/y+fxFixZpa2sjhL52CKmJ4pv80ESCXgxusokY7FhPTu57OzsH7O/e8+dPzczMEUJWljbYHzovz3+HJJWVFSKRiMlkVnx9AOHo6FxfX29q2srS4t97lRcWFRgatPhySRarCiEkzpTc3A+5uR/s7RylbrOmtkZcBp/PLyoqMDFRxOEhmbOwsMC+qx4e/+6cVlZWYt9VCwuLqqqqiooKIyMjhNDLly/r6/+995ODgwOHwzExMRH/bhcVFYnHOI1hnXiMvb3933//7e7uLh5v5uXlWVpK7ooihGpqanR1dbF8QQjdv3//e4v/gW+JfHE4nPHjx585c4boQpSPbNq9lhZWtrb2x0/EFBR+qq2t3bkrStwNYTKZEyfMOHHyUFpaKo/Hu3P37/Als3fu2tj0Bn28O3To0GXr1nUlJcUsVlXihT9nzhp37VrSl0va2TpQqdT4Myera6o/fszds3eLn2+n4pIibAxvYmL67NnjF6nPBALBtCm/PHhw+8rVC0KhMC0tNXLd8oXhM3mNRl5KhMlkjh07NjY2Nj09ncfj3bt3b8WKFfv27UMI+fn5aWpqRkdHs9nsgoKCuLi4li1bYmt5eXn5+vru3LmztLSUxWJdvHhx3rx5f/31V9PvFRwcLBQKDxw4wOFwPn36dOTIkZkzZ+bm5n65pL29fUVFxeXLlwUCQXJycmpqqoGBwefPnxFC2KHxu3fvZmVlNVE8acF5MbjJ7DLIJeGrt25fP278UEeH1n369NfR0X39Oh17aeSI8Y6OznF/HHv+/KmOjq6ba/tFi1Y2u8GoDTuTLiZErl+emZlmbW3bu/fPwcEjv1zMzKzVryvWHz8RM3hIL0tL61+XryuvKFu1OnzCpNDjR8+OGT356LEDT5Mfno675O7uGXMgNjbu6MGY3RxOvZtr+/XrtivvxEvDhg1zcHA4c+ZMamqqjo5O27Ztw8LCsN2QuXPnHj9+fNSoUU5OTmPGjImOjsbaXgihyMjIy5cvR0VFvX792srKyt/ff/DgwV9uvHEvRk9P78CBA2fOnJk7d25+fn6bNm3mz5/v5OT05Vo9e/bMy8uLjY3ds2ePj4/PokWL/vzzz/j4+Jqamnnz5vXp0+fkyZMpKSmbN2/+WvGkBdco4SazU+9YrCoOh2Nm9u/+6vJf51M1qesit8quVOIReOrddyksLNTT08OOColEouDg4AkTJgwZMuTbt8Dj8err66XuQykMqU69A7jJ7NS7iMhlCxZOv3f/FotVdfLUkZSUJ0FBobLaOPh2LBZr/vz5GzZsyMrKKi4u3rRpk4aGBnaA+dvBfDES4LwY3GQ3iqlmbdka+fFj7ufPJbY29uPGTu3atYdMSyWesoxisrKyjh49mp+fz+PxXFxcZsyYYW2tfAfOSDWKqa+v79u3L8lPQSYnmUWMOlCWiPlxMHevBLhGCTcVv9Ia4AMns0qA82Jwg4gBUkAvRgL0YnCDuXtJikqlwjiCPOC8GNwgYkiK2Kt1UlJSLly4gM3AALDzYs6dO0d0FUoJdpSAFDweT3yhI8B6MWS+hIrMIGKAFD4+PuvWrSO6ChLhcDghISFEV6GUYEcJSKGlpSWeOAJgvRjxzF7gu8AoBkiRkpKyevVqoqsgEejF4AYRA6SAXowE6MXgBhEDpIBejAToxeAmvRdDZ2pQOGQ5d5s8qFQNHX0pc8SpHujFSIBeDG7SRzH6xrTiPLbCiyG7ko/1OoZq0SCHXowE6MXgJj1iLB20GwRSLo9Uc9z6BktHbaKrUAToxUiAXgxu0iOGztRw8dO7GVuo8HrI6/75ElNrurG5Wuw+QC9GAvRicPvqsL+tnx5TV/NSTH777kYtTLUYumqxg/AlPkdYVsjJflHt5KHj1pmkt+CQOejFSIBeDG7S54sR+/yJ++I2qzS/vpYlaGIxuWpoEGpqEnbky9BES68F1b2boY2zWuwiYeAaJQkikaisrAz2lXBoZmxiYkXvO5bg24B079792rVrZL4DhuqBXowE6MXgBufFACmgFyMBejG4qWmHBTQNejESoBeDG4xigBTPnj1bubL5e12pDzgvBjeIGCAFn89nsVhEV0Ei0IvBDSIGSOHr67thwwaiqyAR6MXgBr0YIAWNRqPRaERXQSLQi8ENRjFACujFSIBeDG4QMUAK6MVIgF4MbhAxQAroxUiAXgxu0IsBUkAvRgL0YnCDUQyQAnoxEqAXgxtEDJACejESoBeDG0QMkAJ6MRKgF4Mb9GKAFNCLkQC9GNxgFAOkgF6MBOjF4AYRA6SAXowE6MXgBhEDpIBejAToxeAGvRggBfRiJEAvBjcYxQApoBcjAXoxuEHEACmgFyMBejG4QcQAKaAXI4HD4QwZMoToKpQS9GKAFNCLkSASicrLy4muQinBKAZIAb0YCQwG48KFC0RXoZRgFAOkIGEvRiAQCASE3S8QIcRkMjkcDoEFMBgMAt8dN4gYIIWvr6+bmxvRVfwHh8Mh8DdcJBJVVlYaGRkRVQBCiEqlUqnK9wurfBUDBYBezJeEQiHRJSgl6MUAKaAXI4FCoRA7hFFeEDFAChL2YginoQG/LHjAjhKQgoS9GGKRoRejpCCYgRQ0Gk1fX5/oKshF5r2YDRs2XL9+XbbbJCGIGCAF9GIkyKMX8+7dO9lukJxgRwlIQf5eTE5OzqxZsyIjI3fu3GloaLh//36BQHD8+PGnT5+Wlpa6ubkFBQV16NABW7iysnLr1q2ZmZnW1tYDBw4sKCh4+PDhoUOHsNNtpK6Vm5s7c+bMXbt2xcfHP3z4sGXLlj169Jg8ebKmpiZCqKKiIiYmJjMzk8vl+vj4jB492srKCiGUmJgYHx8/d+7c9evXDxo0aNasWbm5uZcvX05NTS0pKbGxsQkMDBw4cCBCKDAwECG0Y8eOmJiYhIQEhNCNGzeuXLmSm5trZ2fXo0ePIUOGUCgUor/NMgCjGCAF+a9Rwo6px8XFhYaGhoWFIYT2799//vz5oKCg48eP//TTT+vXr7937x628I4dO/Lz86OiotauXZucnJycnCzu3X5tLWz7u3bt6tmz58WLF5csWZKQkHD37l2EUENDw9KlS1+9ejV37tzo6GhDQ8OwsLDCwkKEkJaWVn19/eXLlxcvXhwUFIQQOnjwYEpKypw5c9atWxcYGLhv376nT58ihLBzhRcsWIDly61bt7Zv3+7k5HT06NGJEyeeP3/+wIEDhH6DZQYiBkhB/l4M9hfe29s7ODi4TZs2XC735s2bw4cPHzBggL6+fr9+/Xr27BkXF4cQYrFYT58+DQkJcXFxMTIymj9/fklJCbaRJtbC/PTTT927d6fRaO7u7qamptiuTUZGRn5+/pIlS/z8/IyMjKZNm6avr5+YmIhVxeFwhg0b5u/vb2lpiRBavnz5b7/95unp6eHhMXDgwNatWz979uzLj3Pt2rV27dr98ssvLVq08PT0HDdu3MWLFysrKxX4HZUXJYgYkUhEdAlqR1l6Ma1bt8a+ePfuHY/H8/HxEb/Uvn37nJyc6urqnJwchJD4AJmOjo6Xl1eza2EPnZycsC8oFIq+vn5tbS0WMTQazdPTU/xS+/bt09LSxBtxdnYWfy0SiS5cuDB16tTAwMDAwMC3b99WVVVJfAqhUJiZmenr6yt+xtPTUygUpqeny+j7RCQl6MV07969vLycyWQSXYgaEYlEFhYWRFfRPC0tLeyLuro6hNCiRYskFqisrKypqcGuMBI/qaen1+xa2Kn6Us+Fqa2t5fP5WDNFzNDQ8MuqhELh6tWr+Xz+pEmTPDw8dHV1v3wvhBCPx+Pz+ceOHTt27Fjj578MI2WkBBGzYcOGQYMGHThwABt5AnkrKiry8/Pz8/MjupDvYGxsjBAKCwuTSEYTExNst4jP54ufFP/qNrGWxE6KSCRqaGjAvjYyMmIwGBEREY0XwNrAErKzs9+8eRMVFSUeN9XW1mJv2hiDwdDW1u7du3e3bt0aP29ubv493wOSUoKIQQhdvHhxyJAhu3fvtrGxIboWFXf58mUbGxul++G2sLCg0+kIIQ8PD+yZyspKkUjEZDKxYz15eXm2trbYyOXFixdmZmZNr/VlH0S8w+7g4MDhcExMTMTBVFRUZGBg8GVV2FG5li1bYg/z8vLEZUhwcHCora0Vl8Hn84uLi1Vjnj0l6MVgEhMT58+fn5ubS3QhqozH4z19+tTd3Z3oQr4bk8kcO3ZsbGxseno6j8e7d+/eihUr9u3bh+WIjY3NqVOnCgsL6+rq9uzZIw7QJtaSQKFQxFc5e3l5+fr67ty5s7S0lMViXbx4cd68eX/99deXa9na2lKp1LNnz9bU1OTn50dHR/v4+GDTjNPp9JYtW6akpLx8+VIgEEyaNOnRo0fXr1/HWjBRUVFLly7l8Xhy/rYpgnKMYjDnzp0LDQ3dtGmTo6Mj0bWooDdv3lhbW0uM/5XIsGHDHBwczpw5k5qaqqOj07ZtW+xgNnZseNeuXVOmTLG3tw8ICNDR0cnKymp2rSZERkZevnw5Kirq9evXVlZW/v7+gwcP/nIxU1PTJUuWxMbGDhs2zMLCYsmSJRUVFZGRkdOmTTt06NDIkSNPnjz57NmzEydOtGvXbu/evfHx8UeOHOFwOG3btl27di02wlJ2FKU7XjNy5MjIyMjGTXvw4y5fvqynp9e9e3eiC/mq2tpa3PPFsFgsLpdramqKPVy9ejWVSl29evW3b4EM1ygZGhoq43wxSrOjJPbHH3+sXbtW/FcIyMTbt2/JnC8/6LfffluyZMmDBw9YLNbp06dfvHgxYMCA790IzBeDj/KNYjBjx45dvnw5XA384x49etS5c2eiq2jej4xiqqursRN8y8rKrK2tR48ejeMjC4VCYudzUNJRjLJGDEJowoQJ4eHhytibJI+rV682NDRgV82Q3I9EjGpQ0ohRvh0lsePHj2/fvj01NZXoQpRYTU2NUuQL4UQiUUVFBdFVKCUljhiE0NGjR/fu3ZuSkkJ0IconKSkJITR8+HCiC1Ea0IvBR7kjBiF0+PDhmJiY5ORkogtRJnfu3Gl8titoFszdi5vy7dp96eDBg7NmzRIKhR07diS6FuUgEolCQkKIruL7MJlMNb8pgtRrFMhPidu9EubMmTNmzJguXboQXQipHTp0aNq0aURXoXw4HM7IkSOxGRvAd1H6HSWxffv2xcfHi2chAl9KSUlR0jsKEg7uaY2b6oxiMAsWLBgyZEiPHj2ILoR0amtrc3Nz27VrR3QhSokMZ/cqKdUZxWB27NiRlJT0zz//EF0IuezYsUNDQwPyBTdo9+KmahGDENq2bdu1a9ekXvmqnrKzs01NTWFOrx/B4XCGDBlCdBVKSQUjBiG0efPmf/75Rx1uUtOsvLw8bW3tMWPGEF2IcoNeDG6qGTEIoaioqLt37165coXoQoi0ceNGBoMBswX+OAaDgd0zAHwvlY0YbEbOx48fX7p0iehCiFFeXu7k5IRN7wZ+EPRicFPliMGmDnr27Jka/v159uyZlpZWaGgo0YWoCA6HM2jQIKKrUEoqHjEIobVr17569ercuXNEF6I4kZGRRkZG4nn2wY8TiUSqcT8AxVP9iEEIrVq1Kisr6+zZs0QXoiCenp4ODg5EV6FStLW1L168SHQVSknVTr1rwsaNGx0cHFT72uKkpCTsPqcAkIRajGIwy5Yty8vLa3w7URWzbds2mNJYTqAXg5saRQxCaPHixcXFxadOnSK6ELno2rWri4sL0VWoJujF4KZGO0piO3fuNDIyGj9+PNGFyMzBgwdnzJhBdBUqrqqqqvFdZcE3Uq9RDGb+/PksFuvo0aNEFyIbv//+e6dOnYiuQvVBvuCjjhGDEJo7dy6bzT5y5EjjJ2fOnElcRfh16tRJfKNSICfQi8FNTSMGm8KKx+PFxMRgD729vYuKigoLC4mu6zusWrUKIeTq6kp0IaoPejG4qW/EIIRmzZqFEIqOjvb19dXQ0CgpKXny5AnRRX2rCxcuDB06lOgq1AWcF4ObOrZ7Jfj6+mJfiESijh077t+/n+iKmsflcktLS62trYkuBIBmqPUopnG+YJe6FRQUfPz4kdCKmjd9+nQqlQr5okjQi8FNrSOme/fuEjfHKSoqevToEXEVNe/hw4czZ85U0snolRf0YnDTXLt2LdE1EKZly5YUCkUgEHC5XD6fT6FQhEJhfX09ac/Bz83NtbS0hOuPFI9GowUHB8Pk6jgouhfDZQvJ1vwpLy9/8eLFrVu33r9/X1FRYWhouGXLFltbW6LrkjRnzpytW7dqa2sTXchXUbU0qGp9pyMgheIi5l5iWfbLWiMzrc+fSHrzc5EICYUNDQ0NWlpaRNciSSgUIYQ0NChEF9IUvRY0Pk/YtoOBT4CqnaXG4XCGDRsGB5VwUMTdIBsEopgVH7qHtPrZ21DHQBXuPwm+hlXG//S29vLvxQMmtyK6FlmCXgxuihjFHF75YdAMG6Y+hIu6eJtSXfi+dtA0C6ILkSW4RgkfuUfMk2sVdCbN0QNmYFMvz/8ut3BgtPbUIboQQDC5H7TOzawzaEm61gaQN21datEHNtFVyAycF4Ob3COGpqVp1Iou73cBZGNsQedyhN+woHKAXgxuco+Yko/1IiHJDlMD+RM2iGoqBERXITNwjRJuan12LwDfDnq9+EDEANA86MXgBhEDQPOgF4MbRAwAzYNeDG4QMQB8E+jF4AMRA0DzoBeDG0QMAM2DXgxuEDEANA96MbhBxADwTaAXgw9EDADNg14MbhAxADQPejG4qUvETJoyfOeujbhX3/DbyrlhU753rSHBvU+cPIwQ+vAh2z/ANy0tFd+7Dx4agG1HJmS7NTUBvRjc1CVilNqI4ePau3v9yBaGhvQpLCqQ1dbUE/Ri8IGZ6JTA6FETf2T14uKiqqpKWW1NPXE4nODg4CtXrhBdiPIh4ygm6WLC2HFDgob0+m3j6pKSYv8A37//uY4QSjj3R8iwfvcf3A7o02HPvq0IoUeP7m34beWIUQN+HtBt4aKZL1KfiTeSm/th5qxxPw/otvzX+a9fpzfefkbGqyVLfwka7D9uQvD+6B11dXXfUhWNSktNTRk24uc+/TrNmj0+8/+3WVtbe/TYgVlzJvw8oNvYcUP2R+/gcJqa/3z5r/OX/zpf/PD69Uv+Ab5s9r+zNz16dG/k6IEBfTrMmDn26rUk7Enxrk1Oznv/AN/XWRmrVof7B/gOH9k/+sDOhoYGbLFz5+OXLP1lUFDPkGH9ItctLyj8hBB6kfps1JhBCKExYwevXL1IYkfp48fchYtmDgzqMXhoQNiCaeJvYETkssh1yx8+vBs0pFeffp3CFkyT+B6qG5FIVFNTQ3QVSol0EfM6K2PHzqgePXqfPH6uZ/fekeuXI4Q0NDQQQlpaWmx2XVLS2eXLIocOHs7hcDZEreRyucuWRvy2YaeNjd2vKxdUVJQjhPh8/tLlc01MzI79fnbGtHl/xJ8oLy/Dtv+pID98yWwOl7N3z9F1EVs/fHi3YOF0gaD5mU1KSouTLp5dsXzdxqjdPD5vy9ZIbE7Sc+f/iDt9bMTwcb9t2DljRtjtO38dPxGD77M/enRv1ZrwKZPnbIza3a2b/+YtkTf/vtZ4ARqNhhDatn19QEDgjWuPfl2+/syfp27d/gshlJaWumfvFjc3j8jIrcuWRlRWVmz4bSVCyMvTN2rDToRQ7KkL6yO3Nd5aZWXFL3MnmZq2ijkYt2/P0RaGRuvWr8DCjkqlZmS++uvmlQPRJ69evk/XokdtWoPvQ6kGbW3tq1evEl2FUiLdjtKNG5eMjIwnTZxJpVK7dOn+9t3rzMw07CUKhcLhcEaOnODt5Yc9czjmD21tbQMDQ4RQW5d2F5LOpqWn9ugecPfeP6WlJbt2HDYza4UQmjd3ybARP2Or3Lx5lUalrYvYiq0VvmjVqDGD7j+43bNH76YL+/y55ED0ST1dPYRQ8NCRW7etr65mGRgYDh82tkf3AFtbe2yx9PSXT5Mfzpg+D8dnP3rsQPefevXp/TNCyM+3U11dLZstZYTVo3tvrFoPD28Lc8u3b1/3Dgh0dXU/euSMlZUNlUpFCAn4/BUrF7CqWQb6Bl97uz/PxmrR6eGLVmKrLA5fHTq834WkP0eNnIAQqmezF4evZjKZCKGAXoEbN69ls9nYQ/Wkq6tLdAlKiXQR8yEnu23bdtgPPUKo+08Bx08caryASxs38ddsdt3hI3tTX6aIBylY06GgIJ/BYLRqZY49aWzc0tTUDPs6I+Oli4sbli8IoVatzC0srF6lvWg2YhwdnbF8QQgZ6Bti++cGBohGoyU/e7Rx05rs92+x0VCLFkY4PrhQKHz/4V3v3j+Ln5k5I0zqks7ObcVf6+rq1dbWIIQ0NTULCz/t27/tdVa6eNevqrKiiYj5kJPdurWL+Futo6NjbWX79u1r7KG1jZ04UHR19RBCNTXVahsx0IvBjXQRU1tbY2r6v1vwiLNATHwftZKS4rAFU729Oqz69TdXV3cKhdKnXyfspepqlrb2f34Z6HSGePtZbzL9A3wbv1pZUd5sYeJfRWw8Jf465tCeK1cSZ8wI8/PtbGbW6vCRfVeuXvieT/wvDocjFArFdTYB222U8ODBnZWrF40ZPWnG9DBHx9bPUp4sWfpL09upKC+ztLRu/AxDW5tdz27iXdQW9GJwI13E0OkMAZ8vflheUfa1JW/f+YvH4y1bGoHdg7XxQRN9fYP6+v9Mfy/e4zAybunu7jlp4szGr2KjEhxEItHFSwmhIaMHDhiKPYONKb5dg/DfZi2dTtfQ0Kirq8VXyaUr593dPadOmfPtZTB1dDjc/3Sm69lsK0sbfAWoNujF4Ea6v1SWltY5ue/FDx88uP21JaurWXp6+uJ7PN+5+7f4pVZm5hwO58OHbOxhdvbbsrLP2NeODq1LS4s92nt7efpi/1oYGtnY2OGrls/n19fXt2xpij3k8XgPH91tehUtmlbjDkt+fh72haamZps2rmnp/zs979Dhvfv2b//GSqqrWSb/XwZC6N69f5pdpY2z6+vX6fz/D/Tqmuq8jzn29o7f+I7qBnox+JAuYrp26ZGXlxN3+phIJEp+9riJM2IdHFqXl5clXUwQCARPnj58/vypgYFhaWkxQqhLlx5aWlpbt6/ncDhlZZ8j1y/X//+WRGjoGKFQuHf/Ng6Hk5+fdzBm9+SpIz7kZOOrVktLy8bG7uq1pILCTyxW1eatke7tPGtqqps4EN62bbusrAws/p6lPLnfKEMHDwpNTn4Uf+bki9RnF5LOnv7j+Lf/wjs5Oic/e/wi9ZlAIPjzbCz2ZHFJEdZVQQjdvv1X5n8PPA8aFFJXV7tt+4aSkuLc3A9RG1cz6Iz+Pw/B961QbRwOp3///kRXoZRIFzHdf+o1dMjw4ydihob0OZ8YP3XqL+KDtRICevUbN3bKiZOH+vTrlJAQN2/ukj69+8edPrZ9x2+6urq/bdjZIBAMDOoxcXJoaMho8REffT39I4fjtRnaM2aNHT8xJPVlyuLwVc6tXXAXvOrX3xh0xsRJoWPHD/Hx7jB16i8MOmNoSO+i4kKpyw8ZPDygV+D0mWP8A3yvXr0wdvRkbIcLIdSv38AZ0+edPHV44aKZJ08dnj5tbv+fB39jGZMnz+7YocvKVQv7BnYuKSletjTCpY3rsuXzbv59zdLCKrDfoKPHDhw6tKfxKlaW1mtWb8zJyR45euD8hdMRQrt2HtbRgfs3SgG9GNzkfsPZ6CXvRy1x0KRRvmFZhBASCAS5uR+cnJyxh6+zMmbPmXDoYJz4GaAUinPr0+5VBP9iSXQhMlNbWwv7SjiQbhSTlp46bcboXbs3FRcXZWam7dq10c2tvaNja6LrAuoO8gUf0h1R8vL0XbTw16vXkiZPHa6rq+fr02nmzPmNDxLLyfJf56d/pe/Tv/+QWTPnS30JqAk4LwY30kUMQmjggKHiY8AKE75wJY/Pk/oSU1tNzzcDYtCLwY2MEUMIY+OWRJcAyAvOi8GNdL0YAMgJejH4QMQA0Dw4LwY3iBgAmge9GNwgYgBoHvRicIOIAeCbQC8GH4gYAJoHvRjcIGIAaB70YnCDiAGgedCLwQ0iBoBvAr0YfOQeMWa2DIqG3K8wAmSjoUnRM5YyBYeSgl4MbnKPGAFPWFHMlfe7ALIpL+TQGarzpwV6MbjJPWJsXXSqy6VfXghUGKeuwdxOm+gqZAZ6MbjJPWI6/myUfKOsrqr5e6EBlfEmmcUq5xXXvigqKiK6FpmBXgw+imj3Tl5rl7gv79ObujoWBI2KY5XxMx5Wfv5UP2iq+cePH6dPn/7x40eE0NOnT7lcJd5fhl4MbnKfWFPs/oWyd6m1RmZanwuauuWzKhE2CBGFoqE23W5dQ5qAL2zrp+8T0EL8JJ/Pp9Foa9asuXnz5u3btykUyps3b9zc3JrcEunU19f37dv33r17RBeifBQXMRgOW6jgdyTQ9u3bXVxc1OevH42mSdVqagGRSCQQCKZOncrhcOLj41ksFpvNNjc3V1yJPwDm7sVH0RGjVq5du2ZhYdG+fXuiCyGduro6HR2dkpKSqVOnent7R0RElJeXGxgYNL7lJlANEDGAYCUlJWZmZi9fvpwxY8a8efNGjx5dVlbWsiW5JiGEuXtxg7N75ejZs2cfPnwgugqyMzMzQwh5eHg8fvy4a9euCKEnT574+/tjjY/q6mqiC0RwXsyPgIiRo+vXr7969YroKpSJra0tQmjAgAFJSUk2NjYIoWPHjoWGhmZnZyOEGhoaiCpMW1v7xo0bRL27UoMdJTmCXoxM5Obm0mg0S0vLGTNmIIQ2bdpkaGhIdFHgW0HEAGWSkpJiZ2dnbGwcGhraunXr9evXa2hoKOA2WxwOZ/DgwdevX5f3G6ke2FGSI6pcpgcAACAASURBVOjFyJyPj4+xsTFC6Pjx47169UIIcbnc4cOHx8TEiG8NLg8ikYjNZstp46oNIkaOoBcjPzo6On369NHU1GQwGBs3bsR6xtnZ2TNmzJBH0wR6Mbhprl27lugaVBabzbaxscF++oH8tGjRwsXFBSFkbGxsYWFRUVHh4uJy9+7dmJiYli1btmrVSibvQqOpztwUigS9GKCa+Hz+rVu3BAJB//79ExMTP3z4MHLkSAsLC3xbg14MbnAypRw9e/bMyMjIwcGB6ELUEY1G69u3L/a1v78/m83OysqysLCIjY0VCAQhISHfdTUA9GJwg16MHEEvhiQMDAxGjx6NtYe7du1aXV395s0bhNChQ4cuXbr0LafbQC8GN+jFyBH0YkjI0NCwY8eO2B5TQ0PDnTt3HB0dDQ0NDx48yOPxrK2tv7Yi9GLwgV4MAAghdP78+du3b2/ZskUkEsXHx3ft2tXR0VH8KvRicIMdJTmC82KUyNChQ3ft2qWlpUWlUlks1u7duxFCRUVFly5dqq6uhl4MbjCKkaMNGza4ubkNGTKE6EIATjU1Ndu2beNyuVFRUenp6TU1NZ06dVLAycSqBCJGjuAaJVWSl5e3detWS0vLZcuWpaena2lpOTs7E12UEoCIAaB5Er2YlJSUbdu2BQcHh4aGvnz50tLSkmwT3JAH9GLkCHoxKkOiF+Pj4xMXFzdw4ECEUE5OztixY589e4YQyszMJLRMMoKIkSM4L0ZlSD0vhsFgIISGDBly7dq1tm3bYv/jfn5+2K1d4K8LBnaU5Ah6MeqJzWYzmczZs2dnZ2dfuXKFQqGwWCwjIyOi6yIGRAwAzcN3XkxFRYWhoaFAIBg4cGDbtm137dqFzYsutzLJCHaU5Ojp06fv378nugogA/jOizEyMtLQ0NDS0rpx48aCBQsQQlVVVX5+fnv37sXumiKfYslFWUcxQqGQz+cTXUUzYmNjbW1tu3XrRnQhzaDT6USXoATq6+u1tWVzl+7MzExXV9enT5+uXLly7ty5gwYN4nA4WGdH9ShrxNTW1nI4ZL+rJJfL1dDQIP+1LYaGhnADI0JUVFQUFha2a9cuPj4+KSkpPDzcy8uL6KJkDCIGQMQ0TwHXKL1580YgELi5uUVERHz+/HnZsmVWVlbyezuFgR8sOeLxeBoaGvDbqwIUcI1SmzZtsC/WrFnz+PFjLpeLEAoPD9fR0Vm8eLHy3usWRjFyVFNTQ6PRyL+PDaOYbyHDXsy3Y7FY9+/f9/PzMzU1nTNnjouLy5w5czQ0lOkojTLVqnS0tLQ0NTWJrgLIhuLzBZtMa8CAAaampgihhQsXGhgYCIXC+vr6sLCwpKQkxdeDg4pHTEFBQWBgYEpKiky2lpiY2L9//29fnk6nk7/XC74Fh8Pp168fsTU4OjqOHz+eSqVqa2uPGDHi8+fPCKH3799HRETI6idcHlQ8YojF4/EEAgHRVQAZINt8MV26dJkyZQpCyM7Oztvb++XLl9h5WHv27MnLyyO6uv+APXA54nK5NBoN2hwqgLRz92pqag4aNAj72tXVNSsr68GDB7a2tjdu3CgrKxswYICBgQGxFarOT39NTc3hw4evX79uYGDg5eU1efJkbA8Ws2vXrqtXrxoZGXXr1m327NnYkxUVFTExMZmZmVwu18fHZ/To0eLDhPn5+bt27UpPTzc3N+/atev48eO1tLQav11DQ8PKlStLSkp27typr68vtSQtLS0NDY0LFy48ffo0KytLS0vL3d194sSJ2MSxGzZsoFAovXr12rZtW319vYuLy9SpU7H7AdXW1p44cSI5ObmystLZ2blXr16BgYFRUVFVVVWbNm3CNj5t2jQWi3XmzBnsYVRUFJvNXrdu3dc+VE5OzqxZsyIjI3fu3GloaLh//375/D+oLEJ6Md9FV1d3/Pjx2Neurq5nzpy5d+/ewIEDL1y4wGAwAgICCPlrpyI7SgKBYNWqVeXl5Zs2bZo1a9bnz59XrVol3kk5efKku7v7pk2bQkJCkpKS7ty5g2XE0qVLX716NXfu3OjoaENDw7CwsMLCQoRQSUnJggUL3NzcNm7cGBoaeuvWrS9/IXfs2PHu3bsNGzZ8LV+wXsybN2+io6NdXV1Xr14dHh5eVVW1efNm7FUqlfr69eu///579+7diYmJdDp969at2Evbt29//fr1L7/8cujQIRcXlz179mRmZnp5eb158wabLr+ysrK0tBRrNmGrZGRkeHt7N/GhsK5QXFxcaGhoWFiYfP4fVBYZejHfxcrKauHChdh0E7a2tnfv3n3y5Ak2RTE274TCqMgoBhsmHDp0CJtB3srKKiEhobKyEnvVw8MDu8GFh4fHhQsX0tPTe/TokZGRkZ+fv3HjRk9PT2xQ8OjRo8TExNmzZ58/f55Op48fP15TU9PT05NGo717967x28XFxd25c2fjxo3m5uZNVMXj8Vq3bn3w4EFLS0vsD4hAIFizZk11dTUWTPX19QsWLGAymQihnj17btu2DbtINy0tLTQ01MfHByE0efLkn376SV9fv2XLlhwOJycnx8nJKS0tzd7eXkdHJy0tzdLSsqSkpKyszMvLq4kPhU0H6e3tHRwcLOf/DRVEtl7Md/H09MR+HrA/e4cPH27RooWjo2NiYqKnp6ednZ1c311FIiYnJ0dbW1t8hwonJ6elS5eK/8i7ubmJl9TX18dOasrIyKDRaOJvPYVCad++fVpaGrY1Jycn8fHmvn37Yjf9olAoFArl1q1bJ06cWLFiRePNSsXn86lUalFR0cGDB7OyssQ/o1VVVVjEWFtbY/mCjXKxXSQmk+nm5nbu3Lnq6mp3d3cfH5/WrVtjy5ibm2dkZDg5OaWnp7u6ujIYjMzMzMDAwLS0NCMjIzs7u9OnT3/tQ2HEmwLfhc/nk7MX87369+8vPipaXFwcHh4eHx8v11MrVCRi6urqmjjDTep3sLa2ls/nBwYGNn7S0NAQ25rUJplIJGpoaMB2Z77lhDodHZ379++vX79+xIgRU6ZMcXBweP78+a+//ipe4GvnUC1atOjy5cu3b99OSEjQ0dEJCgoaM2YMlUr19PTMzMwcPHhwWlrauHHj6HT6vn37EELp6elYrDTxoTASHSXwLX799dfp06fb2toSXYiMzZw5c+bMmfJ+FxWJGCaTWV9fLxQKv/3ERyMjIwaDERER0fhJLIx0dHSaGBXPmzcvPT1927ZtBw4caNGiRdPvcuPGDVdX1wkTJmCF1dXVfUttenp6I0eOHDFiREZGxsOHD0+fPq2rqxsSEuLl5XXkyBEWi5WTk+Pp6ampqVlUVMRisTIyMoYPH970hwL4nDp1qkePHqqXL5jCwsJDhw6tWbNGfm+hIu1eZ2dnDocj7pjk5+cvXry46ZkNHRwcOByOiYmJx/8zNTXF7j/t7OycmZkp7hbfvn17+fLlWJ9VQ0OjX79+s2fP1tbWFjdum1BTU2NiYlJdXY1NPXH//v1mV6murr5w4QKHw6FQKO3atZs+fbqHh0d2djbWSyopKbl9+7aDgwOTyaTT6c7Ozrdu3crPz/f29m76Q4HvwuPxNmzYgBAaO3as+N7Yqqe+vl7e8w2rSMR4e3tbWFgcOXLkwYMHKSkpe/fuLSsrs7GxaWIVLy8vX1/fnTt3lpaWslisixcvzps376+//kIIBQYG8vn83bt3P3/+/MGDB7///ruxsXHjsYC2tvbKlStfvXqVkJDQdGHYzlFeXp5QKBQvXFJS0sQqVCo1NjZ2w4YNGRkZFRUVN2/ezM7Oxvo+BgYGTk5OiYmJrq6u2MKurq4XLlywt7c3NjZu+kOB7zJ79uw+ffoQXYXcWVpaSox5ZU5FdpSoVGpUVNSWLVvWrVuHEOrYsWNkZGSzZwFERkZevnw5Kirq9evXVlZW/v7+gwcPxr7v69at27lz540bN+h0eu/evSdNmiSxrpOT05gxY44ePert7W1vb/+1t5gwYQKbzV67di2Hwxk4cODChQuLi4tXrVqFdaOlYjKZq1atio6OXrRoEXb65rRp08R/SD09Pc+ePduuXTvsYdu2bc+fP9/4bnBf+1DgW9TU1Ny6dSsoKOjw4cNE16IIDAYDOxVLfuBKa4WqqKgwMDAgW3MErrTG1NXVDRw48MSJE+JDkyqvsLDw4MGDch3IwA+WQhkZGSlppqu28vJyNputq6t769YtomtRqPr6+qysLLm+BYxiftTq1aszMjKkvhQYGDht2rQvn2ez2dra2uS5NbKaj2IyMjIWLVp0/vx58l8iIHMcDic3N1eu+0oQMT+qvLz8axOVa2trf+0itMrKSkNDQ5KkjNpGTElJiZmZ2dOnTzt06EB0LSoLIgaoacScP3/+5s2b2LmLaksBvRgVOWitpKqrq4kuQR1hF69xuVw1zxfoxTSFz+dj58IpNYFAcOzYsalTpxJbBvlnF5ahAwcOGBsbDxs2jOhCSAF6MQDIDI/HKy4uvnHjBuGZrlZgR4l4VVVVMIGLvO3evbusrMzc3BzypbHCwkK5XqAEEUMKhoaGa9eu3bFjB9GFqKyTJ08aGhpaWFjAbO0SoBcDwA85fvz4hAkT6urqdHR0iK6FjBTQi4FRDIm8fftWPK8w+HETJkzApiWEfPkauEZJ7RQXFz969Gjo0KFEF6LcHj582KVLl5qaGj09PaJrITU4L0bttGrVKigoqL6+nuhClFVdXV337t2xm09AvjQLejFq6uHDh3/88cfu3buJLkTJVFVV1dTUGBsbi2dEBk2D82LUV3Fx8adPn3x9fYkuRDlkZ2ePHTv21q1bangpI8nBjhJJtWrVqk2bNkVFRUQXQnbYSd5v3rx58OAB5Mv3gvNi1Jqenl5ycnJkZCTRhZDXP//8M2PGDITQgAEDyDbRl1KAXgxAnz9/rq+vb3oeYjXE5/NpNNq2bduw6UcBPtCLAQgh9OnTJy6X6+joSHQhZHHu3DmhUBgaGkp0IaB5sKOkBKysrO7cuQM3usfulvf+/fusrCzIF5lQQC8GRjFKo6amhkKhYPelVU9nzpzp3bu3lpaWOn8TZOv9+/crVqyIj4+X31vAKEZp6OnpZWdn/1979x3Q1NX+AfxkEWaCIFsEByBLNqK17oEoddZZrQMt9le11rbWga1WaxVHHRXUDqWttW7Ft8WB4kYEBQEXyt6yEhLIzu+P65vyKiLIvTnJ5fn8Bbk3uV8Untzz5N5zqG7O6axDhw4VFBRYWFhAfSGRFtZRghKjT3x9fePj448cOdL0wdWrV+NLpA3EevUDBw784osvcGehGy3cowQlRs988cUXxPLVBH9/f6oXDMVr4cKFIpGIeL/FnYWG4LoY0Ly///571KhR/v7+TCazrq4uJSUFdyLyPXz4ECG0bNmyCRMm4M5CW1q4LgZKjF7at2/f8+fPmUwmMdk1zRYYa2homDRpEvFBRM+ePXHHoTMt9GLgEyX9ExYWVllZqflWrVa7uLgcPnwYayjSSKXSp0+fmpqaOjk54c4CSABnMXpm9uzZAoFApVJpHmEwGNXV1VlZWVhzkaC0tHTcuHEMBsPT0xPqi3ZALwa87MCBA0uXLg0ODu7atSuLxSJqTU1NzfXr13FHa6+EhIQff/zRwMAAd5AOBO5RAq+Vk5Nz+/btixcv1tTUlJaWenh4xMXF4Q71NtLT048ePbphwwbcQToiuEdJLxU9bky/VicWKATPZVo4nFqtVqnUarVKTxeNVSgU7U9uac/lGDB7BZq5BcJMd7oFSgzJsm8Jc9JFrgF8SztDDhfGoVqiVKqrSyUlOWJjM1bf0Ra44+iNkpKS2NjYb7/9lrpD6OX7ns66c6G2qkQ2dLo97iAdDgcxHHoaO/Q0Tj1Xdfno88HvW+FOpB8kEsmTJ08oPQScxZCmukx2O6Hm3Qm2uIN0dLf/fu4WYOroCjPgvZlEIikqKnJxcaHuEHAmT5rS3EYOFyZew8+Yxy560oA7hX4wNDSktL5AiSGTWKi0doR3TvysuhhKxErcKfRDSUlJVFQUpYeAEkOahnqFQqFqxY6AWiqVWlAtx51CP2ihFwMlBoCOy8HBYf369ZQeAkoMAB0X9GIAABSCXgwAgELQiwEAUAh6MQAACmmhFwM3EACgr8RisVzero/nGxsbc3NzPT0925mEx+MRczC+CkoMAPpKoVAoFIr2vAKDwbC2tm7ni7QMBkoAdFwsFsvMjNrpL6DEANBxMRgMqqcZghIDQMelVCrr6+spPQSUGADo4+bNmx9//HFoaOiDBw8mT5586NChlvc/ffr0lClTWt5nxowZv/7661tHghJDNz/s+H7OvMmt2BHQ0NGjRxFCmzZtcnJymjhxopeXV8v7u7u7T506ldJI8IkSAPTR0NDg7e3t4+ODEHrj6QlRYtzd3SmNBGcxANCBQqEIDQ0tKCg4e/bsSwOl/Pz80NDQx48fr1u3LjQ09IMPPti/f79SqUQInTx5MiwsjHiFoqKiDRs2TJ06dcqUKd98803Tlbk4HM7p06fDw8MnTJgQFRUlFApbHwxKDE61tTVfLv9kdPiAhR/PSjgX/9PPP344ZxJCKDf36eChgcnJ1ydNDo1YMA0hlJf3bMfOTR/OmTRyVL+PIj84feaY5kUaGhpWRX0WNubd/1s05/z5/zR9fYVCsXffzjnzJo8OH7B8xeLk5FattXTr1rUN362eMm30qNH9P1sWeS89lXj85KkjEyaNKCzMnzNv8uChgfPmT004F09sUqvVx44fmr9gemjYOx9FfrD/p91KpfJM/PGRo/pprrnYtv27wUMD8/KeEd+eiT8+anR/4sqO14UcO37o8eN/Llk6n/hnAS1gs9kJCQlOTk5jxoxJSEjw8PDQbOJwOAihHTt2DBo0KD4+fvny5cePH7969SrxH0fsI5PJvvzySxaLtX79+o0bN7LZ7G+++UYikRBbr1271tDQsH79+qVLl2ZnZ7dpOR0YKOG0ecu6wqL86M17bKxtd/+4pbi4kLhEkvidiPv9pymTZ3p5+SKEftyztby89LPPVjEYjMLC/B07N9nY2IX0eQchtGXrt8XFhVuiY2xt7I4e+yP59nUjI2Pi9Xfu2vxPwplFn3wxcOCwGzeSvl775coV3w4cMLSFSBKJZMPG1f5+wV8tX4sQunLl4qrVS3+PO2VhYcnhcESi+p27Nn+xLMrd3eu333/eHL3OzzfIxsb2xInDv//xy8KPPu3T553rN5J++vlHY2OTQYOGy2SynJxH7u5eCKHMrHQbG9vsB/e7deuBEMrKzggMCGGz2du2f/e6kBwO5+zfJ/39g4cMGqGt/xPaevfddwcMGIAQ8vb2trOzy8nJGTx4sOaS3OLi4tra2nHjxhGLiK9cuTIzM5M400EIGRsbT5s2jfg6OTm5TUuPwlkMNgJBXXLy9cnvz/Rw97K07Lzss9Xl5aXEJgaDgRAKCgx5f9IM916eCKGoqI3R0Xv8/YL8fAPHvjfJzdU95c5NhFBV1fPLSRemTf3Qw93LwsLyowWLuVxD4kWkUum582enT5v9XvhEPo8fNmrs0CGhcb/tbzmVoaHhT/sOL/tslZ9voJ9vYORHnzY2NmZmpRNb5XL5h7MWeHh4MxiMkSPGqNXqp08fI4Qy7t91c/MYOXKMuXmnMaPH/7j7QJ/gdxzsuxA1hThfKyjIGzF89P3Me8RLZWWm+/sHtxySwWDwePxF//e5p2dvyv4fOgqidhBMTExEIlHTrQ4ODubm5lu3bj18+HB2djaTyfTx8TExMSG2Nr3DgMfjyWRtWCAMSgw2z3JzEEJeXj7Et6ampv7+wU13cHVp0odTq0+cODxr9sTBQwMHDw189PhBXW0NQqisrAQh5OTUXbOjm9uLM+QnTx7KZLKgwL6aTb4+Abm5TwVCQcvBGhrEu3ZHT5ocOnho4KjR/RFCdXW1mq29er34bTMz4yGERKJ64qdIS7u9OXpdwrl4gVDgYN+lZ09XhFCAf5+srAyE0P3Mey493fz8gh5k30cIPX9eWVZeGhjQ540h3Vw9XgkI3kaz9xBpBkpcLjc6Ojo4OPjkyZPLli2bM2dOYmKiZjcW699574n3v9aDgRI29fVChJCJianmER6P33QHAy6X+EKlUn21colcLpsf8Ymvb6CZqdmiJfOITQJhHULI+L8jI4SQkeGLKcqJP37Nnhq1NdX8/z1QUxUV5UuWRvj7BUet+o44Wxk+MqTpDs3+hk2aON3Y2OTGzSubNq9ls9mDBg3/aP7izp2t/PyCdu2ORghlZKR5e/t5uHuXV5Q9f16ZnpFmbW3j6OhUXFzYckhY4ppSTdc4cnR0nD9//syZM9PT08+fPx8dHe3k5NT03OftQInBhhjRyJucc9bW1TS755OcR48eZW+J3hPw39MckajeqrM1QojPM0cISaQSzc4NDWLiC8vOVgihZZ+tcnBwbPpq1tYtrfSUdOWCTCb7avlaIyOjl85fWsBkMseMHj9m9Pj8/Ny7d1MOxO0Ti0Xfrd8eFNRXKBSUlZfez7w3a+Z8Lpfr5uaRmZWelZXu7xf81iEBWTSnNkVFRQ8ePBg5cqShoWFISEhQUNDYsWNzcnKgxOgxR0cnhFBe/jNn5+4IIZFIdPduio2N3at7CgR1CCGipiCE8vNz8/Nzuzn3QAjZ2tojhLKyMtxc3YleSWrabXPzTgihLg5duVwuQsjPN5B4Ym1tjVqtNjY2fvUQGkKhwMyMR9QXhNCVq4kt7Kxx7txZV1f3bt16ODt3d3buXi+q/8/fJxFCfB6/Zw/XmzeuPHuW49PbHyHk7eWbmXkv7W7KnNmRbx0SkE4oFG7fvr2wsDAsLEytVl+9elWhUDT9WOqtQS8GGwf7Lk5O3Q7G7SspLRaJRD/s2Ghn59Dsns5O3dls9l9HfhPWCwsL83ftjg4KDCmvKEMIWVlZe3n5HDgQW1RUIJVK129YpRnIGBsbz/7wo7jf9mdmpstksitXEz//8uMfdnzfcqru3V2qq6vOxB9XKBS3U27evZvC55tXVpa3/KzESwlrvvni5s2rAqEgOfn6teuXvDxf9Jj8/IJOnDzs7NydzzdHCHl5+ty+faOkpCgwoM9bhwRk0QyUPD09Fy9efOnSpXnz5kVERGRlZRGXCLf/ELDgLGkuHankWxm6+vNa/5QHDzK3bFufn5/bo7vL8OFhJSVFDx9m7dv7R3Fx4cwPJ2zetDso8EUfJOnKxYNx+/Lzcx0cHFet+La6pipqzedduzof/PVYaVnJDz9szMxKl8vloSPDzc07Xb+RdOCXo8QT76Qmnzh5+O7dFBMTU0+P3p9/HtVCI4bwy68xf/9zurq6KigwZPmX3xz+K+7Y8UPhYya4urpv3bbhwrlk4t7choaG0eEDvvrym5Ejx1RUlO/+ccv1G0kIIQsLyzGjx78/6QNTU1OEUHLy9RWrPn0vfOLST1cQZ2TjJgxz6em2b+8fmiO+LuT7U0aNHDEmYt7/tek/ojS34cGt2vEfN1+v6UQgELRzSiqFQlFfX9+pU6d2JrGwsHjdlFRQYkjzFiVGIKiTSCQ2Ni/6DitWfcpmsb9dt4WyjB0ClJjWU6vVSqWy/fM5tFBiYKCE09p1Xy39bMG165cFgrrffv85Le32e+/BZaxAe7QwXwy0e3H6+utN0VvW7f9p9/PnFU5du30d9b1mZESdQ38e+PPPA81ucnLuvnvnL1QHALpDqVQ2NDRQOvEdlBic+Dz++nVbtXzQiROmhYdPbHYTA7Xtqiqg79RqNaUT90KJ6Yi4XC73vxf1gQ4O5u4FAFAIejEAgNfi8drw8WWziouL9+/fv3bt2na+Tgs3LkGJAUBftfWOxFfJZLLHjx+3/3VaAAMlADquLl26fPfdd5QeAkoMAB0Xl8vt0aMHpYeAEgNAx1VcXLxy5UpKDwElBoCOSyqVPnv2jNJDQIkBoOOCXow+4RqxOBz498SPyWKYmsNHpa0CvRh9YmjMrCmX4k4BUF2FzIALv9itAr0YfWLdxVAhV+FOAZC0QWnjZIg7hX6AXow+cXQzkkuVT9LasFAeIF1ZbmNZXkOvQGrvu6ENLfRiYEoqkv39S7mFraFrII8D5+rapVSoCx+KH92pm7TYgcmCW8Z1BZQY8iX/pzrjWl0nWy7S1X9atUqtRojJxPN3KJcrOByS27EGRqzSZw1efXkDJliR+8r0VlxcvGfPHkpPZKDxTr6Q0ZYhoy3rnsslYiXuLM2LiopasGCBo6NjK/YlX3l5+cqVX/78888k3hrD5jI72zWzeANomRZ6MXAW0+FUVVWdOnUqIiICYwaVSqVUKgsKCtq/TA9oD6lUWlxcTOnn1lBiADY5OTnbtm2LiYnBHQRQCFqSHc5vv/320pLpuLi4uMydOzcjI0MikbRid0A+uC4GkCwjIyMpKYlY4UgXBAUF+fj4CASCvXv34s7SEUEvBpAsKyvL1NTU2dkZd5CX7d+/v0ePHkOGDMEdpGOBXgzoQCoqKmxsbNLS0gICAnBnAaSBgVIH8vjx4127duFO8Vo2NjYIoT/++OPSpUu4s3QU0IsBZPrrr79IWQidUtu2bWOxWLhTdBTQiwFkKikpcXDQm5We58+fv2jRot69e+MOQmda6MXAWUwHokf1hWgAnz59GncKmoP5YgBpVq1adeXKFdwp2iYqKgohdPjwYZlMhjsLPUEvBpBDoVDcvXt34MCBuIO8jXfffXfQoEFKpY7e8KXXoBcDwAtSqTQ/P9/NzQ13EFqBXgwgR0lJiVgsxp2iXbhcLpvNnjNnjkoFUwuSBnoxgAQCgeDDDz80MTHBHaS9evTo8dlnn2VkZDQ2NuLOQhPQiwEkyMzMXLRoEe4U5PD29vbz8xMKhTt37sSdhQ6gFwNA8+Li4qysrEaNGoU7iH6De5RAe4nF4itXroSFheEOQr7q6mpLS8vk5OSQkBDcWcBrwUCJSixOJQAAHD9JREFU5o4cOZKXl4c7BSUsLS0RQidPnvz7779xZ9FX0IsB7WVpaTl9+nTcKSi0adMmPp9PnPPjzqJ/oBcDQGstWbJkxowZwcHBuIPoE6lUWlZWRun8QXAWQ2eJiYlXr17FnUJLduzYkZiYiDuFnuFyuVTPTwYlhs527NjRoab4X7FiBULo0KFD+n6dodYUFxcvX76c0kPAOkq0JRaLo6Oj7e3tcQehllqtfmmwP2zYsOnTpx87dkz7884wmXr2nk3clkHpIaAXA/SbSqWqqal59XG1Wq1UKtls7b2JcjgcovGsR6AXA97ejBkzdGQxEywYDAaDwaitrcUdRKdBLwa8peTk5E6dOunOYiZYsFgsMzMzuVwOp+qvA70Y8JaCg4P79OmDOwV+xEBJpVKJxeIOXnCbBb0Y8JaIi+txp9CG1/ViXtLY2MhgMAwNDalLAr2YZsFAiYYuXbq0adMm3CmwaWhoiI6OHj9+/KpVq/Ly8kJDQ7OysoyMjAwMDBBCzc7ROXny5EOHDrXwmiUlJaGhoWlpaVQGxwB6MeBtZGdnv//++7hTYJOdnZ2YmDhr1qy5c+fy+fzp06dbWVlpPlGWSqWvLqE9ceJELy8vTHlxgl4MeBu0mR3m7RATVg0ePNjc3BwhNGvWrKZbzczMiBMZtVrNYDCIB6dMmYIpLGZa6MVAiaGbsrKy2tpaDw8P3EHw+PXXX//66y+E0NSpUwMCAiIiIhYuXLhlyxYvL68NGzYwGIwhQ4Zs3bq1sbHRxcVl7ty53t7exEBp3LhxxP2iKSkpx44de/LkSadOnTw9PefOnWthYaF5/R07dvzzzz8WFhb9+/f/+OOPsf6sJOjSpQvVY2oYKNHNhg0bhEIh7hTYzJkzh5id4PDhwxs2bGi6ic1mP3z4MDExcefOnadOnTI2Nt6+fftLT3/69OmaNWt8fX337dv38ccf5+bmbt26VbP1t99+8/b23rRp08SJE8+cOaN3i8a8CnoxoG0kEomHhwdM0fQ6jY2NS5cutbOzY7PZgwYNKi0tbWhoaDoTcHZ2tqGh4dSpU62trYOCgjZu3Dh58mTNVh8fnyFDhvj4+EycONHa2jorKwvTz0EaLfRioMTQiqGhIQ3O3qnj6OhobGxMfE1cJiMSibhcrubSDU9PT4lEsmbNmhMnTpSUlPD5fB8fH83TPT09NV/zeDwazFCjVqvlcjmlh4ASQytpaWmpqam4U+iuZm9TZDKZDMaLC8R69uz57bffWlpa/vLLL/PmzVuxYkV2drZmT+3fV0k1Pp+/evVqSg8BJYZW0tLS7t69izuFXmIwGCKRSKlUBgUFLV269ODBg8uWLRMKhV9//bVCocCdjio8Hq9pM5sKUGJoJSAgwN/fH3cKfWVqapqamnr79m1iQtLhw4dHRkaKRKKKigrc0ShRXFw8Y8YMqo8CH1rTSkBAAO4I+i0vL+/w4cMRERH9+/cvLy8/ffq0paWljY0NLavMxYsXR4wYQfVRoMTQSlpamlqtDgwMxB1EX02YMKGurm7v3r27d+82MDAYOHDg5s2btTnpjDbNnj1bC0eB2yBpZd++fQihBQsW4A6iPa28DbKtRCJRW+/M1q/bIKVS6fPnz7t06UL1gaAXQyvQiyGLqampSqWi8RtwTExMUlKSFg5EzzPADgt6MSRiMpl1dXUmJiYcDgd3FvLV1tbOmTNHCweCgRKtpKamqtXqoKAg3EG0h6KBkoZcLmexWK2Z91u/BkpaAwMlWrl79+69e/dwp6AVDofDYDCovgRWy+7evUv1DdYaUGJoJSgoCD5OIh2DwRCLxSqVCncQ0ixZssTGxkY7x4KBEtB79fX1WjhKWVmZnZ1dCztwOBxKJ+4kS2Fh4ePHj4cPH66dw0GJoZUO2IvRpvPnz3t7e7dcaMBLYKBEK9CLodSIESOio6P1fQbftWvXavNw8KE1rQQFBcFpKaW2bduGO0K7nD9/vtkJ0qkDAyUA2iwuLq5v374uLi64g7TZs2fPOnfurM0P16HE0Ar0YrTmhx9+GDZsWMdct6BNoMTQSge8Rwm03vnz53NzcyMjI7V5UGj30gpcF6Nlmzdv1qM5wI4fP679M1w4iwGgXY4ePern59ezZ0/cQd5MJpMRS2JqE5QYWoFeDC7Z2dlNJw/XQVKpVKVSGRkZafm4MFCiFbguBpdbt25lZmbiTtGS+fPn5+Xlaf+4UGJoBXoxuERERCQnJ+NO8VoVFRW2trZY1giFgRIAZLp582a/fv1wp9AhcBZDK6mpqXfu3MGdokO7cePG48ePcad4WUpKStNFL7UJSgytQC8Guy+++OLp06c6NfPDgwcPdu/erf1GLwEGSrRy7949tVoN0/dip1AokpKShg0bhjsIQghdvnzZzMwMV5MOboOkFT8/P9wRAEIIsdnsixcvuru7Ozg44M6CBg8ejPHoMFCiFejF6I7vv/++rKzspQfnz5+vnaMPGzZs9OjRCKHS0tITJ05o56DNgrMYWiEuZodL73REYGDguXPnnJycevXqhRAaOnSoqalpcXGxFlYvMjMzKyoq8vf3ZzKZ5ubmEyZMoPqIrwNnMbQC18XompEjRyYkJFy5cmXIkCECgaCysjI1NVULxzU3N1er1cTCCXV1df7+/n379sVSaOAshlagF6ODPv300xEjRgiFQuIuofPnz48bN47qg/L5fLVazWAwiG+ZTCaPx8MyYoKzGFqBXowOGjRokGalJwaDUVRUVFhYSPVBbW1tm35rbW29detWqg/aLCgxtALXxeiagQMHikSipo88f/5cC2Olbt26aZaXs7Gx2bZtG67Zs6DE0Ar0YnRNWFhY165dzczMNBegyeXyCxcuUH1cKysrExMThJCdnd3OnTuJfjMW0IuhFejF6Jrly5er1erk5OTLly+npqZWV1eLRKLCwsKCggInJyfqjtu5c2dDQ0Mul7tr1y5nZ2fqDvRGcHUvrcB8MTru9u3bN86WSwSGvVy9ZFJqbzIoKyuztrZhsVo7UulkbcDmMBzdjF39TEmMAWcxtALXxegyYY0i9Yhl/5G9TM3ZpuYcFeXv7tZt2pvJYFSVSkrzpE/T68PmkLYcHZzF0Arco6Sz6qrk/xwoHz3PkaHz/c/M67VigXz49LZVqNeBEgOANpyOLQ0eZW1qrh/jhnuXqzvbcjxCeO1/KZ2vqKAt4LoY3VRbKRfWyPWlviCErB2NctJFrdjxzaDE0ApcF6Obqkqljq4muFO0gZWDoVJJzvhGb8oqaI3g4GDcEUAz5BKVVKJDk1S9EYPFqCySkPJSUGJoxdfXF3cEAP4HDJRoJSUl5fbt27hTAPAvKDG0kp6enpGRgTsFAP+CgRKtQC8G6BooMbQCvRiga2CgRCvQiwG6Bs5idFdjY6NcLm/TU4jle4gJ1tqExyPhOk4AXgUlRnfJZLK2lhhifgCZTNbWY6lUKs0MRgCQCEoMrXA4HNwRAPgf8MZFKzKZ7C1OYQCgDpQYWlEoFAqFAncKAP4FJYZWOBxOy2Olurq60NDQq1evajEU6NCgxNDKG0sM0Be5uU+Xf7Vo+MiQPw79evzE4aHD23VR5bgJw+J++4m8dG0A7V5aIRoxBgYGuIOA9kq8lHA/897arzd37+5SW1s984MI3IneEpQYfVJTU7Nv374HDx5IpdKAgIDp06cTqyOfOXPmzz//3Lx587p164qKirp16zZ+/PgRI0YQz0pKSoqLi6uvrw8JCZk4cSLuHwK0ilgssrW179dvAELI1tbO3R3PKkjtByVGbyiVyuXLl4vF4qVLl/bo0ePYsWNLlizZtWuXvb09h8MRiUR79uxZvHixm5vbsWPHtm/f7uvra21tnZeXt2nTppkzZ4aHh+fm5sbExOD+OcCbRa35/PqNJITQ4KGBEfP+z9DQaE/MtsQLKcSQZ87sSIGg7mDcPiMjo6DAvp/83+eWlp0RQnl5z87EH7t77055eamzU/ewsHFj35uE+0eBXoz+yM7OLioq+vLLL4OCgiwsLObPn8/j8U6dOkVslcvlM2bM8Pb2NjAwGDZsmFqtfvbsGULo7Nmz1tbW06dPNzMz8/HxGTVqFO6fA7zZt+u2jH1vkrNz98uJqTOmz2m6icPh/PVXHJPJPHUy8eCvxzOz0g8c3Ets+nHP1jt3bi1ZvPz7jTvDwsbt2Lkp+fYNTD/Bv+AsRm9kZ2dzOBzNjY4MBqN3796ZmZmaHdzc3IhejKmpKUKIWOe0tLS06ZJgrq6uOLIDMjk4OH4wYy5CCJmaBQX2ffLkIfF4VNTGhgaxna09QsjPNzAh4UzKnZshfd7BmxZKjN4QiURyuTw0NLTpg+bm5pqvGQyGQqF4aUkJoVDo4OCg+dbQ0FArYQGFXF3dNV+bmfHE4v/O461Wnzhx+HbKjaKiAuIBOzuH5l9Ci6DE6A0LCwtDQ8O1a9c2fZDFYjX9ls1mE4MmzSM8Hk8qlWq+bWxs1EpYQCEGg/HqgyqV6quVS+Ry2fyIT3x9A81MzRYtmYcj3cugxOiN7t27SyQSKysre3t74pGysjI+n990H+KimKYlxtra+vbt25q7HGGqB7p6kvPo0aPsLdF7AvxfXEEjEtVbdSZnubX2gHav3vDz8wsMDPzhhx8qKysFAkF8fPzixYsvXLjQdJ9X71EaMGBAXV1dTEyMWq3OyMiIj4/XenCgDQJBHUJIU1Py83Pz83Nxh0JQYvTMunXr3n333Y0bN06ZMuX06dODBw8eO3Zs0x1evUcpICAgIiIiNTV11KhRW7duXbZsGUIIlgClH2en7mw2+68jvwnrhYWF+bt2RwcFhpRXlOHOBQMlvcJkMsPDw8PDw196fNSoUcSn0UTt4HA4CQkJmq2TJk2aNOnf6yOabgK0YWNju2rl+oNx+8aOG+Lg4LhqxbfVNVVRaz7/cM6kg78ewxgM1rTWXQKBoK1TUr01CwsLmJKKOg+ShUVPJf3C8XdGWkkuUx/Zmhv5fY/2vxT8VtEKzBcDdA2UGFqB+WKAroFeDK3ATA5A10CJoRUoMUDXwECJVqAXA3QNnMXoLjMzs7Y+JS4uDiE0a9astj6x2WvSAWg/KDG66y0+RSbuw4aPn4HugBJDK7CmNdA18HZHK7CmNdA1UGJoJT09PSMjA3cKAP4FAyVaCQkJgTtCgE6BEkMrvXv3xh0BgP8BAyVaSU5OTk5Oxp0CvIKJDLisVuynKxgMZNaJnMs4ocTQyv379+/fv487BXgZrxOnpkKCO0UbiGrlZF0oBQMlWoFejG6ysOWqlPr0/yKolju4GJPyUjBfDADacOs/1TIZw3+IBe4grXI4OnfmCidDExIGd1BiaIVoxISEhOAOAppx9US1GiH/oZa4g7RE2qBKOFA8eq5dJxtyejEwUKIVohEDJUY3DZhgmfxPzdm9RSwDhrmVgUJGwru7SqViMBik3GJmxGMVPRLzLTkjPrAhq77AWQzd3L9/X61W+/j44A4CXkvaoKqplInrFCoVCX96Bw4c6NevHymLfLINWBY2HHMrkucDgbMYWoHrYnQf15hp50zampyiAzkWXQNc/dt8U77WwIfWtALXxQBdAyWGVuC6GKBrYKBEK3BdDNA1UGJoBXoxQNfAQIlWoBcDdA2UGFqBXgzQNTBQohXoxQBdAyWGVqAXA3QNDJRoBXoxQNdAiaEV6MUAXQMDJVqBXgzQNVBiaAV6MUDXwECJVqAXA3QNlBhagV4M0DUwUKIV6MUAXQMlhlagFwN0DQyUaAV6MUDXQImhFejFAF0DAyVagV4M0DVQYmgFejFA18BAiVagFwN0DZQYWoFeDNA1MFCiFejFdChKpbKxsdHYmJzFpykCZzG00rt3bx8fn9jY2GPHjuHOAqi1d+/ed955Z/z48b6+vriztARKDA3NmjXr6dOn2dnZuIMASvzyyy9BQUFMJjM5OTksLAx3nDeAEkNDxsbGX331Va9evRBC4eHhV69exZ0IkCMuLq5fv35SqTQlJWX+/Pm447QKlBjaYrFYxDvekydPEEJ5eXm4E4G3d+jQoQEDBggEgqSkpIULFzIYDNyJWgtKDM1ZWVlFREQghKqqqsLDw4uLi3EnAm1z9OjRoUOHlpeXJyQkLFq0yMDAAHeitmHABxAdR1lZWUVFha+vb1JS0qBBg3DHAW9w8uTJ2NjYIUOGREZG8vl83HHeEpSYjmj37t3Jycm///477iCgeWfPno2Nje3bt29kZKSlpSXuOO0CJaaDKi0ttbe3T09Pr6ysHDFiBO444IVz587FxMT4+vouXLjQxsYGdxwSwKV3HZS9vT1CyM3N7ejRo9XV1dOmTcOdqKNLTEyMiYlxc3PbvXt3ly5dcMchDZzFACQQCPh8/qZNmwICAoYNG4Y7Todz9erVmJiYrl27Lly40NnZGXcckkGJAS9UVFRs37591apVbDbbyMgId5wO4ebNm7GxsVZWVpGRkS4uLrjjUAJKDPgfSqWyrq7uk08+iYqK8vDwwB2HtlJSUmJjY83MzD766CN6/ztDiQHNyMnJSUlJmTFjxrNnz3r06IE7Dq3cu3cvNjaWxWJFRkZ2hPl9oMSAlpw+ffro0aMxMTFmZma4s+i9zMzM2NhYmUwWGRkZEBCAO46WQIkBb/Do0SMjIyMnJ6dbt2717dsXdxy99OjRo5iYGKFQGBkZ2adPH9xxtApKDGitRYsW2djYrF69GncQffLs2bOYmJjy8vKFCxe+8847uONgACUGtAHRmrl06ZKJiUlHezduq4KCgpiYmPz8/MjIyI58uwZcegfagGj9+vj4rFmzRq1Wh4SE4E6ki0pKSmJjYx8+fBgZGQnXGcFZDHhLdXV15ubmq1evnjx58kufjAwYMGDNmjW0/+uaOnWqSCQ6e/as5pHKysrY2Ni0tLTIyMhRo0ZhTacrYDIH8JbMzc0RQtOmTTt48CBxiTDx+JgxYxoaGnbu3FlbW4s7I4XWrFnz9OnT8vJy4tuamprvvvtu9uzZvr6+p0+fhvqiAWcxgBzZ2dm7du365ptvwsLCmEymWq328/P76aefcOeixMmTJ3fu3FlfX48QsrGxGTRo0IULFyIjIydMmIA7ms6BsxhADk9Pz4iIiKlTpzKZTIQQg8HIysratm0b7lzkKygo2L9/P1FfiFl4nJyczp8/D/WlWVBiAGkCAwOFQqHmW4VCcfbs2StXrmANRb4VK1ZUVFRovmUymcRQETQLSgwgTWhoKHEKoyEUCrds2dK07ui79evXP3ny5KWpczUdGfAq6MUA0owYMYLD4ahUKmKgRPwdymQyJycnTVNGLFCW5jbWPZfXC5RqJWqoV+BO3TwjMzabg0z57E5WnC4uRlzjF6WT+BkRQiqVSq1WE10ntVrN4XDi4+Nxp9ZFUGIAmdLS0oghElFfiHITEhIik6gyrgme3BOJBQqetSlCiM1lGRhxdPbXj8FAsgaFXKZkMlFtSX0na4NeAWY+A/nJyckMBoNY3YH5Xx3hbsa3BiUGUEutQtfjq7Nv1ll3NzcyNzbi6dkE+oSGOqm0vrHkUW3wSMvgkZ1wx9EnUGIAhQoeNSQerjS3M7Poao47Czmq8mrkDdLh06w7O+hlrdQ+KDGAKqkX6x6miR172+IOQjKVUl1wtzQktJN7MExw8WZQYgAl0q/WP05vtHHR7wU6WlCaXRk03NzFB2YgfQMoMYB8N+JrCp8p7NxoW18IxVmVvfua9O7Pwx1Ep8F1MYBkOffE+Y8ktK8vCKEuXtZ3kwTlBVLcQXQalBhApvoaxb2rQgcvOqwx1hrOAfaXj1Wplbhz6DAoMYBM185UGfJNcKfQKgNTo+vxVbhT6C4oMYA0VSXSymIZz6ZjlRjLrvwHKUKJGM5kmgclBpAm/aqws7PutmCOx2+O3kXJurp2rp3vXhZQ8co0ACUGkObRHYGppSHuFBgY8Q0fpkCJaR6UGECO/GxxJztj3Cnw4BiyWBxWVQl8tNQMmB4ckKM0V2rS2ZSiF1cqFf9cjH345EZdXXk3J59+fd73cHuxYMjXG0eOHLpA3FB3/tJPXAMjN5eQsaM+4/E6I4Sk0oY/jq15mptqZ9OzbxC180XxbExLnko6O3ApPYo+grMYQI7yQgmLTdWv08mzW67d+rN/n/dXLjvl7Tkk7vBX97MuEZtYLE7S9d8ZDOa6Fee/XHwkryDj3OX9xKYjpzZUVRd9NHv3h9M2lVfmPnpyg6J4CCHEZFYUw1lMM6DEAHKIhQoOl0XFK8vl0tT0/wx598O+wRNMjPl9At7z6z3yQtLPmh06W3QZNnCOkZEZj9fZrWdIcckjhJBA+Dwj6+Lg/jOdHL14ZpZjRn7CYVPYJ+IYsMRCHZ37Bi8oMYAcSoWaY0jJuLuo9KFCIXPt+e/KcD2c/csqnoobXnRYuzi4azYZGfEkUhFCqKa2BCFkY91Ns8mxyW6k4xiy5RK4F6cZ0IsB5FDIVGoVJX9jkkYRQujHnxa89Hi9qNrEmI8QQojx6rOIAsQ1+LcDbWBA4S2LapVaoVBR9/r6C0oMIIeRGVsuVbApGCsRvdtJY1d0tnBs+ngnfkvTRBDVRyaXaB6RSMWkZ9OQS5UmPPhragb8owBymPDYCiklV7haWXblcLgIoZ7dA4hH6kU1arWay23pM/JO5vYIofzC+8T4SKGQ5zxLMTGhasI6hVTB58NfUzOgFwPIYevEVcgoKTFcrvGIwfMvXP45tyBdrpDdz7q078CiE2c3t/wsc761c1efc5f2VT4vkMulfxyNQoxmxlNkUatUNl3hE+tmQN0F5HB0NcpJr+7kQMlEcIPfnWlv53r5WlzOszuGhqbOjt7vj135xmdNm/j18fhNP8TMUijlQX5jgv3fy35I1aJOdWX1jpMcKHpxvQZTUgHSxC5/5tq/K5Oyq2N0llQkr3hSMWuVE+4guqjD/TYA6riHmNdXS1qxI9001DV6hvBxp9BRMFACpAkaZv77xkK+TdfX7XDo2NcPHl9vdpNSqWCxmv9tnDphjZf7QLJCXrp68NK1uGY3GXFNG6WiZjdFzNzu3PU1iyWpUcmj6vHze5KVkGZgoATIlHSsqqaaadG1+bf0elGNXN78aY5MLjXgNN8uNTWxMDAg7cLcxsb6Rkl98xlkktcdyMzUkvOaeJVPa3p4cAKG0mQVF9JBiQFkUqvRX9uKbT3scAfREoVMKSiqmviJPe4gugt6MYBMDAYaPt06704J7iBa8vRW8Zi5dFsoilxQYgDJLO0M+o22KMooxx2EcvmpJe8tsOcawx9RS2CgBChRmidN/KvK0Ye27/B5d0rGL7Q3t4IPTN4ACjCghH037sBxnZ5cL5SK5LizkKxBIM26kBceYQv1pTXgLAZQqKFe+Z9fyuUKZuduFgZGev8HKamXVeXXmFuyxsy1be7ubtAMKDGAcjn3RNdOVRnyDI34hjxrExZHz86d5VJlfaVYJpbKJbIB4zo7uXfQKYrfDpQYoCV5WeLH98QFD0R8K2O5XMk2YHOMOCpdnWOFwWTIJXKlTGlgyKqvbnT2NHXzM3V0o3DGGbqCEgO0rapEKhYqxUKFXKqSNupoiTEwZHCNWCY8tgmfbWlngDuOHoMSAwCgkJ6NigEA+gVKDACAQlBiAAAUghIDAKAQlBgAAIWgxAAAKPT/NVSqvly/gzMAAAAASUVORK5CYII=",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpu_tutor(visualize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "429ac5bc-557d-4bd7-9ee8-ce576cd64745",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mhint: Using 'master' as the name for the initial branch. This default branch name\u001b[m\n",
      "\u001b[33mhint: is subject to change. To configure the initial branch name to use in all\u001b[m\n",
      "\u001b[33mhint: of your new repositories, which will suppress this warning, call:\u001b[m\n",
      "\u001b[33mhint: \u001b[m\n",
      "\u001b[33mhint: \tgit config --global init.defaultBranch <name>\u001b[m\n",
      "\u001b[33mhint: \u001b[m\n",
      "\u001b[33mhint: Names commonly chosen instead of 'master' are 'main', 'trunk' and\u001b[m\n",
      "\u001b[33mhint: 'development'. The just-created branch can be renamed via this command:\u001b[m\n",
      "\u001b[33mhint: \u001b[m\n",
      "\u001b[33mhint: \tgit branch -m <name>\u001b[m\n",
      "Initialized empty Git repository in /home/osave/1402SJL-ai-accelerated-spark/.git/\n"
     ]
    }
   ],
   "source": [
    "!git init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "ac2c3e5c-9899-4a8f-8bb9-b2aea3359843",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git add ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "7acd50c3-186e-4182-b884-06a280df722e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git config --global user.email \"omiksave@gmail.com\"\n",
    "!git config --global user.name \"omiksave\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "1e66451e-f14f-4bc9-8cca-b626f0baee6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[master (root-commit) e450087] Final commit\n",
      " 82 files changed, 42465 insertions(+)\n",
      " create mode 100644 .ipynb_checkpoints/rag_self_corrective-checkpoint.ipynb\n",
      " create mode 100644 .ipynb_checkpoints/requirements-checkpoint.txt\n",
      " create mode 100644 .ipynb_checkpoints/workflow_visualization-checkpoint.png\n",
      " create mode 100644 gpu_data_science_urls.txt\n",
      " create mode 100644 gpu_datascience_vectorstore/index.faiss\n",
      " create mode 100644 gpu_datascience_vectorstore/index.pkl\n",
      " create mode 100644 rag_self_corrective.ipynb\n",
      " create mode 100644 requirements.txt\n",
      " create mode 100644 resource/.ipynb_checkpoints/5.0_Cupy-checkpoint.ipynb\n",
      " create mode 100644 resource/.ipynb_checkpoints/5.1_Cupy_Lab-checkpoint.ipynb\n",
      " create mode 100644 resource/.ipynb_checkpoints/5.2_Cupy_Lab_solution-checkpoint.ipynb\n",
      " create mode 100644 resource/.ipynb_checkpoints/5.3_Cupy_nvmath-checkpoint.ipynb\n",
      " create mode 100644 resource/0.0_Welcome.ipynb\n",
      " create mode 100644 resource/1.0_CPU_GPU_Comparison.ipynb\n",
      " create mode 100644 resource/2.0_Numba.ipynb\n",
      " create mode 100644 resource/2.1_Numba_lab.ipynb\n",
      " create mode 100644 resource/2.2_Numba_lab_solution.ipynb\n",
      " create mode 100644 resource/3.0_Numba_gauss.ipynb\n",
      " create mode 100644 resource/3.1_Numba_lab_2.ipynb\n",
      " create mode 100644 resource/3.2_Numba_lab_2_solution.ipynb\n",
      " create mode 100644 resource/4.0_pyNVML.ipynb\n",
      " create mode 100644 resource/4.1_CUDA_Array_Interface.ipynb\n",
      " create mode 100644 resource/5.0_Cupy.ipynb\n",
      " create mode 100644 resource/5.1_Cupy_Lab.ipynb\n",
      " create mode 100644 resource/5.2_Cupy_Lab_solution.ipynb\n",
      " create mode 100644 resource/5.3_Cupy_nvmath.ipynb\n",
      " create mode 100644 resource/6.0_cuDF.ipynb\n",
      " create mode 100644 resource/7.0_cuML.ipynb\n",
      " create mode 100644 resource/8.0_Multi-GPU_with_Dask.ipynb\n",
      " create mode 100644 resource/Chapter_01_GPU_Computing_Basics.ipynb\n",
      " create mode 100644 resource/Chapter_02_Brief_Intro_to_CUDA.ipynb\n",
      " create mode 100644 resource/Chapter_03_Python_on_the_GPU.ipynb\n",
      " create mode 100644 resource/Chapter_04_Scientific_Computing_with_CuPy.ipynb\n",
      " create mode 100644 resource/Chapter_05_CUDA_Kernels_with_Numba.ipynb\n",
      " create mode 100644 resource/Chapter_06_Intro_to_nvmath-python.ipynb\n",
      " create mode 100644 resource/Chapter_07_Intro_to_cuDF.ipynb\n",
      " create mode 100644 resource/Chapter_08_Intro_to_cuML.ipynb\n",
      " create mode 100644 resource/Chapter_09_Intro_to_cuGraph.ipynb\n",
      " create mode 100644 resource/Chapter_10_Developer_Tools.ipynb\n",
      " create mode 100644 resource/Chapter_11_Distributed_Computing_cuPyNumeric.ipynb\n",
      " create mode 100644 resource/CostMatrix.ipynb\n",
      " create mode 100644 resource/NYCTaxi-E2E.ipynb\n",
      " create mode 100644 resource/Pagerank.ipynb\n",
      " create mode 100644 resource/Renumber-2.ipynb\n",
      " create mode 100644 resource/Renumber.ipynb\n",
      " create mode 100644 resource/Symmetrize.ipynb\n",
      " create mode 100644 resource/ai_with_a_conscience.ipynb\n",
      " create mode 100644 resource/bfs_benchmark.ipynb\n",
      " create mode 100644 resource/census_education2income_demo.ipynb\n",
      " create mode 100644 resource/compiler.py\n",
      " create mode 100644 resource/cuda_paths.py\n",
      " create mode 100644 resource/cudadecl.py\n",
      " create mode 100644 resource/cudaimpl.py\n",
      " create mode 100644 resource/cudamath.py\n",
      " create mode 100644 resource/decorators.py\n",
      " create mode 100644 resource/descriptor.py\n",
      " create mode 100644 resource/fig_helpers.py\n",
      " create mode 100644 resource/gen_550M.ipynb\n",
      " create mode 100644 resource/libdevice.py\n",
      " create mode 100644 resource/libdevicedecl.py\n",
      " create mode 100644 resource/libdevicefuncs.py\n",
      " create mode 100644 resource/libdeviceimpl.py\n",
      " create mode 100644 resource/line_intersection.ipynb\n",
      " create mode 100644 resource/mathimpl.py\n",
      " create mode 100644 resource/models.py\n",
      " create mode 100644 resource/nvvmutils.py\n",
      " create mode 100644 resource/nx_cugraph_bc_benchmarking.ipynb\n",
      " create mode 100644 resource/printimpl.py\n",
      " create mode 100644 resource/random.py\n",
      " create mode 100644 resource/random_walk_benchmark.ipynb\n",
      " create mode 100644 resource/random_walk_perf.ipynb\n",
      " create mode 100644 resource/release.ipynb\n",
      " create mode 100644 resource/simulator_init.py\n",
      " create mode 100644 resource/sssp_benchmark.ipynb\n",
      " create mode 100644 resource/stubs.py\n",
      " create mode 100644 resource/synth_release_single_gpu.ipynb\n",
      " create mode 100644 resource/synth_release_single_node_multi_gpu.ipynb\n",
      " create mode 100644 resource/target.py\n",
      " create mode 100644 resource/testing.py\n",
      " create mode 100644 resource/umap_demo_full.ipynb\n",
      " create mode 100644 resource/weather.ipynb\n",
      " create mode 100644 workflow_visualization.png\n"
     ]
    }
   ],
   "source": [
    "!git commit -m \"Final commit\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "5ea6f3e7-9cd3-4503-a74a-5300c42303f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git remote add origin https://github.com/omiksave/ai-accelerated-spark.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "c6cb1f17-6cf8-486c-b558-1cdcc5e06566",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(gnome-ssh-askpass:3089355): Gtk-\u001b[1;33mWARNING\u001b[0m **: \u001b[34m22:25:56.487\u001b[0m: cannot open display: \n",
      "error: unable to read askpass response from '/usr/libexec/openssh/gnome-ssh-askpass'\n",
      "Username for 'https://github.com': ^C\n"
     ]
    }
   ],
   "source": [
    "!git branch -M main\n",
    "!git push -u origin main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "e22a8cf5-f48a-44ee-9f71-f3ef2d6f9a49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reinitialized existing Git repository in /home/osave/1402SJL-ai-accelerated-spark/.git/\n",
      "error: remote origin already exists.\n",
      "remote: Permission to omiksave/ai-accelerated-spark.git denied to omiksave.\n",
      "fatal: unable to access 'https://github.com/omiksave/ai-accelerated-spark.git/': The requested URL returned error: 403\n"
     ]
    }
   ],
   "source": [
    "username = \"omiksave\"\n",
    "token = \"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\"\n",
    "repo = \"ai-accelerated-spark\"\n",
    "\n",
    "remote_url = f\"https://{username}:{token}@github.com/{username}/{repo}.git\"\n",
    "!git init\n",
    "# Update the remote\n",
    "\n",
    "\n",
    "!git remote add origin https://github.com/yourusername/your-repo.git\n",
    "!git remote set-url origin \"{remote_url}\"\n",
    "\n",
    "# Push to GitHub\n",
    "!git push -u origin main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aae0bf1f-7b20-4b97-b62f-8c4f7e637306",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genai25.07",
   "language": "python",
   "name": "genai25.07"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
